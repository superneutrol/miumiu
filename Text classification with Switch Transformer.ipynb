{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 # Only consider the top 20k words\n",
    "num_tokens_per_example = 200 #  Only consider the first 200 words of each movie review\n",
    "(x_train , y_train) ,(x_val , y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), 'Training sequences')\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "\n",
    "x_train = keras.utils.pad_sequences(\n",
    "    x_train , maxlen=num_tokens_per_example\n",
    ")\n",
    "x_val = keras.utils.pad_sequences(x_val , maxlen=num_tokens_per_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32 # Embedding size for each token \n",
    "num_heads = 2 # Number of attention heads \n",
    "ff_dim = 32 # Hidden layer size in feedforward network.\n",
    "num_experts = 10 # Number of experts used in the Switch Transformer.\n",
    "batch_size = 50  # Batch size \n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.25 \n",
    "num_epochs = 3 # Number of epochs \n",
    "num_tokens_per_batch = (\n",
    "    batch_size * num_tokens_per_example\n",
    ") # Total number of tokens per batch\n",
    "print(f'Number of tokens per batch: {num_tokens_per_batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement token & position embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the feedforward network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedforward_network(ff_dim, name=None):\n",
    "    return keras.Sequential(\n",
    "        [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(ff_dim)], name=name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the load-balanced loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balanced_loss(router_probs ,expert_mask):\n",
    "    # Router_probs [token_per_batch , num_experts] là xác xuất được chỉ định cho mỗi chuyên gia \n",
    "    # vơí mỗi mã thông báo \n",
    "    # Expert_mask [ token_per_batch , num_experts] chứa chuyên gia có xác xuất bộ định tuyến cao nhất \n",
    "    # ở định dạng one-hot \n",
    "    num_experts = tf.shape(expert_mask)[-1]\n",
    "    # Lấy số lượng chuyên gia từ kích thước của ma trận expert_mask, \n",
    "    # là một ma trận nhị phân có giá trị 1 nếu token được gửi đến chuyên gia tương ứng và 0 nếu không.\n",
    "    density = tf.reduce_mean(expert_mask , axis=0)\n",
    "    # Tính tỷ lệ token được gửi đến mỗi chuyên gia, bằng cách lấy trung bình theo trục 0 của ma trận expert_mask. \n",
    "    # Kết quả là một vector density có độ dài bằng số lượng chuyên gia và tổng bằng 1.\n",
    "    density_proxy = tf.reduce_mean(router_probs , axis=0)\n",
    "    # Tính tỷ lệ xác suất được gán cho mỗi chuyên gia từ bộ định tuyến, \n",
    "    # bằng cách lấy trung bình theo trục 0 của ma trận router_probs, \n",
    "    # là một ma trận có giá trị từ 0 đến 1 cho biết xác suất của token được gửi đến chuyên gia tương ứng. \n",
    "    # Kết quả là một vector density_proxy có độ dài bằng số lượng chuyên gia và tổng bằng 1.\n",
    "    loss = tf.reduce_mean(density_proxy * density) * tf.cast(\n",
    "        (num_experts ** 2), tf.dtypes.float32\n",
    "    )\n",
    "    # Tính hàm mất mát bằng cách lấy tích vô hướng của hai vector density và density_proxy, \n",
    "    # rồi nhân với bình phương của số lượng chuyên gia. Hàm mất mát này sẽ có giá trị nhỏ nhất khi hai vector này là đồng nhất, \n",
    "    # tức là khi cả hai đều có giá trị bằng 1/số lượng chuyên gia. \n",
    "    # Điều này có nghĩa là token và xác suất được phân phối đều cho các chuyên gia.\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_capacity):\n",
    "        self.num_experts = num_experts\n",
    "        self.route = layers.Dense(units=num_experts)\n",
    "        self.expert_capacity = expert_capacity\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs shape: [tokens_per_batch, embed_dim]\n",
    "        # router_logits shape: [tokens_per_batch, num_experts]\n",
    "        router_logits = self.route(inputs)\n",
    "\n",
    "        if training:\n",
    "            # Nếu đang ở chế độ huấn luyện, thêm nhiễu ngẫu nhiên vào router_logits để khuyến khích việc khám phá các chuyên gia khác nhau. \n",
    "            # Nhiễu ngẫu nhiên có giá trị từ 0.9 đến 1.1.\n",
    "            router_logits += tf.random.uniform(\n",
    "                shape=router_logits.shape, minval=0.9, maxval=1.1\n",
    "            )\n",
    "        # Tính router_probs là một tensor có kích thước [tokens_per_batch, num_experts] chứa xác suất của mỗi token được gửi đến mỗi chuyên gia. \n",
    "        # Xác suất được tính bằng hàm softmax trên trục -1 của router_logits\n",
    "        router_probs = keras.activations.softmax(router_logits, axis=-1)\n",
    "        # Lấy chuyên gia có xác suất cao nhất cho mỗi token. \n",
    "        # expert_gate là một tensor có kích thước [tokens_per_batch, 1] chứa xác suất cao nhất từ router_probs cho mỗi token. \n",
    "        # expert_index là một tensor có kích thước [tokens_per_batch, 1] chứa chỉ số của chuyên gia tương ứng với xác suất cao nhất cho mỗi token.\n",
    "        expert_gate, expert_index = tf.math.top_k(router_probs, k=1)\n",
    "        # Tính expert_mask là một tensor có kích thước [tokens_per_batch, num_experts] chứa giá trị nhị phân cho biết token nào được gửi đến chuyên gia nào. \n",
    "        # Giá trị này được tính bằng cách sử dụng hàm tf.one_hot với expert_index và depth là num_experts.\n",
    "        expert_mask = tf.one_hot(expert_index, depth=self.num_experts)\n",
    "        # Tính hàm mất mát cân bằng tải bằng cách gọi hàm load_balanced_loss với router_probs và expert_mask làm đầu vào. \n",
    "        # Thêm hàm mất mát này vào danh sách các hàm mất mát của lớp bằng phương thức self.add_loss.\n",
    "        aux_loss = load_balanced_loss(router_probs, expert_mask)\n",
    "        self.add_loss(aux_loss)\n",
    "        # Tính position_in_expert là một tensor có kích thước [tokens_per_batch, num_experts] cho biết vị trí của token trong hàng đợi của mỗi chuyên gia. \n",
    "        # Giá trị này được tính bằng cách lấy tổng tích lũy theo trục 0 của expert_mask rồi nhân với expert_mask. \n",
    "        # Sau đó, ép kiểu tensor này sang kiểu int32.\n",
    "        position_in_expert = tf.cast(\n",
    "            tf.math.cumsum(expert_mask, axis=0) * expert_mask, tf.dtypes.int32\n",
    "        )\n",
    "        # Lọc ra các token có vị trí trong hàng đợi của chuyên gia nhỏ hơn expert_capacity, tức là các token không vượt quá khả năng xử lý của chuyên gia. \n",
    "        # Điều này được thực hiện bằng cách sử dụng hàm tf.math.less để so sánh position_in_expert và expert_capacity, \n",
    "        # rồi ép kết quả sang kiểu float32 và nhân với expert_mask. Kết quả là expert_mask được cập nhật lại để loại bỏ các token không được gửi đến các chuyên gia.\n",
    "        expert_mask *= tf.cast(\n",
    "            tf.math.less(\n",
    "                tf.cast(position_in_expert, tf.dtypes.int32), self.expert_capacity\n",
    "            ),\n",
    "            tf.dtypes.float32,\n",
    "        )\n",
    "        # Tính expert_mask_flat là một tensor có kích thước [tokens_per_batch] bằng cách lấy tổng theo trục -1 của expert_mask. \n",
    "        # Giá trị này cho biết token nào được gửi đến ít nhất một chuyên gia.\n",
    "        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)\n",
    "\n",
    "        # Cập nhật lại expert_gate bằng cách nhân với expert_mask_flat để loại bỏ các token không được gửi đến bất kỳ chuyên gia nào.\n",
    "        expert_gate *= expert_mask_flat\n",
    "        #  Giá trị này cho biết xác suất định tuyến và hệ số cân bằng tải của mỗi token đối với mỗi chuyên gia và mỗi vị trí trong hàng đợi của chuyên gia.\n",
    "        combined_tensor = tf.expand_dims(\n",
    "            expert_gate\n",
    "            * expert_mask_flat\n",
    "            * tf.squeeze(tf.one_hot(expert_index, depth=self.num_experts), 1),\n",
    "            -1,\n",
    "        ) * tf.squeeze(tf.one_hot(position_in_expert, depth=self.expert_capacity), 1)\n",
    "        # Tính dispatch_tensor là một tensor có kích thước [tokens_per_batch, num_experts, expert_capacity] bằng cách ép kiểu combined_tensor sang kiểu float32. \n",
    "        # Giá trị này cho biết token nào được gửi đến chuyên gia nào và vị trí nào trong hàng đợi của chuyên gia bằng giá trị nhị phân 0 hoặc 1.\n",
    "        dispatch_tensor = tf.cast(combined_tensor, tf.dtypes.float32)\n",
    "\n",
    "        return dispatch_tensor, combined_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Switch(layers.Layer):\n",
    "    def __init__(self, num_experts, embed_dim, num_tokens_per_batch, capacity_factor=1):\n",
    "        self.num_experts = num_experts\n",
    "        self.embed_dim = embed_dim\n",
    "        self.experts = [\n",
    "            create_feedforward_network(embed_dim) for _ in range(num_experts)\n",
    "        ]\n",
    "\n",
    "        self.expert_capacity = num_tokens_per_batch // self.num_experts\n",
    "        self.router = Router(self.num_experts, self.expert_capacity)\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        num_tokens_per_example = tf.shape(inputs)[1]\n",
    "\n",
    "        # inputs shape : [num_token_per_patch , embed_dim]\n",
    "        inputs = tf.reshape(inputs, [num_tokens_per_batch, self.embed_dim])\n",
    "        # dispatch_tensor (tensor gửi đi ) shape : [expert_capacity , num_experts, tokens_per_batch]\n",
    "        # combine_tensor (tensor kết hợp) shape : [token_per_batch , num_experts , expert_capacity]\n",
    "        dispatch_tensor, combine_tensor = self.router(inputs)\n",
    "        # expert_inputs shape : [num_experts, expert_capacity , embed_dim]\n",
    "        # tính toán một tensor mới có kích thước là [expert_capacity, num_experts, embed_dim] \n",
    "        # bằng cách thực hiện phép nhân ma trận giữa tensor inputs và tensor dispatch_tensor theo công thức ‘ab,acd->cdb’\n",
    "        expert_inputs = tf.einsum(\"ab,acd->cdb\", inputs, dispatch_tensor)\n",
    "        expert_inputs = tf.reshape(\n",
    "            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n",
    "        )\n",
    "        # Dispatch to experts (gửi đến chuyên gia)\n",
    "        # dùng hàm tf.unstack để tách tensor expert_inputs thành một danh sách các tensor có kích thước là [expert_capacity, embed_dim] theo chiều thứ nhất (num_experts). \n",
    "        # Hàm tf.unstack cho phép bạn tách một tensor có kích thước là R thành một danh sách các tensor có kích thước là R-1 theo một chiều nào đó\n",
    "        expert_input_list = tf.unstack(expert_inputs, axis=0)\n",
    "        expert_output_list = [\n",
    "            self.experts[idx](expert_input)\n",
    "            for idx, expert_input in enumerate(expert_input_list)\n",
    "        ]\n",
    "        # Expert_outputs shape : [expert_capacity , num_expert , embeb_dim ]\n",
    "        expert_outputs = tf.stack(expert_output_list, axis=1)\n",
    "        # expert_outputs_combined shape: [tokens_per_batch, embed_dim]\n",
    "        expert_outputs_combined = tf.einsum(\n",
    "            \"abc,xba->xc\", expert_outputs, combine_tensor\n",
    "        )\n",
    "        # output_shape : [batch_size , num_tokens_per_example , embed_dim]\n",
    "        outputs = tf.reshape(\n",
    "            expert_outputs_combined,\n",
    "            [batch_size, num_tokens_per_example, self.embed_dim],\n",
    "        )\n",
    "        return outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Transformer block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # The ffn can be either a standard feedforward network or a switch\n",
    "        # layer with a Mixture of Experts.\n",
    "        self.ffn = ffn\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier():\n",
    "    switch = Switch(num_experts, embed_dim, num_tokens_per_batch)\n",
    "    transformer_block = TransformerBlock(ff_dim, num_heads, switch)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_tokens_per_example,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(\n",
    "        num_tokens_per_example, vocab_size, embed_dim\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(classifier):\n",
    "    classifier.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    history = classifier.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "classifier = create_classifier()\n",
    "run_experiment(classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
