{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import scipy.io\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to images and annotations\n",
    "path_images = \"/101_ObjectCategories/airplanes/\"\n",
    "path_annot = \"/Annotations/Airplanes_Side_2/\"\n",
    "\n",
    "path_to_downloaded_file = keras.utils.get_file(\n",
    "    fname=\"caltech_101_zipped\",\n",
    "    origin=\"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\",\n",
    "    extract=True,\n",
    "    archive_format=\"zip\",  # downloaded file format\n",
    "    cache_dir=\"/\",  # cache and extract in current directory\n",
    ")\n",
    "\n",
    "# Extracting tar files found inside main zip file\n",
    "shutil.unpack_archive(\"/datasets/caltech-101/101_ObjectCategories.tar.gz\", \"/\")\n",
    "shutil.unpack_archive(\"/datasets/caltech-101/Annotations.tar\", \"/\")\n",
    "\n",
    "# list of paths to images and annotations\n",
    "image_paths = [\n",
    "    f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))\n",
    "]\n",
    "annot_paths = [\n",
    "    f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))\n",
    "]\n",
    "\n",
    "image_paths.sort()\n",
    "annot_paths.sort()\n",
    "\n",
    "image_size = 224  # resize input images to this size\n",
    "\n",
    "images, targets = [], []\n",
    "\n",
    "# loop over the annotations and images, preprocess them and store in lists\n",
    "for i in range(0, len(annot_paths)):\n",
    "    # Access bounding box coordinates\n",
    "    annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]\n",
    "\n",
    "    top_left_x, top_left_y = annot[2], annot[0]\n",
    "    bottom_right_x, bottom_right_y = annot[3], annot[1]\n",
    "\n",
    "    image = keras.utils.load_img(\n",
    "        path_images + image_paths[i],\n",
    "    )\n",
    "    (w, h) = image.size[:2]\n",
    "\n",
    "    # resize train set images\n",
    "    if i < int(len(annot_paths) * 0.8):\n",
    "        # resize image if it is for training dataset\n",
    "        image = image.resize((image_size, image_size))\n",
    "\n",
    "    # convert image to array and append to list\n",
    "    images.append(keras.utils.img_to_array(image))\n",
    "\n",
    "    # apply relative scaling to bounding boxes as per given image and append to list\n",
    "    targets.append(\n",
    "        (\n",
    "            float(top_left_x) / w,\n",
    "            float(top_left_y) / h,\n",
    "            float(bottom_right_x) / w,\n",
    "            float(bottom_right_y) / h,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Convert the list to numpy array, split to train and test dataset\n",
    "(x_train), (y_train) = (\n",
    "    np.asarray(images[: int(len(images) * 0.8)]),\n",
    "    np.asarray(targets[: int(len(targets) * 0.8)]),\n",
    ")\n",
    "(x_test), (y_test) = (\n",
    "    np.asarray(images[int(len(images) * 0.8) :]),\n",
    "    np.asarray(targets[int(len(targets) * 0.8) :]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x , hidden_units , dropout_rate):\n",
    "    for units in hidden_units: \n",
    "        x = layers.Dense(units , activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    #     Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[1]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images ,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        # return patches\n",
    "        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 32  \n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(x_train[0].astype('unit8'))\n",
    "plt.axis('off')\n",
    "\n",
    "patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]]))\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches , projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches \n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches , output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start=0 , limit=self.num_patches, delta=1)\n",
    "        return self.projection(patch) + self.position_embedding(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_object_detector(\n",
    "        input_shape , \n",
    "        patch_size, \n",
    "        num_patches, \n",
    "        projection_dim,\n",
    "        num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    "):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches \n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches \n",
    "    encoded_patches = PatchEncoder(num_patches , projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the transformer block \n",
    "    for _ in range (transformer_layers):\n",
    "        # Layer normalization 1 \n",
    "        x1 = layers.LayerNormalization(epsilin=1e-6)(encoded_patches)\n",
    "\n",
    "        # create a multi head  attention layers\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim , dropout=0.1\n",
    "        )(x1 , x1)\n",
    "\n",
    "        # skip connection 1 \n",
    "        x2 = layers.Add()([attention_output , encoded_patches])\n",
    "         \n",
    "        # Layer normalization 2\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        \n",
    "        # MLP \n",
    "        x3 = mlp(x3 , hidden_units=transformer_units , dropout_rate=0.1)\n",
    "        # skip connection 2 .\n",
    "        encoded_patches = layers.Add()([x3,x2])\n",
    "    \n",
    "    # \n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units , dropout_rate=0.3)\n",
    "    bounding_box = layers.Dense(4)(features) \n",
    "\n",
    "    return keras.Model(inputs=inputs , outputs=bounding_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Compile model.\n",
    "    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "    checkpoint_filepath = \"logs/\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "input_shape = (image_size, image_size, 3)  # input image shape\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "# Size of the transformer layers\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 4\n",
    "mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n",
    "\n",
    "\n",
    "history = []\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "vit_object_detector = create_vit_object_detector(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = run_experiment(\n",
    "    vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
