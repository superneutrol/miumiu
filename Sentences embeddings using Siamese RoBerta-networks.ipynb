{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp \n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds \n",
    "import sklearn.cluster as cluster \n",
    "from tensorflow import keras  \n",
    "# Thiết lập chính sách độ chính xác toàn cục cho các lớp keras \n",
    "# 1 Xác định kiểu tính toán và kiểu dữ liệu biến cho một lớp hoặc\n",
    "# một mô hình sử dựng float 16 cho các phép tính và float 32 cho các biến \n",
    "# nhằm tăng tốc độ huấn luyện và giảm bộ nhớ trên GPU\n",
    "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dowload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 6\n",
    "VALIDATION_BATCH_SIZE = 8 \n",
    "\n",
    "TRAIN_NUM_BATCHS = 300 \n",
    "VALIDATION_NUM_BATCHS = 40\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Xây dựng phương thức chage_range  \n",
    "# sử dụng để chuyền nhãn của tập dữ liệu [0 ,5] -> [-1 , 1]\n",
    "# nhằm phù hợp với hàm mất mát của mô hình \n",
    "def change_range(x):\n",
    "    return (x / 2.5) - 1\n",
    "\n",
    "# Xây dựng phương thức Prepare_dataset để chuẩn bị cho \n",
    "# việc huấn luyện mạng siamese , nhận đầu vào tập dữ liệu thô , \n",
    "# số lượng lô , và kích thước lô \n",
    "def prepare_dataset(dataset , num_batchs , batch_size):\n",
    "    # áp dụn hàm map để biến đổi mỗi phần tử trong tập dữ liệu thành một cặp    \n",
    "    # gồm danh sách 2 câu và danh sách một bộ nhãn được chuyển đổi \n",
    "    dataset = dataset.map(\n",
    "        lambda z : (\n",
    "            [z['sentence1'], z['sentence2']],\n",
    "            [tf.cast(change_range(z['label']), tf.float32)]\n",
    "        ), \n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "    )\n",
    "    # áp dụng batch để lấy ra kích thước batch \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # áp dụng hàm take để lấy số lượng batch theo batch_size \n",
    "    dataset = dataset.take(num_batchs)\n",
    "    # áp dụng hàm prefectch để tải các các tiếp theo trong khi xử lý các \n",
    "    # batch hiện tại áp dụng tham số autotune để tự động điều chỉnh \n",
    "    # số lượng batch được tiền tải \n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset \n",
    "\n",
    "# Tải tập dữ liệu glue/stsb\n",
    "stsb_ds = tfds.load(\n",
    "    \"glue/stsb\",\n",
    ")\n",
    "# Lấy ra dữ liệu train và vlidation từ data glue/stsb đã load trước đó \n",
    "stsb_train , stsb_valid = stsb_ds['train'],  stsb_ds['validation']\n",
    "\n",
    "# Biến đổi 2 bộ dữ liệu train và validation thô thành tiêu chuẩn để có thể \n",
    "# sử dụng cho mô hình , và truyền vào đó các tham số cần thiết \n",
    "stsb_train  = prepare_dataset(stsb_train , TRAIN_NUM_BATCHS , TRAIN_BATCH_SIZE)\n",
    "\n",
    "stsb_valid = prepare_dataset(stsb_valid, VALIDATION_NUM_BATCHS, VALIDATION_BATCH_SIZE)\n",
    "\n",
    "\n",
    "# In ra các mẫu từ data train \n",
    "for x , y in stsb_train:\n",
    "    for i , example in enumerate(x):\n",
    "        print(f\"sentence 1 : {example[0]} \")\n",
    "        print(f\"sentence 2 : {example[1]} \")\n",
    "        print(f\"similarity : {y[i]} \\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the encoder model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Một lớp tiền sử lý để mã hóa và tạo mặt nạ đệm cho các câu \n",
    "preprocessor = keras_nlp.models.RobertaPreprocessor.from_preset(\"roberta_base_en\")\n",
    "# Một lớp kiến trúc xương sống tạo ra sự hiểu biết biểu diễn theo ngữ cảnh \n",
    "backbone = keras_nlp.models.RobertaBackbone.from_preset(\"robert_base_en\")\n",
    "# Thêm 1 lớp đầu vào dạng chuỗi shape = 1 là cho phép 1 câu văn bản \n",
    "inputs = keras.Input(shape=(1), dtype=\"string\" , name='sentence')\n",
    "# Thêm 1 lớp tiền sử lý đầu vào \n",
    "x = preprocessor(inputs)\n",
    "# đưa dữ liệu đã xử lý qau mô hình kiến trúc backbone\n",
    "h = backbone(x) \n",
    "# Thêm 1 lớp gộp Pooling nhận đầu vào h và x với padding mask x \n",
    "embedding = keras.layers.GlobalAveragePooling1D(name=\"pooling_layer\")(\n",
    "    h, x[\"padding_mask\"]\n",
    ")\n",
    "# áp dụng chuẩn hóa theo trục để chuẩn hóa vector đầu ra theo chuẩn EUCLIDEAN \n",
    "# Trả về 2 giá trị vector chuẩn hóa và giá trị chuẩn hóa của vector ban đầu \n",
    "n_embedding = tf.linalg.normalize(embedding , axis=1)[0] # lấy vector đã được chuẩn hóa\n",
    "roberta_normal_encoder = keras.Model(inputs=inputs , outputs = n_embedding)\n",
    "roberta_normal_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Siamese network with the regression objective funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng mạng siamese hồi quy sử dụng mô hình mã hóa Robert\n",
    "# để tính toán độ tương đồng cosin giữa 2 văn bản '\n",
    "class RegressionSiamese(keras.Model):\n",
    "    # xây dựng phương thức khởi tạo nhận vào là khối mã hóa của Robert \n",
    "    # và **kwargs là tham số bất kỳ \n",
    "    def __init__(self, encoder , **kwargs):\n",
    "        # Thiết lập đầu vào dạng chuỗi với 2 câu \n",
    "        inputs = keras.Input(shape=(2), dtype='string', name='sentences')\n",
    "        # tách ra thành 2 câu riêng lẻ để có thể tính toán độ tương tự \n",
    "        # cosin , tách theo trục \n",
    "        sen1 , sen2 = tf.split(inputs , num_or_size_splits=2, axis=1 , name='split')\n",
    "        # Thực hiện mã hóa sen1  và 2 bởi Robert encoder \n",
    "        u = encoder(sen1)\n",
    "        v = encoder(sen2)\n",
    "        # Tính toán tích vô hướng (tích gộp) giữa u và v chuyển vị\n",
    "        # kết quả là 1 tensor shape = [batch_size , batch_size]\n",
    "        # chưa các giá trị độ tương đồng cosin giữa các cặp câu văn bản trong batch \n",
    "        cosine_similarity_scores = tf.matmul(u, tf.transpose(v))\n",
    "\n",
    "        # gọi lại phương thức khởi tạo của lớp cha \n",
    "        # và chyền vào các tham số \n",
    "        super().__init__(\n",
    "            inputs=inputs,\n",
    "            outputs=cosine_similarity_scores,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # khởi tạo encoder \n",
    "        self.encoder = encoder \n",
    "\n",
    "    # Xây dựng phuuwong thức trả về bộ phận mã hóa \n",
    "    def get_encoder (self):\n",
    "        return self.encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo một bộ từ điển với danh sách chưa  câu \n",
    "sentences = [\n",
    "    \"Today is a very sunny day.\",\n",
    "    \"I am hungry, I will get my meal.\",\n",
    "    \"The dog is eating his food.\",\n",
    "]\n",
    "# tạo một danh sách chưa câu truy vấn \n",
    "# để so sánh độ tươnh đồng về ngữ nghĩa với các cau trong từ điển trên\n",
    "query = ['The dog is enjoying his meal.']\n",
    "# đặt encoder = Robert model \n",
    "encoder = roberta_normal_encoder \n",
    "\n",
    "# thực hiện mã hóa bộp từ điển và câu truy vấn sử dụng tf.constant để \n",
    "# không thay đổi giá trị \n",
    "sentence_embeddings = encoder(tf.constant(sentences))\n",
    "query_embedding = encoder(tf.constant(query))\n",
    "\n",
    "# Tính toán độ tương tự cosin sử dụng hàm tf.matmul để tính tích vô hướng \n",
    "cosine_similarity_scores = tf.matmul(query_embedding, tf.transpose(sentence_embeddings))\n",
    "\n",
    "# Duyệt qua các cặp một và in ra danh sách cùng với độ tương đồng của nó\n",
    "for i , sim in enumerate(cosine_similarity_scores[0]):\n",
    "    print(f\"cosine similarity score between sentence {i+1} and the query = {sim} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo mô hình mạng siamese hồi quy robert \n",
    "roberta_regression_siamese = RegressionSiamese(roberta_normal_encoder)\n",
    "\n",
    "# Trình biên dịch mô hình \n",
    "roberta_regression_siamese.compile(\n",
    "    # ,loss = MeanSquaredError \n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # trình tối ưu hóa = Adam lr = 2e-5 \n",
    "    optimizer=keras.optimizers.Adam(2e-5),\n",
    ")\n",
    "roberta_regression_siamese.fit(stsb_train, validation_data=stsb_valid, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test affter Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Today is a very sunny day.\",\n",
    "    \"I am hungry, I will get my meal.\",\n",
    "    \"The dog is eating his food.\",\n",
    "]\n",
    "query = [\"The dog is enjoying his food.\"]\n",
    "\n",
    "encoder = roberta_regression_siamese.get_encoder()\n",
    "\n",
    "sentence_embeddings = encoder(tf.constant(sentences))\n",
    "query_embedding = encoder(tf.constant(query))\n",
    "\n",
    "cosine_simalarities = tf.matmul(query_embedding, tf.transpose(sentence_embeddings))\n",
    "for i, sim in enumerate(cosine_simalarities[0]):\n",
    "    print(f\"cosine similarity between sentence {i+1} and the query = {sim} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune Using the triplet Objective Function\n",
    "*  Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://sbert.net/datasets/wikipedia-sections-triplets.zip -q\n",
    "!unzip wikipedia-sections-triplets.zip  -d  wikipedia-sections-triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_BATCHS = 200\n",
    "NUM_TEST_BATCHS = 75 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE \n",
    "\n",
    "# Xây dựng phương thức tách dữ liệu và dựng giữ liệu từ wiki \n",
    "def prepare_wiki_data(dataset , num_batchs):\n",
    "    # tách ra thành 3 câu từ bộ dữ liệu \n",
    "    datasetc= dataset.map(\n",
    "        # 0 được sử dụng như một nhãn giả cho việc huấn luyện và kiểm tra \n",
    "        lambda z: ((z['Sentence1'], z['Sentence2'] , z['Sentence3']),0)\n",
    "    )\n",
    "    # Đặt số lô , kích thước cho mỗi batch, và phép biến đổi prefetch \n",
    "    dataset = dataset.batch(6)\n",
    "    dataset = dataset.take(num_batchs)\n",
    "    # áp dụng hàm prefectch để tải các các tiếp theo trong khi xử lý các \n",
    "    # batch hiện tại áp dụng tham số autotune để tự động điều chỉnh \n",
    "    # số lượng batch được tiền tải \n",
    "    dataset = dataset.prefecth(AUTOTUNE)\n",
    "\n",
    "\n",
    "# Lấy ra bộ wiki train và test \n",
    "wiki_train = tf.data.experimental.make_csv_dataset(\n",
    "     \"wikipedia-sections-triplets/train.csv\",\n",
    "    batch_size=1,\n",
    "    num_epochs=1,\n",
    ")\n",
    "wiki_test = tf.data.experimental.make_csv_dataset(\n",
    "    \"wikipedia-sections-triplets/test.csv\",\n",
    "    batch_size=1,\n",
    "    num_epochs=1,\n",
    ")\n",
    "\n",
    "# Tạo bộ dữ liệu train , test hoàn  chỉnh \n",
    "wiki_train = prepare_wiki_data(wiki_train , NUM_TRAIN_BATCHS)\n",
    "wiki_test = prepare_wiki_data(wiki_test, NUM_TEST_BATCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Encoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập lớp xử lý với Robert\n",
    "preprocessor = keras_nlp.models.RobertaPreprocessor.from_preset(\"roberta_base_en\")\n",
    "# Thêm lớp Backbone Robert \n",
    "backbone = keras_nlp.models.RobertBackbone.from_preset(\"roberta_base_en\")\n",
    "# Thiết lập lơp xử lý đầu vào \n",
    "input = keras.Input(shape=(1), dtype='string', name=\"sentence\")\n",
    "\n",
    "x = preprocessor(input)\n",
    "h = backbone(x)\n",
    "embedding = keras.layers.GlobalAveragePooling1D(name=\"pooling_layer\")(\n",
    "    h, x[\"padding_mask\"]\n",
    ")\n",
    "\n",
    "roberta_encoder = keras.Model(inputs=input, outputs=embedding)\n",
    "roberta_encoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Siamese network with the triplet objective funcion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng mạng siamese với hàm mục tiêu triplet loss \n",
    "# tính khoảng cách của negative và positive \n",
    "class TripletSiamese(keras.Model):\n",
    "    def __init__(self, encoder , **kwargs):\n",
    "\n",
    "        anchor = keras.Input(shape=(1) , dtype='string')\n",
    "        positive = keras.Input(shape=(1), dtype='string')\n",
    "        negative = keras.Input(shape=(1) , dtype='string')\n",
    "\n",
    "        ea = encoder(anchor)\n",
    "        ep = encoder(positive)\n",
    "        en = encoder(negative)\n",
    "\n",
    "        positive_dist = tf.math.reduce_sum(tf.math.pow(ea - ep, 2), axis=1)\n",
    "        negative_dist = tf.math.reduce_sum(tf.math.pow(ea - en , 2), axis=1)\n",
    "\n",
    "        positive_dist = tf.math.sqrt(positive_dist)\n",
    "        negative_dist = tf.math.sqrt(negative_dist)\n",
    "\n",
    "        outputs = tf.stack([positive_dist , negative_dist] , axis=0)\n",
    "\n",
    "        super().__init__(\n",
    "            inputs = [anchor , positive , negative],\n",
    "            outputs = outputs , \n",
    "            **kwargs \n",
    "        )\n",
    "        self.encoder = encoder \n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buil Triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(keras.losses.Loss):\n",
    "    def __init__(self, margin=1 , **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin \n",
    "\n",
    "    def call(self, y_true , y_pred):\n",
    "        positive_dist , negative_dist = tf.unstack(y_pred , axis=0)\n",
    "\n",
    "        losses = tf.nn.relu(positive_dist - negative_dist + self.margin)\n",
    "        return tf.math.reduce_mean(losses , axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_triplet_siamese = TripletSiamese(roberta_encoder)\n",
    "\n",
    "roberta_triplet_siamese.compile(\n",
    "    loss=TripletLoss(),\n",
    "    optimizer=keras.optimizers.Adam(2e-5),\n",
    ")\n",
    "\n",
    "roberta_triplet_siamese.fit(wiki_train, validation_data=wiki_test, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một tập từ điển chứa 1 danh sách gồm 6 câu hỏi . \n",
    "questions = [\n",
    "    \"What should I do to improve my English writting?\",\n",
    "    \"How to be good at speaking English?\",\n",
    "    \"How can I improve my English?\",\n",
    "    \"How to earn money online?\",\n",
    "    \"How do I earn money online?\",\n",
    "    \"How to work and ean money through internet?\",\n",
    "]\n",
    "# gọi ra lớp mã hóa từ Robert \n",
    "encoder = roberta_triplet_siamese.get_encoder()\n",
    "embeddings = encoder(tf.constant(questions))\n",
    "# Phân loại với Kmeans \n",
    "kmeans = cluster.KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(embeddings)\n",
    "\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    print(f\"sentence ({questions[i]}) belongs to cluster {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
