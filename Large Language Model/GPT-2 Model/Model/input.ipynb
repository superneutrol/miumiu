{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf \n",
    "\n",
    "# Expects .tfrecords files as produced by the script in datasets in a google storage bucket\n",
    "# Standard openwebtext \n",
    "# nhận đầu vào gồm params là một từ điển chứa các thông số cấu hình cho hàm như \n",
    "# data_path, batch_size , n_ctx . Tham số eval là một giá trị bool mặc định là false \n",
    "# xác định xem hàm này có được sử dụng cho mục đích đánh giá hay không . \n",
    "# tham số Batch cũng là một giá trị boolean mặc định True xác định xem hàm này có trả về một \n",
    "# đối tượng batch hay không\n",
    "def openwebtext(params ,eval=False, batch=True):\n",
    "    # nếu eval là False hàm này sẽ tạo một danh sách có tên là number, chứa các số 0->92\n",
    "    # trừ 32 (vì 32 là trống) và một số khác. \n",
    "    if not eval:\n",
    "        # các só trong danh mục numbers có chức năng là chỉ định các tệp tfrecord mà hàm openwebtext sẽ đọc \n",
    "        # các tệp tfrecord này được đặt tên theo dạng openwebtext-newspaper_{i}.tfrecord trong đó \n",
    "        # {i} là một số từ 0 -> 92  \n",
    "        numbers = [0, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, # 32, (32 is empty)\n",
    "                33, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 63, 64,\n",
    "                65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
    "    # nếu Eval = True hàm này sẽ tạo một danh sách khác tên là numbers, chứa các số \n",
    "    # từ ->83 trừ một số khác .\n",
    "    else:\n",
    "        numbers = [1, 2, 11, 12, 16, 34, 36, 58, 60, 61, 62, 67, 83]\n",
    "\n",
    "    # sau đó tạo một danh sách có tên là files bằng cách nối đường dẫn của các tệp tfrecords trong \n",
    "    # thư mục data_path của từ điển params với các số trong danh mục numbers . Các tệp tfrecord này chứa các văn bản \n",
    "    # từ nguồn OpenWebtext \n",
    "    files = [os.path.join(params['data_path'], \"openwebtext-newspaper_{}.tfrecord\" .format(str(i)))  for i in numbers]\n",
    "\n",
    "\n",
    "    # cuối cùng hàm này sẽ gọi một hàm khác có tên là bpe_text với các tham số là batch_size \n",
    "    # files, amount, iterations, stitch và batch . Hàm này se sẽ trả về một danh sách văn bản được\n",
    "    # mã hóa bằng phương pháp byte pair encoding \n",
    "    return bpe_text(params[\"batch_size\"], files, amount=params[\"n_ctx\"], iterations=params[\"iterations\"], stitch=42, batch=batch)\n",
    "\n",
    "\n",
    "# thiết lập một phương thức tương tự \n",
    "# nhưng chỉ lọc những văn bản có dài hơn 512 tokens \n",
    "\n",
    "def openwebtext_long(params ,eval=False, batch=True):\n",
    "    # nếu eval là False hàm này sẽ tạo một danh sách có tên là number, chứa các số 0->92\n",
    "    # trừ 32 (vì 32 là trống) và một số khác. \n",
    "    if not eval:\n",
    "        # các só trong danh mục numbers có chức năng là chỉ định các tệp tfrecord mà hàm openwebtext sẽ đọc \n",
    "        # các tệp tfrecord này được đặt tên theo dạng openwebtext-newspaper_{i}.tfrecord trong đó \n",
    "        # {i} là một số từ 0 -> 92  \n",
    "        numbers = [0, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, # 32, (32 is empty)\n",
    "                33, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 63, 64,\n",
    "                65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
    "    # nếu Eval = True hàm này sẽ tạo một danh sách khác tên là numbers, chứa các số \n",
    "    # từ ->83 trừ một số khác .\n",
    "    else:\n",
    "        numbers = [1, 2, 11, 12, 16, 34, 36, 58, 60, 61, 62, 67, 83]\n",
    "\n",
    "    # sau đó tạo một danh sách có tên là files bằng cách nối đường dẫn của các tệp tfrecords trong \n",
    "    # thư mục data_path của từ điển params với các số trong danh mục numbers . Các tệp tfrecord này chứa các văn bản \n",
    "    # từ nguồn OpenWebtext \n",
    "    files = [os.path.join(params['data_path'], \"openwebtext-newspaper_{}.tfrecord\" .format(str(i)))  for i in numbers]\n",
    "\n",
    "\n",
    "    # cuối cùng hàm này sẽ gọi một hàm khác có tên là bpe_text với các tham số là batch_size \n",
    "    # files, amount, iterations, stitch và batch . Hàm này se sẽ trả về một danh sách văn bản được\n",
    "    # mã hóa bằng phương pháp byte pair encoding \n",
    "    return bpe_text(params[\"batch_size\"], files, amount=params[\"n_ctx\"], iterations=params[\"iterations\"], stitch=2, batch=batch)\n",
    "\n",
    "\n",
    "\n",
    "# Thiết lập phương thức kết hợp lấy văn bản 70 % trong đó thiên về việc lấy văn bản dài hơn \n",
    "# tham số params là một từ điển chứa các thông tin cấu hình cho hàm , như batch_size , interation và data_path\n",
    "# tham số eval là một giá trị boolean xác định xem hàm này có được sử dụng cho mục đích \n",
    "# đánh giá hay không \n",
    "def openwebtext_logbiased(params , eval=False):\n",
    "    # lấy tất cả các tập văn bản từ 2 phương thức openwebtext(phương thức chứa taonf bộ văn bản) và \n",
    "    # openwebtext_long (phương thức chỉ lấy văn bản dài hơn 512 tokens) gán cho biến dataset\n",
    "    datasets = [openwebtext(params , eval=eval, batch=False), openwebtext_long(params, eval=eval, batch=False)]\n",
    "    # khởi tạo 1 danh sách với 2 tham số 0.3 và 0.7 \n",
    "    weights = [0.3,0.7]\n",
    "\n",
    "    # trích xuất từ danh sách nguồn dataset với 30 % văn bản và 70% văn bản dài hơn 512 tokens bằng phương thức \n",
    "    # tf.data.experimental.sample_fromdatasets gán nó cho biến dataset \n",
    "    dataset = tf.data.experimental.sample_from_datasets(datasets, weights=weights)\n",
    "    # sử dụng phương thức batch để chia danh sách văn bản thành các batch size = params[batch]\n",
    "    # với số lượng văn bản trong đối tượng dataset không chia hết cho kích thước batch , thì batch \n",
    "    # cuối cùng sẽ bị bỏ qua , do tham số drop_remaindr = True \n",
    "    #  sử dụng phương thức .prefetch để tìm nạp trước (tải trước )một số lượng văn bản \n",
    "    # với kích thước = params['interations'] để tăng tốc độ huấn luyện cho mô hình \n",
    "    dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True).prefetch(params[\"iterations\"])\n",
    "\n",
    "    # Trả về đối tượng dataset cuối cùng, đã được chia thành các batch và tải trước.\n",
    "    return dataset\n",
    "\n",
    "# A generic function to take in any tfrecords files filled with the correct BPE text\n",
    "# Thiết lập một hàm chung để nhận bất kỳ tệp tfrecord nào chứa văn bản BPE chính xác\n",
    "# từ điển params chứa các tham số batch_size , interations (số nguyên xác định số lượng văn bản đuọc nạp trước)\n",
    "#  stitch: Một số nguyên, xác định số lượng văn bản được ghép lại thành một văn bản dài hơn.\n",
    "\n",
    "def generic_text(params):\n",
    "    # dataset: Một danh sách các cặp ([files], weight), trong đó [files] là một danh sách\n",
    "    # các tệp tfrecords chứa các văn bản đã được mã hóa bằng phương pháp BPE,\n",
    "    # còn weight là một số thực xác định tỉ lệ lấy mẫu từ [files]\n",
    "    datasets = [bpe_text(params[\"batch_size\"], dataset[0], amount=params[\"n_ctx\"], iterations=params[\"iterations\"], stitch=params[\"stitch\"], batch=False)\n",
    "                for dataset in params[\"dataset\"]]\n",
    "    # lấy ra giá trị tham số dataset từ bộ từ điển params gán cho weights\n",
    "    weights = [dataset[1] for dataset in params[\"dataset\"]]\n",
    "\n",
    "    # tương tự như phương thức trên trích xuất danh sách các tệp văn bản ngẫu nhiên theo tỷ lệ \n",
    "    # weights từ danh sách các đối tượng dataset ban đầu \n",
    "    dataset = tf.data.experimental.sample_from_datasets(datasets, weights=weights)\n",
    "    # chia danh scahs dataset mới thành các batch và tải trước một lượng để tăng tốc độ huấn luyện \n",
    "    dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True).prefetch(params[\"iterations\"] * 2)\n",
    "\n",
    "    #  Trả về đối tượng dataset cuối cùng, đã được chia thành các batch và tải trước.\n",
    "    return dataset\n",
    "  \n",
    "\n",
    "# thiết lập phương thức xử lý nén văn bản \n",
    "def bpe_text(batch_size, files ,iterations, stitch, amount=1024, batch=True):\n",
    "    # tạo một đối tượng dataset là bằng căchs sử dụng phương tức tf.data.Dataset.from_tensor_slices với tham số files\n",
    "    # Hàm này sẽ tạo một đối tuowngj Dataset chứa các phần tử là tên các tệp tfrecord trong\n",
    "    # danh sách files \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(files)\n",
    "    # Biến đổi danh sách dataset bằng cách sử dụng methood apply với hàm tf.data.experiment.parallel_interleave\n",
    "    # có tham số tf.data.TFRecordDataaset. tạo ra một đối tượng dataset mói bằng cách đọc song song các tệp tfrecord\n",
    "    # với số lượng tệp tối đa = 4 và cho phéo sự không đồng nhất trong thứ tự các phần tử \n",
    "    dataset = dataset.apply(tf.data.experimental.parallel_interleave(tf.data.TFRecordDataset, cycle_length=4, sloppy=True))\n",
    "\n",
    "    # định nghĩa 1 hàm có tên _parse_function với tham số example_proto \n",
    "    # có chức năng giải mã một phần tử của đối tượng dataset, là một bản ghi tfrecord , \n",
    "    # thành 1 cặp text , length với text là văn bản được mã hóa bằng BPE và độ dài length củab nó\n",
    "    def _parse_function(example_proto):\n",
    "        # tạo mo0otj từ điển features với key = \"hash\" ; \"text\"\n",
    "        # từ điển này xác định cấu trúc của một bản ghi tfrecord gôm 2 trường với kiểu \n",
    "        # dữ liệu của nó \n",
    "        features = {\n",
    "            \"hash\" : tf.VarLenFeature(tf.string),\n",
    "            \"text\" : tf.VarLenFeature(tf.int64)\n",
    "        }\n",
    "        # sử dụng một hàm có tên là tf.parse_sigle_example với accs tham số là example_proto và feature\n",
    "        # để giải mã một bản ghi tfrecord thành một từ điển có tên là parsed_features, chứa 2 cặp khóa và\n",
    "        # giá trị : Một spareTensor là một đối tượng biểu diễn tensor thưa ó ba thuộc tính là indices, values và dense_shape.\n",
    "        parsed_features = tf.parse_sigle_example(example_proto, features)\n",
    "        \n",
    "        # trả về văn bản đã được phân tích và độ dài của văn bản \n",
    "        return parsed_features[\"text\"] , parsed_features[\"text\"].dense_shape[0]\n",
    "    \n",
    "    # Biến đổi phương thức dataset thành 1 phương thức mới bằng cách áp dụng hàm _parse_function \n",
    "    # cho danh sách dataset chứa các tệp tfrecord . với số lượng hàm được gọi song song là 1#\n",
    "    # điều này có nghĩa là mỗi phần tử dataset sẽ là 1 cặp text ; length chứ không phải 1 bản ghi \n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=1).shuffle(1000* stitch)\n",
    "\n",
    "\n",
    "    # stitch là một số nguyên xác định số lượng văn bản cần ghép lại \n",
    "    #  # Since samples can be less than the correct length, and TPUs don't like variable lengths, this function stitches together enough samples\n",
    "    # to have a text at least 1024 tokens long. For this to work the stitch parameter must be correctly tuned so that\n",
    "    # stitch * min(characters_in_text) >= amount\n",
    "    \"\"\"\n",
    "        Vì các mẫu có thể nhỏ hơn độ dài chính xác và TPU không thích độ dài thay đổi \n",
    "            nên chức năng này sẽ ghép đủ các mẫu lại với nhau để có văn bản dài ít nhất 1024 mã thông báo.\n",
    "            Để làm được việc này, tham số mũi khâu phải được điều chỉnh chính xác sao cho\n",
    "\n",
    "        stitch * min(characters_in_text) >= amount\n",
    "        Hàm này có chức năng là ghép các văn bản trong x thành một văn bản duy nhất, \n",
    "            với độ dài của mỗi văn bản được lưu trong y\n",
    "    \"\"\"\n",
    "    def _stitch_text(x, y):\n",
    "        # chuyển 1 spareTensor thành 1 tensor dày bằng hàm tf.spare.to_dense \n",
    "        # hàm này sẽ tạo một tensor mới có cùng kích thước với x nhưng không có phần tử thưa \n",
    "        x = tf.sparse.to_dense(x)\n",
    "        # đingh nghĩa 1 hàm get_x với tham số i  có chức annwg lấy ra một văn bản từ x với \n",
    "        # chỉ số i , và cắt bỏ các phần tử thưa bằng cách sử dụng hàm tf.gather \n",
    "        # Hàm này sẽ tạo ra một tensor mới, chứa các giá trị của x[i] tại các chỉ số trong tf.range(y[i]). \n",
    "        # Điều này có nghĩa là hàm này sẽ trả về một văn bản có độ dài bằng với giá trị của y[i].\n",
    "        def _get_x(i):\n",
    "            return tf.gather(x[i], tf.range(y[i]))\n",
    "\n",
    "        # gán giá trị của out = _get_x(0) điều này có nghĩa là out sẽ là văn bản đầu tiên trong x\n",
    "        out = _get_x(0)\n",
    "        # duyệt qua 1 danh sách từ 1 đến stitch - 1\n",
    "        for i in range(1, stitch):\n",
    "            # sử dụng hàm tf.concat để nói các tensor trong danh scahs theo chiều dọc và gán lại cho out \n",
    "            # Hàm này tạo ra một tensor mới chứa các giá trị của out một số 50256 vè kết quả của get_x(i)\n",
    "            # theo thứ tự đó , số 50256 là mã BPE của ký tự đặc biệt dùng để ngăn cách văn bản \n",
    "            out = tf.concat([out, [50256], _get_x(i)], axis=0) # text1<|endoftext|>text2\n",
    "            \n",
    "        # trả về danh sách out \n",
    "        return out\n",
    "\n",
    "    # Hack-y way to stitch together multiple texts\n",
    "    # chia dnah sách dataset ban đầu thành cách batch số lượng văn bản trong đối tượng dataset không chia hết cho kích thước batch , thì batch \n",
    "    # cuối cùng sẽ bị bỏ qua , do tham số drop_remaindr = True . sử dụng map để áp dụng phương thức _stitch_text cho toàn bộ các batch data\n",
    "    # để ghép các văn bản trong 1 batch thành 1 văn bản duy nhất cách nhau = ký tự đặc biệt với các đoạn độ dài = 1024 và số lượng phương thức \n",
    "    # được gọi song song tự thiết lập \n",
    "    dataset = dataset.batch(stitch, drop_remainder=True).map(_stitch_text, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Sample 1024(+1) tokens from the stitched together text\n",
    "    # Định nghĩa một hàm con có tên là _sample_text, với một tham số là x. \n",
    "    # Hàm này có chức năng là lấy mẫu một đoạn văn bản có độ dài 1024 tokens từ một văn \n",
    "    # bản dài hơn, và trả về một cặp (vals1, vals2), trong đó vals1 là đoạn văn bản được lấy mẫu, \n",
    "    # còn vals2 là đoạn văn bản kế tiếp của vals1.\n",
    "    def _sample_text(x):\n",
    "        # lấy ra kích thước x là một số nguyên cho biết số \n",
    "        # luowngj phần tử của x \n",
    "        s = tf.size(x)\n",
    "        # tạo một số ngẫu nhiên 0 -> s - (amount + 1) kiểu int32\n",
    "        r = tf.random.uniform([], maxval=s-(amount+1), dtype=tf.dtypes.int32)\n",
    "        # tạo 2 tensor r1  và r2 từ tham số r và r1 + 1 đến r+amoun và r+1 + amount \n",
    "        r1 = tf.range(r, r+amount)\n",
    "        r2 = tf.range(r+1, (r+1)+amount)\n",
    "\n",
    "        # định hình laị 2 tensor r1  , r2 với các giá trị không thay đổi nhưng kích thước = [amount]\n",
    "        # điều này làm cho trình biên dịch hài lòng vì TPU muốn có kích thước input cố định \n",
    "        r1 = tf.reshape(r1,[amount])\n",
    "        r2 = tf.reshape(r2, [amount])\n",
    "\n",
    "        # lấy ra các giá trị x theo chỉ số của danh sách r1 và r2\n",
    "        vals1 = tf.gather(x, r1)\n",
    "        vals2 = tf.gather(x, r2)\n",
    "        # Trả về một cặp (vals1, vals2), là hai đoạn văn bản có độ dài 1024 tokens, được lấy mẫu từ x.\n",
    "        return vals1, vals2\n",
    "    \n",
    "    # kiểm tả xem giá trị batch có rồn tại \n",
    "    if batch:\n",
    "        # nếu có ta áp dụng phuuwong thức  tf.data.experimental.map_and_batch\n",
    "        # với ahmf chức năng _sample_text để chia văn bản trong 1 batch thành các đoạn\n",
    "        # mỗi đoạn 1024 tokens , với số lượt kêu gọi phương thức song song = auto \n",
    "        dataset = dataset.apply(tf.data.experimental.map_and_batch(\n",
    "            map_func=_sample_text, batch_size=batch_size,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "            # nếu số lượng văn bản không chia hết cho batch thì batch cuối cùng sẽ bị bỏ qua \n",
    "            drop_remainder=True))\n",
    "        # sau đó lặp các đối tượng dataset liên tục và tìm nạp trước mộtsố lượng batch \n",
    "        # = interations \n",
    "        dataset = dataset.repeat().prefetch(iterations)\n",
    "\n",
    "    # nếu batch không tồn tại \n",
    "    else:\n",
    "        dataset = dataset.map(_sample_text, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of text\n",
    "def gpt2_pred_input(params, text=None):\n",
    "    # import encoder mã hóa từ GPT2\n",
    "    from models.gpt2 import encoder\n",
    "    # mã hóa dường đãn encoder_path Hàm này sẽ trả về một đối tượng của lớp encoder.Encoder, \n",
    "    # là một bộ mã hóa BPE được khởi tạo từ một tệp json chứa từ vựng và các quy tắc mã hóa.\n",
    "    enc = encoder.get_encoder(params[\"encoder_path\"])\n",
    "    # sử dụng bộ mã hóa BPE để mã hóa văn bản \n",
    "    tokens = enc.encode(text)\n",
    "    # kiểm tra xem số lượng token trong danh sách có > 1024 \n",
    "    if len(tokens) > 1024:\n",
    "        # nếu lớn hơn ta chia lại danh sách \n",
    "        tokens = tokens[:1024]\n",
    "    # Tạo một tensor có tên là t, bằng cách sử dụng một hàm có tên là tf.broadcast_to,\n",
    "    # với các tham số là tokens và [params[\"batch_size\"], len(tokens)]\n",
    "    #  Hàm này sẽ tạo ra một tensor mới, có cùng giá trị với tensor đầu tiên, nhưng có kích thước được xác định bởi tensor thứ hai.\n",
    "    #  tensor t sẽ có kích thước là params[\"batch_size\"] nhân với len(tokens), và mỗi hàng của tensor t sẽ chứa danh sách tokens.\n",
    "    t = tf.broadcast_to(tokens, [params[\"batch_size\"], len(tokens)])\n",
    "    # tạo ra một đối tượng dataset chứa một phần tử duy nhất là tensor t.\n",
    "    dataset = tf.data.Dataset.from_tensors(t)\n",
    "    # Trả về đối tượng dataset, là một tập hợp các tokens đã được mã hóa bằng bộ mã hóa BPE của GPT-2.\n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
