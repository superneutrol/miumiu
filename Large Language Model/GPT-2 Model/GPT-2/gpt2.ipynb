{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np \n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập phương thức get_shape_list nhận đầu vào là 1 tensor \n",
    "# có chức năng lấy ra hình dạng của 1 tensor mong muốn \n",
    "def shape_list(x):\n",
    "    \"\"\"Xử lý hình dạng động trong tensor một cach dõ ràng.\"\"\"\n",
    "    # sử dụng hàm shape.as_list để trả về 1 danh sách chứa các giá trị là kích thước \n",
    "    # của tensor \n",
    "    static = x.shape.as_list()\n",
    "    # lấy ra kích thước tensor x sử dụng hàm tf.shape \n",
    "    dyamic = tf.shape(x)\n",
    "    # trả về kết quả là danh scahs các chiều của tensor đầu vào theo chỉ số của danh sách static \n",
    "    return [dyamic[i] if s is None else s for i , s in enumerate(static)]\n",
    "\n",
    "\n",
    "# Thiết lập phương thức softmax là một hàm soft max\n",
    "# nhận đầu vào là tensor x và chỉ số axis =-1\n",
    "# softmax(x) = [exp(x_i) / Σ_j exp(x_j)]_i\n",
    "def softmax(x, axis=-1):\n",
    "    # trừ giá trị của tensor x cho giá trị lớn nhất của tensor này theo chiều cuối \n",
    "    # để tránh chàn số khi tính mũ kết quả được lưu vào x \n",
    "    # với keepdims = True tức là giữ lại kích thước ban đầu \n",
    "    x = x - tf.reduce_max(x , axis=axis, keepdims=True)\n",
    "    # Thực hiện tính e ^x để các giá trị thực thành giá trị dương kết quả được lưu vào ẽ\n",
    "    ex = tf.exp(x)\n",
    "\n",
    "    # trả về kết quả là phép chia giá trị ex cho tổng các phần tử của ex theo chiều axis=-1 \n",
    "    # để biến các giá trị dương thành các giá trị có tổng bằng 1\n",
    "    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "# Thiết lập phương thức gelu là hàm kích hoạt gelu \n",
    "# có chức năng biến đổi đầu vào thành đầu ra cho tensor tham số \n",
    "# gelu(x) = x * 0.5 * (1.0 + erf(x / sqrt(2.0)))\n",
    "def gelu(x):\n",
    "    # trả về kết quả là phép biến đổi qua công thức trên của tensor x \n",
    "    # hàm pow có chức năng xây dựng phép tính mũ lũy thừa ở đây là mũ 3 cho tất cả các giá trị \n",
    "    # trong tensor x \n",
    "    return 0.5 * x *(1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044175 * tf.pow(x,3))))\n",
    "\n",
    "# Thiêts lập phương thức norm là hàm chức năng layernormalization \n",
    "# được sử dụng để chuẩn hóa lớp , luồng dữ liệu \n",
    "# phương thức này nhận đầu vào x là 1 tensor , scope là 1 chuỗi xác định phạm vi \n",
    "# biến của tensorflow , params là một từ điển chứa các thông số cấu hình .. \n",
    "def norm(x , scope , * , axis=-1, epsilon=1e-5, params=None):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    # tạo một biến phạm vị với tham số phạm vi scope là 1 chuỗi xác định phạm vi \n",
    "    with tf.variable_creator_scope(scope):\n",
    "        # từ tensor đầu vào lấy ra kích thước theo chiều cuối cùng gán cho n_state \n",
    "        n_state = x.shape[-1].value\n",
    "        # kiểm tra xem kiểu dữ liệu của khóa \"precision\" trong từ điển params có phảo bfloat16\n",
    "        if params[\"percision\"] == \"bfloat16\":\n",
    "            # nếu đúng ta tạo 2 tensor g và b có kích thước = n_state với tham số \n",
    "            # khởi tạo = 1 , 0 . Biến g , b là các tham số của phép biến đổi chéo  tuyến tính \n",
    "            # có thể được học và điều chỉnh trong quá trình huấn luyện . value 1 , 0 cho phép \n",
    "            # giữu nguyên phân bố của tensor x sau khi chuẩn hóa \n",
    "            g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1, dtype=tf.bfloat16), dtype=tf.bfloat16)\n",
    "            b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0, dtype=tf.bfloat16), dtype=tf.bfloat16)\n",
    "        # trường hợp còn lại \n",
    "        else:\n",
    "            # ta tạo b và g theo mặc định \n",
    "            g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
    "            b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        # tính trung bình của tensor x theo chiều axis = - 1 gán cho u \n",
    "        u = tf.reduce_mean(x , axis=axis, keepdims=True)\n",
    "        # sau đó tính trung bình tensor x với các giá trị - u ) ^2\n",
    "        # và gán kết quả cho s\n",
    "        s = tf.reduce_mean(tf.square(x - u), axis=axis, keepdims=True)\n",
    "\n",
    "        # nhân tensor x với tensor g rồi sau đó cộng tensor b \n",
    "        #  nhân tensor x với biến g và cộng với biến b để thực hiện phép biến đổi tuyến tính chéo\n",
    "        x = x*g + b\n",
    "        return x\n",
    "    \n",
    "\n",
    "# thiết lập phương thức split_states nhận đầu vào là tensor x \n",
    "# và n là một giá trị nguyên mong muốn \n",
    "def split_states(x , n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1] / n]\"\"\"\n",
    "    \"\"\"Định hình lại kích thước cuôi cùng của x nằm trong [n , x.shape[-1] / n]\"\"\"\n",
    "    # sử dụng hàm shape_list để lấy ra kích thước của tensor x dưới dạng list \n",
    "    # gán nó cho start và ,m ssuwr dụng dấu sao để nén kết quả của phép shape_list \n",
    "    # cho biến start trừ đi chiều cuối cùng được gán cho m \n",
    "    *start , m  = shape_list(x)\n",
    "    # trả về kết quả cuối cùng của x sau khi được biến đổi \n",
    "    # nối list start với 1 list = [n , m // n ] thành 1 list mới \n",
    "    # các tham số trong list này sẽ là hình dạng mới cho tensor x \n",
    "    return tf.reshape(x, start + [n , m//n])\n",
    "\n",
    "# thiết lập phương thức marge để hợp nhất 2 chiều cuối cùng của tensor thành 1 chiều duy nhất\n",
    "def merge_states(x):\n",
    "\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    # lấy ra kích thước của tensor dưới dạng list gán nó cho biến nén *start nếu \n",
    "    # x có 4 chiều thì start  sẽ có 2 chiều và 2 chiều cuối gán cho a và b \n",
    "    *start, a, b = shape_list(x)\n",
    "    # trả về 1 tensor mới với shape bằng kết quả phép cộng danh sách start + [a*b]\n",
    "    # khi đó x sẽ alf 1 tensor với số chiều giảm đi 1\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def dropout(x, pdrop, train):\n",
    "    if train and pdrop > 0:\n",
    "        x = tf.nn.dropout(x, rate=pdrop)\n",
    "    return x\n",
    "\n",
    "# Thiết lập phương thức Conv1d Là ,một lớp tích chập \n",
    "# phương thức này nhận các tham số tensor x , chuỗi ký tự chỉ định phạm vi scope , \n",
    "# nf số nguyên  hỉ định số lượng kênh của tín hiệu đầu ra, hay còn gọi là số lượng bộ lọc (filters) của phép tích chập.\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02, params=None, scale=False):\n",
    "    # kiểm tra xem có tồn tại khóa \"scale_by_depth\" và scale có tồn tại \n",
    "    if params[\"scale_by_depth\"] and scale: # Scale by sqrt(num_layers), only happens at the final projection before a res block output\n",
    "        # nhân giá trị độ lệnh phân phối chuẩn với tỷ lệ 1/ tham số khóa layer trong từ điển params (là 1 giá trị nguyên n_layer của model)\n",
    "         w_init_stdev = w_init_stdev * (1. / math.sqrt(params[\"n_layer\"]))\n",
    "    # nếu có tồn tại khóa \"scale_by_in\" = True\n",
    "    if params[\"scale_by_in\"]: # Scale by sqrt(num_input_features)\n",
    "        # tương tự như trên ở đây 1 / căn bậc 2 kích thước tensor x theo chiều cuối \n",
    "        w_init_stdev = w_init_stdev * (1. / math.sqrt(x.shape[-1].value))\n",
    "\n",
    "    # khởi tạo một biến phạm vi với chuỗi ký tự chỉ định phạm vi scope \n",
    "    with tf.variable_scope(scope):\n",
    "        # lấy ra danh sách hình dạng tensor x gán chiều cuối cho nx và các chiều còn lại \n",
    "        # cho *start \n",
    "        *start, nx = shape_list(x)\n",
    "        # kiểm tra xem giá trị của khóa \"precision\" trong từ điển params có phải kiểu dữ liệu bfloat16\n",
    "        if params[\"precision\"] == \"bfloat16\":\n",
    "            # nếu có khởi atoj 2 tensor w và b \n",
    "            w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev, dtype=tf.bfloat16), dtype=tf.bfloat16)\n",
    "            b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0, dtype=tf.bfloat16), dtype=tf.bfloat16)\n",
    "        \n",
    "        # trường hợp còn lại khởi tạo w và b theo mặc định \n",
    "        else:\n",
    "            w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
    "            b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
    "        # sử dụng hàm tf.matmul để thực hiện phép tích chập và sau đó reshape lại hình dạng mong muốn \n",
    "        # và gán cho c kết quả là tensor mới có dạng như tensor x nhưng khác chiều cuối cùng \n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng mặt nạ attention \n",
    "# dựa trên kiến trúc masked self-attention transformer \n",
    "# nhận các tham số nd , ns là kích thước 2 chiều của tensor , dtype là kiểu dữ liệu \n",
    "def attention_mask(nd, ns, * , dtype):\n",
    "    # Tạo 1 ma trận i là 1 dãy số  0-> nd -1 i là 1 ma trận mà có chiều thứ 2 = 1\n",
    "    # với mỗi phần tủ của i trở thành 1 hàng duy nhất \n",
    "    i = tf.range(nd)[:None]\n",
    "    # và tương tự ma trận j 0 -> ns-1 nhưng có 1 chiều duy nhất \n",
    "    j = tf.range(ns)\n",
    "\n",
    "    # so sánh i và j để tạo ma trận boolean m nơi mỗi phần tử m[i, j] là True nếu \n",
    "    # điều kiện i >= j - ns + nd ngược lại là False ma trận có dạng như sau \n",
    "    # [[True, False, False, False, False],\n",
    "    # [True, True, False, False, False],\n",
    "    # [True, True, True, False, False]]\n",
    "    m = i >= j - ns + nd \n",
    "    # sử dụng tf.cast để áp dụng dtype cho ma trận m sau đó trả về kết quả \n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "# Xây dựng bộ phận self-attention \n",
    "# nhận đầu vào tensor đầu vào x ,chuỗi ký tự chỉ định phạm vi các biến trong Tensorflow \n",
    "# n_state: Có thể là số lượng trạng thái hoặc đặc trưng mà mô hình sẽ học (ví dụ: kích thước của vector attention).\n",
    "# past: Đây có thể là một tensor chứa thông tin từ các bước trước đó của mô hình, cho phép mô hình sử dụng thông tin từ quá khứ để đưa ra quyết định hiện tại.\n",
    "# features = embedding dim \n",
    "def attn(x ,scope, n_state, * , past, params, train=False):\n",
    "    # kiểm tra xem tensor đầu vào x có phải có 3 chiều \n",
    "    # nếu không trương trình sẽ bị ngắt \n",
    "    assert x.shape.ndims == 3 #Should be [batch_size , sequence_length, features]\n",
    "    # Sau đó kiểm tra thêm điều kiện \n",
    "    assert n_state % params[\"n_head\"] == 0\n",
    "    # kiểm tra xem tensor chứa thông tin trước đó của mô hình past có tồn tại \n",
    "    if past is not None : # nếu như past không = None \n",
    "        # kiểm tra điều kiện về hình dạng của tenor này \n",
    "        assert past.shape.ndims == 5 # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    # thiết lập phương thức split_head để biến đổi tensor đầu vào thành 1 tensor 4 chiều \n",
    "    def split_heads(x):\n",
    "        # features = embedding_dim \n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(split_states(x, params[\"n_head\"]), [0, 2, 1, 3])\n",
    "\n",
    "    # thiết lập phương thức merge_head có chức năng ngược lại với phương thưcs split_head \n",
    "    def merge_heads(x):\n",
    "        # Reverse of split heads [batch, sequence, embedding_dim]\n",
    "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "    \n",
    "    # Xây dựng phương thức masked_attention \n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        # lấy ra kích thước của tensor w \n",
    "        _, _, nd , ns = shape_list(w)\n",
    "        # gọi đến phương thức attention_mask để xây dựng mặt nạ attention \n",
    "        b = attention_mask(nd, ns, dtype=w.type)\n",
    "        # định hình lại tensor shape = [1 , 1 , seq_length , seq_length] để phù hợp với tensor \n",
    "        # đâu vào \n",
    "        b = tf.reshape(b , [1 , 1 , nd , ns])\n",
    "        # nhân tensor w với b sau đó trừ đi 1 giá trị 1e-9 * 1 -b theo công thức gốc từ bài báo \n",
    "        # transformer \n",
    "        w = w*b - tf.cast(1e-10,w.type)* (1 - b)\n",
    "        return w\n",
    "    \n",
    "    # xây dựng lớp kiến trúc multihead attention \n",
    "    def multihead_attention(q, k ,v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features(head_dims)]\n",
    "        # theo công thức tính q * k.T\n",
    "        # w shape = [batch_size , num_heads , seq_length, num_per_head]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        # Nhân w với căn bậc 2 nghịch đảo của tensor v theo kích thước chiều cuối cùng \n",
    "        # shape shape = [batch_Size , num_heads , seq_length, seq_length]\n",
    "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.type))\n",
    "\n",
    "        # áp dụng mặt nạ attention lên tensor w \n",
    "        w = mask_attn_weights(w)\n",
    "        # áp dụng hàm softmax cho tensor này \n",
    "        w = softmax(w)\n",
    "\n",
    "        # thêm attention_dropout \n",
    "        w = dropout(w, params[\"attn_dropout\"], train)\n",
    "\n",
    "        # tính toán attn score * v\n",
    "        a = tf.matmul(w, v)\n",
    "        # return a\n",
    "        return a \n",
    "    \n",
    "    # xây dựng một biến phạm vi được chỉ định bởi chuỗi ký tự scope \n",
    "    with tf.variable_creator_scope(scope):\n",
    "        # sử dụng phép tham chiếu văn bản với lớp tích chập conv1d\n",
    "        c = conv1d(x, 'c_attn', n_state*3 , params=params)\n",
    "        # áp dụng hàm spilit_head cho toàn bộ q, k ,v \n",
    "        # [batch_size , seq_length,3, embedding_dim]\n",
    "        q , k , v = map(split_heads, tf.split(c, 3 , axis=2))\n",
    "        # sử dụng hàm stack để tách 2 tensor k , v theo chiều thứ 2 \n",
    "        present = tf.stack([k, v], axis=1) \n",
    "        # kiểm tra xem trạng thái biểu diễn trước đó pats cps tồn tại 0 \n",
    "        if past is not None: \n",
    "            # tách qp và qv từ past\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            # nối chúng với vector k , v tương ứng \n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "\n",
    "        # áp dụng phép multihead_attention cho q , k , v\n",
    "        a = multihead_attention(q, k ,v)\n",
    "        # chuyển vị kích thước của a giảm kích thước chiểu cuối cùng \n",
    "        # TĂNG chiều [-2] lên \n",
    "        a = merge_heads(a)\n",
    "        # áp dụng phép tham chiếu văn bản \n",
    "        a = conv1d(a, 'c_proj', n_state, params=params)\n",
    "        a = dropout(a, params[\"res_dropout\"], train)\n",
    "        # a shape = [batch_size, seq_length, hiddent_sizr]\n",
    "        return a, present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thiết lập lớp đa xử lý trong kiến trúc Transformer original \n",
    "def mlp(x, scope, n_state, *, params, train=False):\n",
    "    # khởi tạo một biến phạm vi được chỉ định bởi 1 chuỗi ký tự\n",
    "    with tf.variable_scope(scope):\n",
    "        # lấy ra hình kích thước chiều cuối cùng của tensor đầu vào\n",
    "        nx = x.shape[-1].value\n",
    "        # áp dụng hàm kích hoạt gelu lên đầu ra của lớp tham chiếu conv1d cho tensor x\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state, params=params))\n",
    "        # áp dụng thêm 1 lớp conv1d \n",
    "        h2 = conv1d(h, 'c_proj', nx, params=params, scale=True)\n",
    "        #  dropout_mlp \n",
    "        h2 = dropout(h2, params[\"res_dropout\"], train)\n",
    "        return h2\n",
    "\n",
    "# Xây dựng khối xử lý toàn phần tương tự như MLP \n",
    "def block(x, scope, *, past, params, train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = x.shape[-1].value\n",
    "        # áp dụng lớp norm cho đầu ra lớp chú ý \n",
    "        a, present = attn(norm(x, 'ln_1', params=params), 'attn', nx, past=past, params=params, train=train)\n",
    "        x = x + a\n",
    "        # áp dụng lớp mlp cho kết quả của đầu ra normlization layer\n",
    "        m = mlp(norm(x, 'ln_2', params=params), 'mlp', nx*4, params=params, train=train)\n",
    "        x = x + m\n",
    "        # cộng đầu vào với đầu ra \n",
    "        return x, present\n",
    "\n",
    "# thiết lập phương thức trả về 1 danh sách các giá trị batch_size , num_layer , 2 , num_head , seq_length, embedding_dim , num_head\n",
    "# là các giá trị tham số trong từ điển params \n",
    "def past_shape(*, params, batch_size=None, sequence=None):\n",
    "    return [batch_size, params[\"n_layer\"], 2, params[\"n_head\"], sequence, params[\"n_embd\"] // params[\"n_head\"]]\n",
    "\n",
    "\n",
    "# Xây dựng phương thức tiện ích dùng để thêm 1 trục mới vào tensor nguồn\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Thêm 1 trục mới vào kích thước cố định.\"\"\"\n",
    "    # chuyển đổi mảng value thành 1 tensor \n",
    "    value = tf.convert_to_tensor(value)\n",
    "    # lấy ra số lượng chiêu của tensor \n",
    "    ndims = value.shape.ndims \n",
    "    # Lặp lại tensor đã được mở rộng chiều (từ bước 3) theo mẫu được chỉ định trong danh sách (từ bước 4),\n",
    "    # tạo ra một tensor mới với trục mới được thêm vào có kích thước là size.\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "\n",
    "# thiết lạp phương thức tạo tensor vị chí cho mỗi token \n",
    "def positions_for(tokens , past_length):\n",
    "    # Lấy kích thước batch từ tensor tokens, tức là số lượng chuỗi trong batch.\n",
    "    batch_size = tf.shape(tokens)[0]\n",
    "    # Lấy số bước (số lượng token) từ tensor tokens, tức là độ dài của mỗi chuỗi.\n",
    "    nsteps = tf.shape(tokens)[1]\n",
    "    # Tạo một tensor vị trí cho mỗi bước, bắt đầu từ past_length. \n",
    "    # past_length có thể là độ dài của các token đã được xử lý trước đó \n",
    "    # (trong trường hợp của mô hình autoregressive như GPT-2),\n",
    "    # và tf.range(nsteps) tạo ra một dãy số từ 0 đến nsteps - 1.\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "    # Kết quả là một tensor mới, trong đó mỗi chuỗi trong batch có một dãy vị trí tương ứng, bắt đầu từ past_length và tăng dần theo mỗi bước\n",
    "\n",
    "\n",
    "def _assert_float_dtype(dtype):\n",
    "    if not dtype.is_floating:\n",
    "        raise ValueError(\"Expected floating point type, got %s.\" % dtype)\n",
    "    return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, params, labels=None, past=None, scope='model', reuse=False, train=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        if params[\"precision\"] == \"bfloat16\":\n",
    "            wpe = tf.get_variable('wpe', [params[\"n_ctx\"], params[\"n_embd\"]], # Position encoding\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01, dtype=tf.bfloat16), dtype=tf.bfloat16)\n",
    "            wte = tf.get_variable('wte', [params[\"n_vocab\"], params[\"n_embd\"]], # Text encoding\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02, dtype=tf.bfloat16), dtype=tf.bfloat16)\n",
    "\n",
    "        else:\n",
    "            wpe = tf.get_variable('wpe', [params[\"n_ctx\"], params[\"n_embd\"]], # Position encoding\n",
    "                                initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "            wte = tf.get_variable('wte', [params[\"n_vocab\"], params[\"n_embd\"]], # Text encoding\n",
    "                                initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "\n",
    "        wpe = dropout(wpe, params[\"embed_dropout\"], train)\n",
    "        wte = dropout(wte, params[\"embed_dropout\"], train)\n",
    "\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * params[\"n_layer\"]\n",
    "        assert len(pasts) == params[\"n_layer\"]\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, params=params, train=train)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f', params=params)\n",
    "\n",
    "        h_flat = tf.reshape(h, [batch*sequence, params[\"n_embd\"]])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, params[\"n_vocab\"]])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
