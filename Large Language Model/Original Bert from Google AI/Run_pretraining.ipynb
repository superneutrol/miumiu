{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os \n",
    "import tensorflow as tf \n",
    "from import_ipynb import * \n",
    "import Bert_model \n",
    "import Optimization \n",
    "\n",
    "flags = tf.flags \n",
    "\n",
    "FLAGS = flags.FLAGS \n",
    "\n",
    "# Required parametters \n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"input_file\", None,\n",
    "    \"Input TF example files (can be a glob or comma separated).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 128,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded. Must match data generation.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_predictions_per_seq\", 20,\n",
    "    \"Maximum number of masked LM predictions per sequence. \"\n",
    "    \"Must match data generation.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_steps\", 100000, \"Number of training steps.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_warmup_steps\", 10000, \"Number of warmup steps.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_eval_steps\", 100, \"Maximum number of eval steps.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    tf.logging.info(\"*** Features ***\")\n",
    "    for name in sorted(features.keys()):\n",
    "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "    next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    (masked_lm_loss,\n",
    "     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "         bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "         masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "\n",
    "    (next_sentence_loss, next_sentence_example_loss,\n",
    "     next_sentence_log_probs) = get_next_sentence_output(\n",
    "         bert_config, model.get_pooled_output(), next_sentence_labels)\n",
    "\n",
    "    total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n",
    "                    masked_lm_weights, next_sentence_example_loss,\n",
    "                    next_sentence_log_probs, next_sentence_labels):\n",
    "        \"\"\"Computes the loss and accuracy of the model.\"\"\"\n",
    "        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n",
    "                                         [-1, masked_lm_log_probs.shape[-1]])\n",
    "        masked_lm_predictions = tf.argmax(\n",
    "            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
    "        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n",
    "        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n",
    "        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n",
    "        masked_lm_accuracy = tf.metrics.accuracy(\n",
    "            labels=masked_lm_ids,\n",
    "            predictions=masked_lm_predictions,\n",
    "            weights=masked_lm_weights)\n",
    "        masked_lm_mean_loss = tf.metrics.mean(\n",
    "            values=masked_lm_example_loss, weights=masked_lm_weights)\n",
    "\n",
    "        next_sentence_log_probs = tf.reshape(\n",
    "            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n",
    "        next_sentence_predictions = tf.argmax(\n",
    "            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n",
    "        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n",
    "        next_sentence_accuracy = tf.metrics.accuracy(\n",
    "            labels=next_sentence_labels, predictions=next_sentence_predictions)\n",
    "        next_sentence_mean_loss = tf.metrics.mean(\n",
    "            values=next_sentence_example_loss)\n",
    "\n",
    "        return {\n",
    "            \"masked_lm_accuracy\": masked_lm_accuracy,\n",
    "            \"masked_lm_loss\": masked_lm_mean_loss,\n",
    "            \"next_sentence_accuracy\": next_sentence_accuracy,\n",
    "            \"next_sentence_loss\": next_sentence_mean_loss,\n",
    "        }\n",
    "\n",
    "      eval_metrics = (metric_fn, [\n",
    "          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n",
    "          masked_lm_weights, next_sentence_example_loss,\n",
    "          next_sentence_log_probs, next_sentence_labels\n",
    "      ])\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          eval_metrics=eval_metrics,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      raise ValueError(\"Only TRAIN and EVAL modes are supported: %s\" % (mode))\n",
    "\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,\n",
    "                         label_ids, label_weights):\n",
    "  \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "  input_tensor = gather_indexes(input_tensor, positions)\n",
    "\n",
    "  with tf.variable_scope(\"cls/predictions\"):\n",
    "    # We apply one more non-linear transformation before the output layer.\n",
    "    # This matrix is not used after pre-training.\n",
    "    with tf.variable_scope(\"transform\"):\n",
    "      input_tensor = tf.layers.dense(\n",
    "          input_tensor,\n",
    "          units=bert_config.hidden_size,\n",
    "          activation=modeling.get_activation(bert_config.hidden_act),\n",
    "          kernel_initializer=modeling.create_initializer(\n",
    "              bert_config.initializer_range))\n",
    "      input_tensor = modeling.layer_norm(input_tensor)\n",
    "\n",
    "    # The output weights are the same as the input embeddings, but there is\n",
    "    # an output-only bias for each token.\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\",\n",
    "        shape=[bert_config.vocab_size],\n",
    "        initializer=tf.zeros_initializer())\n",
    "    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    label_ids = tf.reshape(label_ids, [-1])\n",
    "    label_weights = tf.reshape(label_weights, [-1])\n",
    "\n",
    "    one_hot_labels = tf.one_hot(\n",
    "        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "    # The `positions` tensor might be zero-padded (if the sequence is too\n",
    "    # short to have the maximum number of predictions). The `label_weights`\n",
    "    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n",
    "    # padding predictions.\n",
    "    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "    numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "    denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "    loss = numerator / denominator\n",
    "\n",
    "  return (loss, per_example_loss, log_probs)\n",
    "\n",
    "\n",
    "def get_next_sentence_output(bert_config, input_tensor, labels):\n",
    "  \"\"\"Get loss and log probs for the next sentence prediction.\"\"\"\n",
    "\n",
    "  # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n",
    "  # \"random sentence\". This weight matrix is not used after pre-training.\n",
    "  with tf.variable_scope(\"cls/seq_relationship\"):\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\",\n",
    "        shape=[2, bert_config.hidden_size],\n",
    "        initializer=modeling.create_initializer(bert_config.initializer_range))\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
    "\n",
    "    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    labels = tf.reshape(labels, [-1])\n",
    "    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, per_example_loss, log_probs)\n",
    "\n",
    "\n",
    "def gather_indexes(sequence_tensor, positions):\n",
    "  \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "  batch_size = sequence_shape[0]\n",
    "  seq_length = sequence_shape[1]\n",
    "  width = sequence_shape[2]\n",
    "\n",
    "  flat_offsets = tf.reshape(\n",
    "      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "  flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "  flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
    "                                    [batch_size * seq_length, width])\n",
    "  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "  return output_tensor\n",
    "\n",
    "\n",
    "def input_fn_builder(input_files,\n",
    "                     max_seq_length,\n",
    "                     max_predictions_per_seq,\n",
    "                     is_training,\n",
    "                     num_cpu_threads=4):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    name_to_features = {\n",
    "        \"input_ids\":\n",
    "            tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        \"input_mask\":\n",
    "            tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        \"segment_ids\":\n",
    "            tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        \"masked_lm_positions\":\n",
    "            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "        \"masked_lm_ids\":\n",
    "            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "        \"masked_lm_weights\":\n",
    "            tf.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "        \"next_sentence_labels\":\n",
    "            tf.FixedLenFeature([1], tf.int64),\n",
    "    }\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    if is_training:\n",
    "      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=len(input_files))\n",
    "\n",
    "      # `cycle_length` is the number of parallel files that get read.\n",
    "      cycle_length = min(num_cpu_threads, len(input_files))\n",
    "\n",
    "      # `sloppy` mode means that the interleaving is not exact. This adds\n",
    "      # even more randomness to the training pipeline.\n",
    "      d = d.apply(\n",
    "          tf.contrib.data.parallel_interleave(\n",
    "              tf.data.TFRecordDataset,\n",
    "              sloppy=is_training,\n",
    "              cycle_length=cycle_length))\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "    else:\n",
    "      d = tf.data.TFRecordDataset(input_files)\n",
    "      # Since we evaluate for a fixed number of steps we don't want to encounter\n",
    "      # out-of-range exceptions.\n",
    "      d = d.repeat()\n",
    "\n",
    "    # We must `drop_remainder` on training because the TPU requires fixed\n",
    "    # size dimensions. For eval, we assume we are evaluating on the CPU or GPU\n",
    "    # and we *don't* want to drop the remainder, otherwise we wont cover\n",
    "    # every sample.\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            num_parallel_batches=num_cpu_threads,\n",
    "            drop_remainder=True))\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "def _decode_record(record, name_to_features):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "  # So cast all int64 to int32.\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.to_int32(t)\n",
    "    example[name] = t\n",
    "\n",
    "  return example\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  if not FLAGS.do_train and not FLAGS.do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "  input_files = []\n",
    "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "\n",
    "  tf.logging.info(\"*** Input Files ***\")\n",
    "  for input_file in input_files:\n",
    "    tf.logging.info(\"  %s\" % input_file)\n",
    "\n",
    "  tpu_cluster_resolver = None\n",
    "  if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master=FLAGS.master,\n",
    "      model_dir=FLAGS.output_dir,\n",
    "      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "          num_shards=FLAGS.num_tpu_cores,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=FLAGS.init_checkpoint,\n",
    "      learning_rate=FLAGS.learning_rate,\n",
    "      num_train_steps=FLAGS.num_train_steps,\n",
    "      num_warmup_steps=FLAGS.num_warmup_steps,\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size=FLAGS.train_batch_size,\n",
    "      eval_batch_size=FLAGS.eval_batch_size)\n",
    "\n",
    "  if FLAGS.do_train:\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "    train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        max_predictions_per_seq=FLAGS.max_predictions_per_seq,\n",
    "        is_training=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\n",
    "\n",
    "  if FLAGS.do_eval:\n",
    "    tf.logging.info(\"***** Running evaluation *****\")\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
    "\n",
    "    eval_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        max_predictions_per_seq=FLAGS.max_predictions_per_seq,\n",
    "        is_training=False)\n",
    "\n",
    "    result = estimator.evaluate(\n",
    "        input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)\n",
    "\n",
    "    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n",
    "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "      tf.logging.info(\"***** Eval results *****\")\n",
    "      for key in sorted(result.keys()):\n",
    "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  flags.mark_flag_as_required(\"input_file\")\n",
    "  flags.mark_flag_as_required(\"bert_config_file\")\n",
    "  flags.mark_flag_as_required(\"output_dir\")\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
