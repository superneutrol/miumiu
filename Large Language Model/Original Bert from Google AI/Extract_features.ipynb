{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from import_ipynb import * \n",
    "import Tokenization\n",
    "import Bert_model \n",
    "import collections \n",
    "import re \n",
    "import json \n",
    "import tensorflow as tf  \n",
    "import codecs \n",
    "\n",
    "# sử dụng module tf.flags để định nghĩa các cờ dòng lệnh cho trương trinh . \n",
    "# Các cờ dòng lệnh là những tham số có thể được lan truyền vào khi chạy trương trình từ terminal \n",
    "# hoặc một môi trường khác . Giúp điều chỉnh các tham số của mô hình mà không cần sửa đổi mã nguồn \n",
    "flags = tf.flags \n",
    "FLAGS = flags.FLAGS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện định nghĩa các cở lệnh với \n",
    "# kiểu dtype , tham số , giá trị và mô tả của tham số \n",
    "\n",
    "# 1 : định nghĩa một cờ lệnh kiểu string cho tham số input_file gán giá trị = None và mô tả None\n",
    "flags.DEFINE_string(\"input_file\", None , \"\")\n",
    "\n",
    "# 2 : định nghĩa một cò lệnh kiểu string cho tham số output_file gán giá trị = None và mô tả .\n",
    "flags.DEFINE_string(\"output_file\", None, \"\")\n",
    "\n",
    "# 3 : định nghĩa một cò lệnh kiểu string cho tham số layers gán giá trị = None và mô tả .\n",
    "flags.DEFINE_string(\"layers\", \"-1,-2,-3,-4\", \"\")\n",
    "\n",
    "# 4 : định nghĩa một cò lệnh kiểu string cho tham số bert_config_file gán giá trị = None và mô tả .\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "# 5 : định nghĩa một cò lệnh kiểu integer cho tham số max_seq_length gán giá trị = 128 và mô tả của nó.\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 128,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "# 6 : định nghĩa một cò lệnh kiểu string cho tham số init_checkpoint gán giá trị = None và mô tả của nó.\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "# 7 : định nghĩa một cò lệnh kiểu integer cho tham số vocab_file gán giá trị = None và mô tả của nó.\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "# 8 : định nghĩa một cò lệnh kiểu bool cho tham số do_lower_cáe gán giá trị = True và mô tả của nó.\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "# 9 : định nghĩa một cò lệnh kiểu integer cho tham số batch_size gán giá trị = 32 và mô tả của nó.\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for predictions.\")\n",
    "\n",
    "# 10 : định nghĩa một cò lệnh kiểu bool cho tham số use_tpu gán giá trị = False và mô tả của nó.\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "# 11 : định nghĩa một cò lệnh kiểu string cho tham số master gán giá trị = None và mô tả của nó.\n",
    "flags.DEFINE_string(\"master\", None,\n",
    "                    \"If using a TPU, the address of the master.\")\n",
    "\n",
    "# 12 : định nghĩa một cò lệnh kiểu integer cho tham số num_tpu_cores gán giá trị = 8 và mô tả của nó.\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
    "\n",
    "# 13 : định nghĩa một cò lệnh kiểu bool cho tham số  gán giá trị = False và mô tả của nó.\n",
    "flags.DEFINE_bool(\n",
    "    \"use_one_hot_embeddings\", False,\n",
    "    \"If True, tf.one_hot will be used for embedding lookups, otherwise \"\n",
    "    \"tf.nn.embedding_lookup will be used. On TPUs, this should be True \"\n",
    "    \"since it is much faster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng một lớp là InputExample để định dạng cho  các mẫu đầu vào \n",
    "# gồm câu a, câu b và chỉ số id duy nhất của các tokens hoặc của câu \n",
    "class InputExample(object):\n",
    "    # Thiết lập phương thức khởi tạo và định nghĩa các tham số \n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        # gán các tham số cho các thuộc tính \n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "\n",
    "# Tương tự xây dựng một lớp là Input_features để định dnagj cho \n",
    "# một tập các đặc trưng riêng lẻ của dữ liệu \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    # thiết lập phương thức khởi tạo và định nghĩa các tham số cần sử dụng \n",
    "    def __init__(self, unique_id , tokens, input_ids,\n",
    "                input_mask , input_type_ids):\n",
    "        # gán các tham số cho các thuộc tính \n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens \n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask \n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "    \n",
    "\n",
    "def input_fn_builder(features,seq_length):\n",
    "    \"\"\"Create an 'ionput_fn' clousure to be passed to TPUEstimator.\n",
    "        Tạo một bao đóng input_fn để chuyển tới TPUestimator.\n",
    "    \"\"\"\n",
    "    # khởi tạo 4 danh sách \n",
    "    all_unique_ids = []\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_input_type_ids = []\n",
    "\n",
    "    # sau đó duyệt qua danh sách chứa các đặc trưng \n",
    "    # và thêm các đặc trưng tương ứng từ features vào danh sách tương ứng \n",
    "    for feature in features: \n",
    "        all_unique_ids.append(feature.unique_id)\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_input_type_ids.append(feature.input_type_ids)\n",
    "\n",
    "    # xây dựng một phương thức con bên trong input_fb_build \n",
    "    def input_fn(params):\n",
    "        \"\"\"Chức năng đầu vào thực tế . The actual input function.\"\"\"\n",
    "        batch_size = params['batch_size']\n",
    "\n",
    "        # lấy ra số lượng mẫu = độ dài của dnah sách features\n",
    "        num_examples = len(features)\n",
    "        \n",
    "        # Đây là mục đích Demo và không mở rộng ra các tập dữ liệu lớpk . \n",
    "        # không sử dụng Dataset.from_generator() bởi vì nó sử dụng tf.py_func\n",
    "        # không tương thích với TPU \n",
    "\n",
    "        # khởi tạo một biến d và gán nó bằng một định dạng dataset Tf.data.Datase.from_tensor_slice\n",
    "        # từ các lát cắt tensor [unique_ids , input_ids , input_mask , input_type_ids]\n",
    "        d = tf.data.Dataset.from_tensor_slices(\n",
    "            {\n",
    "                \"unique_ids\":\n",
    "                tf.constant(\n",
    "                    all_unique_ids,\n",
    "                    shape=[num_examples] , dtype=tf.int32), \n",
    "                \"input_ids\":\n",
    "                tf.constant(\n",
    "                    all_input_ids , \n",
    "                    shape=[num_examples], dtype= tf.int32), \n",
    "                \"input_mask\":\n",
    "                tf.constant(\n",
    "                    all_input_mask,\n",
    "                    shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "                \"input_type_ids\":\n",
    "                    tf.constant(\n",
    "                        all_input_type_ids,\n",
    "                        shape=[num_examples, seq_length],\n",
    "                        dtype=tf.int32),\n",
    "                })\n",
    "            # chia dữ liệu thành các batch_size mỗi batch gồm batch_size đoạn \n",
    "        d = d.batch(batch_size=batch_size , dropout_remainder=False)\n",
    "        return d \n",
    "    \n",
    "\n",
    "\n",
    "# xây dựng phương thức trả về việc đóng model_fn cho TPUestimator \n",
    "def model_fn_builder(bert_config, init_checkpoint, \n",
    "                    layer_indexes, use_tpu, \n",
    "                    use_one_hot_embeddings):\n",
    "    \"\"\"Return 'model_fn' closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    # xây dựng một phươnh thức là model_fn trong phương thức model fn \n",
    "    def model_fn(features, labels, mode , params):\n",
    "        \"\"\"The 'model_fn' for TPUEstimator.\"\"\"\n",
    "        # gán cho các biến với các giá trị tương ứng trong danh sách chứa các đặc trưng \n",
    "        # features \n",
    "        unique_ids = features[\"unique_ids\"]\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        input_type_ids = features[\"input_type_ids\"]\n",
    "\n",
    "        # Thiết lập model = Bert_model.Bertmodel \n",
    "        model = Bert_model.BertModel(\n",
    "            config = bert_config, \n",
    "            is_training =False , \n",
    "            input_ids = input_ids, \n",
    "            input_mask = input_mask , \n",
    "            token_type_ids = input_type_ids, \n",
    "            use_one_hot_embeddings = use_one_hot_embeddings\n",
    "        )\n",
    "\n",
    "        # kiểm tra xem chế độ của mô hình có phải là Predict hay không. \n",
    "        if mode !=  tf.estimator.ModelKeys.PREDICT:\n",
    "            # nếu không đưa ra một cảnh báo lỗi \n",
    "            raise ValueError(\"Only PREDICT models are supported: %s\" %(model))\n",
    "        \n",
    "        # gán tvar bằng các biến có thể được huấn luyện từ mô hình \n",
    "        tvars = tf.trainable_variable()\n",
    "        # Đặt scaffold_fn = None một biến dùng để tạo khung mô hình\n",
    "        scaffold_fn = None \n",
    "        # Lấy bản đồ gán (assigment_map) và tên các biến đã khởi tạo (initialized_variable_names)\n",
    "        # từ checkpoint , một tệp lưu trữ trạng thái của mô hình \n",
    "        (assigment_map , initialized_variable_names) = Bert_model.get_assigment_map_from_checkpoint(\n",
    "            tvars, init_checkpoint\n",
    "        )\n",
    "\n",
    "        # kiểm tra xem mô hình này có đáng ử dụng tpu không \n",
    "        if use_tpu: \n",
    "            # nếu sử dụng tpu thi định nghĩa một hàm tpu_scaffold trong đó khởi tạo mô hình \n",
    "            # từ checkpoint và trả vê 1 đối tượng Scaffold\n",
    "            def tpu_scaffold():\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assigment_map)\n",
    "                return tf.train.Scaffold()\n",
    "            \n",
    "            # gán cho scaffold_fn = kết quả của phương thức tpu_scaffold \n",
    "            scaffold_fn = tpu_scaffold\n",
    "        # trường hợp không sử dụng TPU \n",
    "        else: # khởi tạo mô hình trực tiếp từ checkpoint\n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assigment_map)\n",
    "        \n",
    "        # ghi nhật ký và in ra thôngbtin của những biến co thể huấn luyện\n",
    "        tf.logging.infor(\"*** Trainable Variables ***\")\n",
    "        # duyệt qua dnh sách các biến có thể huấn luyện từ danh sách tvar\n",
    "        for var in tvars: \n",
    "            # khởi tạo một biến dạng string \n",
    "            init_string = \"\"\n",
    "            # kiểm tra xem tên của các biến có thể huấn luyện có nằm trong danh sách \n",
    "            # tên của các biến được khởi tạo hay không\n",
    "            if var.name in initialized_variable_names:\n",
    "                # Gán biến init_string = 1 chuỗi \n",
    "                init_string = \", *INIT_FROM_CKPT*\"\n",
    "            # ghi nhật ký các mã thông báo ra màn hình với tên biến , hình dạng và đuôi \n",
    "            # cho biết ní được khởi tạo từ checkpoint\n",
    "            tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                            init_string)\n",
    "        \n",
    "        # lấy ra danh sách tất tả các kết quả của lớp encoder từ mô hình \n",
    "        all_layers = model.get_all_encoder_layers()\n",
    "\n",
    "        # tạo một từ điển bao gòm uique_id và accs lớp mã hóa được chọn theo chỉ số tiếp theo \n",
    "        predictions = { # từ điển có khóa là unique_id và value là unique_ids \n",
    "            \"unique_id\": unique_ids,\n",
    "        }\n",
    "\n",
    "        # lấy ra chỉ số và giá trị tương ững của các phần tử trong danh sách chỉ số lớp \n",
    "        # layer_indexes \n",
    "        for (i , layer_index) in enumerate(layer_indexes):\n",
    "            # thêm một cặp khóa và giá trị vào tử điển predict\n",
    "            # với key = layer_output  theo chỉ số i và values theo giá trị tương ứng của i trong \n",
    "            # danh sách đầu ra các lớp mã hóa\n",
    "            predictions[\"layer_ouput_%d\" %i] = all_layers[layer_index]\n",
    "\n",
    "        # tạo một đối tương ouput_space \n",
    "        output_space = tf.contrib.tpu.TPUEsimatorSpec(\n",
    "            mode =mode, predictions=predictions, scaffold_fn = scaffold_fn\n",
    "        )\n",
    "        # trả về giá trị output_space \n",
    "        return output_space\n",
    "    # trả về model_fn \n",
    "    return model_fn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Xây dựng phương thức sử dụng để chuyển đổi các ví dụ thành các đặc trưng cơ bản \n",
    "def convert_examples_to_features(examples , seq_length , tokenizer):\n",
    "    \"\"\"Load a data file into a list of 'InputBatch'.\n",
    "        Tải một file dữ liệu trong dnah sách các lô đầu vào .\n",
    "    \"\"\"\n",
    "    # khởi tạo một dnah sách để lưu trữ các đặc trưng \n",
    "    features = []\n",
    "    # duyệt qua các ví dụ lấy ra các chỉ số và giá trị tương ứng \n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        # lấy ra danh sách token_a từ Tokenizer.tokenize là kết quả của phương thức token hóa \n",
    "        # mã thông báo và tìm đến các chỉ số của câu a tương ứng \n",
    "        token_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        # tạo một biến token_b gán = None \n",
    "        token_b = None \n",
    "        # kiểm tra xem trong accs mẫu ví dụ có tồn tại các token thuộc về danh scahs token_b(câu b)\n",
    "        if example.token_b : \n",
    "            # tương tự lấy ra danh sách token_b từ kết quả của việc token hóa mã thông báo \n",
    "            token_b = tokenizer.tokenize(example.token_b)\n",
    "        \n",
    "        # kiểm tra xem token_b CÓ tồn tại \n",
    "        if token_b: \n",
    "            # sửa đổi token_a và token_b tại trỗ sao cho tổng độ dài nhỏ hơn độ dài cụ thể \n",
    "            _truncate_seq_pair(token_a , token_b, seq_length - 3)\n",
    "        \n",
    "        # trường hợp còn lại tức là không tồn tại token b \n",
    "        else: \n",
    "            # kiểm tra xem độ dài của token_a có vượt quá độ dài seq_length - 2\n",
    "            if len(token_b) > seq_length - 2:\n",
    "                # gán cho tokens a = một chuỗi các phần tử từ 0 -> seq_length - 2\n",
    "                token_a = token_a[0 :(seq_length -2)]\n",
    "        \n",
    "        \"\"\" The convention in Bert is:\n",
    "            Quy ước trong Bert là : \n",
    "                (a) For seqence pairs:\n",
    "                tokens : [CLS] is this jack ##son ##ville ? [SEP] no it is not. [SEP]\n",
    "                type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "\n",
    "                (b) For single sentences:\n",
    "                tokens:   [CLS] the dog is hairy . [SEP]\n",
    "                type_ids: 0     0   0   0  0     0 0\n",
    "\n",
    "            \n",
    "            Trong đó \"type_id\" được sử dụng để cho biết đây là chuỗi đầu tiên hay chuỗi thứ 2. \n",
    "            Các vectoe nhúng cho type = 0 và type = 1 đã được học trong quá trình đào tạo trước\n",
    "                và được thêm vào vector nhúng từ wordpiece (và vector vị trí) . \n",
    "            Điều này không thực sự cần thiết vì các mã thông báo [SEP] phân tách rõ dàng các chuỗi ,\n",
    "                nhưng nó almf cho mô hình dễ dàng học được khái niệm về trình tự hơn.\n",
    "        \n",
    "          \n",
    "            Cho các nhiệm vụ phân loại vector đầu tiên tương ứng với [CLS] được sử dụng như vector câu. \n",
    "            Lưu ý điều này chỉ hợp lý vì toàn bộ mô hình đã đựoc tinh chỉnh .\n",
    "        \"\"\"\n",
    "\n",
    "        # tạo một danh sách tokens [] để lưu trữ danh sách các token và token đặc biệt\n",
    "        tokens = []\n",
    "        # và danh sách token_type_ids để lưu trữ các mã hóa one hot \n",
    "        input_type_ids = []\n",
    "        # thêm token [CLS] vào vị trí đầu tiên cho danh sách tokens\n",
    "        tokens.append(\"[CLS]\")\n",
    "        # tương tự thêm token 0 vào vị trí đầu tiên của danh sách input_type_ids \n",
    "        input_type_ids.append(0)\n",
    "\n",
    "        # duyệt qua danh sách các phần tử trong danh sách token_a \n",
    "        for token in token_a: \n",
    "            # thêm các token từ dnah sách token_a vào dnah sách lưu trữ token\n",
    "            tokens.append(token)\n",
    "            # và đồng thời thêm các token nhãn = 0 cho danh sách tokens_a \n",
    "            input_type_ids.append(0)\n",
    "        \n",
    "        # cuối cùng Thêm token [SEP] và tokens cuối cùng cho 2 dnah sách nhằm mụ đích ngăn cách câu \n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "        # kiểm tra xem có tồn tại danh sách token_b không \n",
    "        if token_b: \n",
    "            # nếu có ta thực hiện tương tự như làm việc vớ danh sách token_b\n",
    "            # duyệt qua danh sách các phần tử trong danh sách token_b\n",
    "            for token in token_b: \n",
    "                # thêm các token từ dnah sách token_n vào dnah sách lưu trữ token\n",
    "                tokens.append(token)\n",
    "                # và đồng thời thêm các token nhãn = 1 cho danh sách tokens_b \n",
    "                input_type_ids.append(1)\n",
    "            \n",
    "            # cuối cùng Thêm token [SEP] và tokens cuối cùng cho 2 dnah sách nhằm mụ đích ngăn cách câu \n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "        \n",
    "        # chuyển đổi dnah sách tokens thành token_ids dtype = tf.int\n",
    "        input_ids  = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # tạo ma trận pasdding mask \n",
    "        # có nhãn là 1 cho các token và 0 cho phần đệm \n",
    "        input_mask = [1] * len(input_ids) \n",
    "\n",
    "        # sau đó zero_pad đệm 0  theo chiều dài chuỗi \n",
    "        while len(input_ids) < seq_length: \n",
    "            # thêm các token = 0 vào cuối dnah sách \n",
    "            #input_ids \n",
    "            input_ids.append(0)\n",
    "            # input_mask \n",
    "            input_mask.append(0)\n",
    "            # input_type_ids\n",
    "            input_type_ids.append(0)\n",
    "        \n",
    "\n",
    "        # kiểm tra xem độ dài của các danh sách đã đạt đến độ dài của seq_length \n",
    "        assert len(input_ids) == seq_length\n",
    "        assert len(input_mask) == seq_length \n",
    "        assert len(input_type_ids) == seq_length \n",
    "\n",
    "        # sau đó kiểm tra xem chỉ số hiện tại của examples có < 5 không \n",
    "        if ex_index < 5 : \n",
    "            # GHI NHẬT KÝ MÃ THÔNG BÁO IN RA MÀN HÌNH THÔNG TIN \n",
    "            tf.logging.info(\"*** Example ***\")\n",
    "            # GHI NHẬT KÝ CÁC ID DUY NHẤT VÀ IN RA MÀN HÌNH THÔNG TIN CỦA CHÚNG \n",
    "            tf.logging.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            # một danh sách các token được tạo ra từ văn bản đầu vào bằng cách sử dụng một hàm \n",
    "            # tokenization.printable_text \n",
    "            # để chuyển đổi các ký tự không in được thành dạng có thể in được.\n",
    "            tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "                [Tokenization.printable_text(x) for x in tokens]))\n",
    "            # TƯƠNG TỰ NHƯ TRÊN CÁC MÃ IDS CÁCH NHAU BỞI KHOẲNG TRẮNG\n",
    "            tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            tf.logging.info(\n",
    "                \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            \n",
    "            # thêm các tham số được sử lý là kết quả của phương thức InputFeature\n",
    "            # vào danh sách features.\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=example.unique_id,\n",
    "                    tokens=tokens,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    input_type_ids=input_type_ids))\n",
    "    # trả về dnah scahs cac đặc trưng \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "# xây dựng phương thức  __truncate_seq_pair để cắt các cặp câu a, b \n",
    "def _truncate_seq_pair(tokens_a, tokens_b ,max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "\n",
    "        Đây là một phương pháp phỏng đoán cơ bản sẽ luôn cắt ngắn chuỗi dài hơn mỗi token tại một \n",
    "        bước thời gian. Điều này có nghĩa hơn việc cắt bớt phần trăm số token bằng nhau từ mỗi mã\n",
    "        thông báo \n",
    "    \"\"\"\n",
    "    # sử dụng một vòng lặp mô tận \n",
    "    while True:\n",
    "        # kiểm tra xem tổng số độ dài total_length của đoạn = len a  + len b không\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        # nếu nhỏ hơn không cần cắt \n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        # sau đó kiểm tra danh sách tokens_a nếu len(tokens_a) > len(token_b)\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            # bỏ bớt đi chỉ số cuối của đoạn a\n",
    "            tokens_a.pop()\n",
    "        else: # trường hợp còn lại bỏ bớt đi chỉ số cuối của đoạn b\n",
    "            tokens_b.pop()\n",
    "        \n",
    "\n",
    "#  xây dựng phương thức đọc các ví dụ \n",
    "def read_examples(input_file):\n",
    "    \"\"\"Read a list of 'InputExample's from an input file.\"\"\"\n",
    "    # tạo một danh sách examples để lưu trữ các ví dụ \n",
    "    examples = []\n",
    "    # và một danh sách lưu trữ các id duy nhất \n",
    "    unique_id = 0\n",
    "    # đặt phạm vi cho các file được đọc bởi tf.gfile.GFile gán nó cho reader \n",
    "    with tf.gfile.GFile(input_file , \"r\") as reader: \n",
    "        # sử dụng một vòng lặp vô tận \n",
    "        while True: \n",
    "            # gán cho line = kết quả của mã chuyển đổi định dạng thông thường sang unicode \n",
    "            # tiêu chuẩn \n",
    "            line = Tokenization.convert_to_unicode(reader.readline())\n",
    "            # kiểm tra xem có tồn tại giá trị line không . \n",
    "            if not line : \n",
    "                break \n",
    "            # chuẩn hóa các dòng văn bản \n",
    "            line = line.strip()\n",
    "            # gán text_a và text_b = None \n",
    "            text_a = None \n",
    "            text_b = None \n",
    "            # tìm kiếm và thay thế danh sách các dongf văn bản với việc so sách các từ với \n",
    "            # một biểu thức chính quy nếu = thì bỏ từ hay ký tự đó đi \n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "            # kiểm tra xem m có tồn tại hay không\n",
    "            if m is None:\n",
    "                # nếu m không tồn tại thì gán _text_a = line\n",
    "                text_a = line\n",
    "            else: # trường hợp còn lại \n",
    "                # nhóm dữ liệu thành 2 nhóm 1 và 2 và gán nó cho text_a và text_b \n",
    "                text_a = m.group(1)\n",
    "                text_b = m.group(2)\n",
    "            # thêm các cí dụ đã được xử lý là kết quả của phương thức InputXample cho một danh sach \n",
    "            # Example đã tạo trước đó\n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            # sau đó tăng giá trị unique_id lên 1\n",
    "            unique_id += 1\n",
    "            # trả về danh sách  example \n",
    "        return examples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    # Thiết lập một mức ghi nhật nhật ký của tensorflow là Infor nghĩa là tensorflow sẻ hiển thik \n",
    "    # tất cả các thông tin có nhãn Infor hoă qu trọng hơn. \n",
    "    tf.logging._set_verbosity(tf.logging.INFO)\n",
    "    # tạo một danh sách các chỉ số của các lớp Bert mà muốn trích xuất đặc trưng . \n",
    "    # được đingh nghĩa trong mã nguồn FLAGS.layer là 1 chuỗi chứa các chỉ số được phân tách \n",
    "    # bởi dấu , \n",
    "    layer_indexes = [int(x) for x in FLAGS.layers.split(\",\")]\n",
    "    # Tạo một đối tượng Bert Config từ 1 tập Json chứa các thônng số cấu hình của BERT.FLAGS.bert_config\n",
    "    # LÀ ĐƯỜNG DẪn đến tệp Json \n",
    "    bert_config = Bert_model.BertConfig.from_json_file(FLAGS.bert_config_file,)\n",
    "    # tạo một đối tượng tokenizer dể tách văn bản thành các token . Đối tượng này sử dụng một tệp \n",
    "    # từ vựng được chỉ định bởi FLAGS.vocab_file \n",
    "    tokenizer = Tokenization.FullTokenizer(\n",
    "        vocab_file =FLAGS.vocab_file , do_lower_case=FLAGS.do_lower_case\n",
    "    )\n",
    "\n",
    "    # Thiết lập một biến is_per_hot để chỉ định các cung cấp dữ liệu cho TPU \n",
    "    # để chỉ định cách cung cấp dữ liệu cho TPU \n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2 \n",
    "\n",
    "    # tạo một đối tượng run_config để cấu hình các tham số chạy cho TPU . \n",
    "    # đối tượng anyf có 2 tham số master và tpu_config . master là địa chỉ máy chủ TPU , \n",
    "    # config là một đối tượng khác chứa các cấu hình cho TPU \n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        master = FLAGS.master, \n",
    "        tpu_config =tf.contrib.tpu.TPUConfig(\n",
    "            num_shards =FLAGS.num_tpu_cores, \n",
    "            per_host_input_for_training=is_per_host\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # đọc một tệp văn bản đầu vào và trả về một đối tượng danh sách các đối tượng InputEXample \n",
    "    # mỗi đối tượng trong InputExample chứa một định danh duy nhất và một đoạn văn bản . \n",
    "    # FLAGS/input_file là đường dẫn đến tệp văn bản đó \n",
    "    examples = read_examples(FLAGS.input_file)\n",
    "\n",
    "    # chuyển đổi danh sách các ví dụ thành các đặc trưng . Mỗi danh sách InputExample chứa danh sách \n",
    "    # các thông tin cần thiết để đưa vào mô hình Bert T, bao gồm các token,\n",
    "    # các chỉ số của token, các nhãn vị trí, và các mặt nạ\n",
    "    features = convert_examples_to_features(\n",
    "        examples=examples , seq_length= FLAGS.max_seq_length , tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Tạo một từ điển  rỗng để lưu chữ các đối tượng InputDeatures theo mã định danh duy nhất của chúng \n",
    "    unique_id_to_feature = {}\n",
    "    # duyệt qua danh sách các đặc trưng features : \n",
    "    for feature in features: \n",
    "        # thêm vào từ điển các key và value là id và value_id (feature)\n",
    "        unique_id_to_feature[feature.unique_id] = feature \n",
    "    \n",
    "    # Thiết lập model_fn = Phương thức model_fn_buil đã được xây dưng trước \n",
    "    model_fn = model_fn_builder(\n",
    "        bert_config=bert_config,\n",
    "        init_checkpoint=FLAGS.init_checkpoint,\n",
    "        layer_indexes=layer_indexes,\n",
    "        use_tpu=FLAGS.use_tpu,\n",
    "        use_one_hot_embeddings=FLAGS.use_one_hot_embeddings\n",
    "    )\n",
    "\n",
    "    #If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "    # or GPU.\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=FLAGS.use_tpu,\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        predict_batch_size=FLAGS.batch_size)\n",
    "\n",
    "\n",
    "    # Tạo một hàm input_feature để cung cấp dữ liệu cho TPU . hàm này sẽ trả về một đối tượng \n",
    "    # tf.data.Dataset \n",
    "    input_fn = input_fn_builder(\n",
    "        features=features , seq_length=FLAGS.max_seq_length ,\n",
    "    )\n",
    "\n",
    "    # mở một tệp văn bản đầu ra với chế độ ghi và mã hóa UTF8 tệp này se chứa các đặc trưng được tríc \n",
    "    # xuất từ mô hình BERT.FLAGS.output_file  là đường dẫn đến tập đó đuôi tệp.JSON \n",
    "    with codecs.getwriter(\"utf-8\")(tf.gfile.Open(FLAGS.output_file, \"w\")) as writer: \n",
    "\n",
    "        # duyệtq au các kết quả được trả về bởi phương thức predict của đối tượng estimator\n",
    "        # phương thức predict sẽ chạy mô hình bert \n",
    "        for result in estimator.predict(input_fn , yield_single_examples=True ):\n",
    "            # lấy giá trị của khóa unique_id trong từ điển result và chuyển nó thành một số nguyên . \n",
    "            # dây là định dnah duy nhất của một đoạn avwn bản đầu vào\n",
    "            unique_id = int(result[\"unique_id\"])\n",
    "            # lấy đối tượng InputFeature tương ứng với định danh duy nhất từ từ điển unique_id_to_f\n",
    "            feature = unique_id_to_feature[unique_id]\n",
    "            # tạo một từ điển output json là một từ điển có thứ tự \n",
    "            output_json = collections.OrderedDict()\n",
    "            # Thêm một khóa index_line vào từ điển output_json với giá trị định dnah duy nhất của đoan văn bản \n",
    "            output_json[\"linex_index\"] = unique_id\n",
    "\n",
    "            # tạo một danh sách all_feature để lưu trữ các thông tin về các token và các lớp trong mô hình BERT.\n",
    "            all_features = []\n",
    "            # duyệt qua 1 danh sách các tokens trong đối tương feature \n",
    "            for (i , token) in enumerate(feature.tokens):\n",
    "                # tạo một danh scags all_layer để lưu trữ các lớp và thông tin accs lớp trong Bert \n",
    "                all_layers = []\n",
    "                # Duyệt qua danh sách các chỉ số của các lớp trong mô hình BERT, \n",
    "                # với chỉ số j và giá trị layer_index\n",
    "                for (j , layer_index) in enumerate(layer_indexes):\n",
    "                    #  Lấy giá trị của khóa “layer_output_%d” trong từ điển result, với %d là giá trị của j.\n",
    "                    layer_output = result[\"layer_output_%d\" % j]\n",
    "                    # tạo một đối tượng layers là một từ điển có thứ tự \n",
    "                    layers = collections.OrderedDict()\n",
    "                    # : Thêm một khóa “index” vào từ điển layers, \n",
    "                    # với giá trị là chỉ số của lớp thứ j, ví dụ 0, 1, 2, 3.    \n",
    "                    layers[\"index\"] = layer_index\n",
    "                    # Thêm một khóa “values” vào từ điển layers, với giá trị là một danh sách các số thực\n",
    "                    layers[\"values\"] = [\n",
    "                        round(float(x), 6) for x in layer_output[i:(i + 1)].flat\n",
    "                    ]\n",
    "                    # hêm đối tượng layers vào danh sách all_layers.\n",
    "                    all_layers.append(layers)\n",
    "                # tạo một từ điển có thứ tự và gán nó cho features \n",
    "                features = collections.OrderedDict()\n",
    "                # : Thêm một khóa “token” vào từ điển features, với giá trị là token thứ i, ví dụ “Hello”.\n",
    "                features[\"token\"] = token\n",
    "                #  Thêm một khóa “layers” vào từ điển features, với giá trị là danh sách all_layers, \n",
    "                # chứa các thông tin về các lớp trong mô hình BERT cho token thứ i\n",
    "                features[\"layers\"] = all_layers\n",
    "                # Thêm đối tượng features vào danh sách all_features.\n",
    "                all_features.append(features)\n",
    "            #: Thêm một khóa “features” vào từ điển output_json, với giá trị là danh sách all_features,\n",
    "            #  chứa các thông tin về các token và các lớp trong mô hình BERT ch\n",
    "            output_json[\"features\"] = all_features\n",
    "            # : Ghi đối tượng output_json vào tệp văn bản đầu ra, sử dụng phương thức dumps của thư viện\n",
    "            # json để chuyển đổi từ điển thành chuỗi JSON. \n",
    "            # Thêm ký tự xuống dòng “\\n” để phân tách các đoạn văn bản khác nhau.\n",
    "            writer.write(json.dumps(output_json) + \"\\n\")\n",
    "\n",
    "\n",
    "# phần cuối của chương trình sử dụng mô hình BERT để trích xuất các đặc trưng của văn bản\n",
    "if __name__ == \"__main__\":\n",
    "  #  Đánh dấu rằng tham số “input_file” là bắt buộc phải có khi chạy chương trình.\n",
    "  flags.mark_flag_as_required(\"input_file\")\n",
    "  # Đánh dấu rằng tham số “vocab_file” là bắt buộc phải có khi chạy chương trình. \n",
    "  # Tham số này là đường dẫn đến tệp từ vựng của mô hình BERT,\n",
    "  flags.mark_flag_as_required(\"vocab_file\")\n",
    "  #  Đánh dấu rằng tham số “bert_config_file” là bắt buộc phải có khi chạy chương trình. \n",
    "  # Tham số này là đường dẫn đến tệp JSON chứa các thông số cấu hình của mô hình BERT\n",
    "  flags.mark_flag_as_required(\"bert_config_file\")\n",
    "  #  Đánh dấu rằng tham số “init_checkpoint” là bắt buộc phải có khi chạy chương trình. Tham \n",
    "  # số này là đường dẫn đến điểm kiểm tra được lưu trữ của mô hình BERT, ví dụ “bert_model.ckpt”\n",
    "  flags.mark_flag_as_required(\"init_checkpoint\")\n",
    "  # Đánh dấu rằng tham số “output_file” là bắt buộc phải có khi chạy chương trình.\n",
    "  #  Tham số này là đường dẫn đến tệp văn bản đầu ra\n",
    "  flags.mark_flag_as_required(\"output_file\")\n",
    "  # : Chạy chương trình bằng cách gọi hàm main() được định nghĩa ở đâu đó trong mã nguồn. \n",
    "  # Hàm main() sẽ tạo một đối tượng estimator để huấn luyện và suy luận mô hình BERT,\n",
    "  #  sử dụng các tham số được truyền vào từ dòng lệnh\n",
    "  tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
