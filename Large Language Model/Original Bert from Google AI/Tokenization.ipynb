{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections \n",
    "import re \n",
    "import unicodedata \n",
    "import six\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
    "    \"\"\" Kiểm tra xem liệu cấu hình vỏ bọc có nhất quán với tên checkpoint .\n",
    "    Đoạn mã này được sử dụng để kiểm tra xem cấu hình chữ hóa hay chữ thường do_lower_case có \n",
    "    phù hợp với tên checkpoint hay không . \n",
    "\n",
    "    Checkpoint là một tệp chứa các trọng số của mô hình bERY ĐÃ ĐƯỢC TIỀN HUẤN LUYỆN TRÊN DỮ LIỆU CỤ THỂ \n",
    "    \"\"\"\n",
    "    # người dùng phải chuyển vỏ bọc này vào và không có kiểm tra dõ dàng nào về việc \n",
    "    # liệu nó có khác với checkpoint không . Thông tin vỏ bọc đáng lẽ phải được lưu trữ trong \n",
    "    # tệp bert.cònig, nhưng nó không vì vậy phải phát hiện nó theo phương pháp phòng đoán và \n",
    "    # xác thực \n",
    "\n",
    "\n",
    "\n",
    "    # kiểm tra xem init_checkpoint có được khởi tạo hay không \n",
    "    if not init_checkpoint: \n",
    "        # lập tưc trả vè\n",
    "        return \n",
    "\n",
    "    # Còn không thì sử dụng biểu thức chính qua regulary experession để tách tên mô hình từ init_checkpoint \n",
    "    # sử dụng hàm re.match để xem một chuỗi có khơp shay phù hợp với một biểu thức chính quy hay không \n",
    "    m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
    "    # kiểm tả xem m có chưa tên của mô hình từ init checkpoint \n",
    "    if m is None: \n",
    "        # lập tức trả về \n",
    "        return  \n",
    "    \n",
    "    # tạo biến model_name  = nhóm các phần tử có trong m \n",
    "    # với nhóm con đầu tiên \n",
    "    model_name = m.group(1)\n",
    "\n",
    "    # tạo 2 danh sách  là lowe_cased và cased_model chứa tên của các checkpoint init \n",
    "    lower_models = [\n",
    "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
    "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
    "    ]\n",
    "\n",
    "    # với cased_model là dạn sách chứa tên của các mô hình với các văn abnr dạng nguyên bản \n",
    "    # gồm cả chữ in lẫn thường \n",
    "    cased_models = [\n",
    "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
    "      \"multi_cased_L-12_H-768_A-12\"\n",
    "    ]\n",
    "\n",
    "    # Khởi tạo 1 biến is_bad_config  = False sử dụng để kiểm tra xem cấu hình chữ hoa \n",
    "    # chữ thường (do_lowe_case) có phù hợp với tên checkpoint hay không \n",
    "    # nếu không biến này giá trị = True và đoạn mã sẽ sinh ra một lỗi \n",
    "    is_bad_config = False # và để đảm bảo rằng Bert được tinh chỉnh theo cachs tương thích với \n",
    "    # các mô hình tiền huấn luyện. \n",
    "    # kiểm tra xem nhóm các phần tử tên mô hình trong model name có giống với tên của các \n",
    "    # mô hình trong danh sách lower_models và không thuộc do_lowe_cased\n",
    "    if model_name in lower_models and not do_lower_case:\n",
    "        # ta gán is bad_ = true \n",
    "        is_bad_config = True\n",
    "        # Bieens actual là giá trị hiện tại của mô hình tiền huấn luyện \n",
    "        # nếu giá trị không phù hợp sẽ gây lỗi \n",
    "        actual_flag = \"False\"\n",
    "        # case_naem tên của cách mô hình bert được tiền huấn luyênh dựa trên checkpoint \n",
    "        case_naem = \"lowercased\"\n",
    "        opposite_flag = \"True\"\n",
    "\n",
    "    # Thực hiện kiểm tra tương tự  với danh sách case_model \n",
    "    if model_name in cased_models and do_lower_case:\n",
    "        is_bad_config = True\n",
    "        actual_flag = \"True\"\n",
    "        case_name = \"cased\"\n",
    "        opposite_flag = \"False\"\n",
    "\n",
    "    # nếu is_bed là true sẽ ném một lỗi  VALUEerror \n",
    "    if is_bad_config:\n",
    "        raise ValueError(\n",
    "            \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
    "            \"However, `%s` seems to be a %s model, so you \"\n",
    "            \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
    "            \"how the model was pre-training. If this error is wrong, please \"\n",
    "            \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
    "                                            model_name, case_name, opposite_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng phương thức chuyển đổi văn bản thành dạng unicode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng phương thức chuyển đổi văn bản thành dạng unicode \n",
    "def convert_to_unicode(text):\n",
    "    \"\"\" chuyển đổi văn bản thành dnagj unicode nếu chưa có giả sử đầu vào UTF-8\"\"\"\n",
    "    # kiểm tra phiên bản python config \n",
    "    if six.PY3: # = True \n",
    "        # sử dụng hàm isinstance giá trị không thay đổi để kiểm tra xem \n",
    "        # text có dạng string hay không \n",
    "        if isinstance(text , str):\n",
    "            return text \n",
    "        # trường hợp nếu text dạng bytes\n",
    "        elif isinstance(text, bytes):\n",
    "            # sử dụng mã hóa unicode để đưa dữ liệu về dạng unicode \n",
    "            # với decode(\"utf8\" \"ignore\")\n",
    "            return text.decode(\"utf8\" ,\"ignore\")\n",
    "        # trường hợp còn lại đưa ra cảnh báo lỗi \n",
    "        else: \n",
    "            raise ValueError(\"Unsuppoeted string type: %s\" %(type(text)))\n",
    "        \n",
    "        # ta thực hiện tương tự với bản python 2\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "# Xây dựng một phương thức tương tự mã hóa theo cách phù hợp để in hau tf.logging \n",
    "def printable_text(text):\n",
    "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
    "\n",
    "  # These functions want `str` for both Python2 and Python3, but in one case\n",
    "  # it's a Unicode string and in the other it's a byte string.\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng phương thức load vocab_Size từ một từ điển \n",
    "def load_vocab(vocab_file):\n",
    "    \"Load a vocabulary file into a dictionary.\"\n",
    "    # xây dựng một từ điển  với hàm OrderedDict là một lớp con củad dối tượng Dictionary \n",
    "    # nhưng khác ở nó duy trì thứu tự trèn của kháo . Nghãi là khi duyệt qua các phần tử của \n",
    "    # OrderedDict sẽ nhận được chúng theo thứ tự đã thêm vào không phải ngẫu nhiên như từ điển \n",
    "    vocab = collections.OrderedDict()\n",
    "    # tạo một biến index init =  0 \n",
    "    index = 0 \n",
    "    # mở file vocab đặt cho phéo đọc file và gán nó vaò biến reader \n",
    "    with tf.gfile.GFile(vocab_file , \"r\") as reader: \n",
    "        # Trong khi điều kiện = True \n",
    "        while True : \n",
    "            # sử dụng biến token để luuư trữ các dòng mã được đọc từ các file reader \n",
    "            # đã được sử lý mã hóa unicode \n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            # nếu không có danh sách token nào được tạo ra \n",
    "            if not token : \n",
    "                # lập tức dừng lại \n",
    "                break \n",
    "            # chuẩn hóa danh sách toke với hàm strip \n",
    "            token = token.strip()\n",
    "            # đặt các chỉ số token qua tập từ điển và đặt chỉ số index \n",
    "            # cho thứ tự các toke được đư vào từ điển\n",
    "            vocab[token] = index \n",
    "            index += 1 \n",
    "    # Trả về tập từ điển \n",
    "    return vocab \n",
    "\n",
    "# Xây dựng phuuwong thức chuyển đổi 1 chuỗi token[ids] bằng cách sử dụng bộ từ điển \n",
    "def convert_by_vocab(vocab, items):\n",
    "    \"\"\" Convert a sequence of [tokens[ids]] using the vocab. \n",
    "        Chuyển đổi 1 chuỗi id mã thông báo bằng cách sử dụng tập từ điển \n",
    "    \"\"\"\n",
    "    # tạo một danh sách output = none để \n",
    "    output = []\n",
    "    # duyệt qua dnah sách item là dnah sách chứa các tokens hoặc các id  cho tokens \n",
    "    for item in items: \n",
    "        # thêm các items vào bộ từ điển \n",
    "        output.append(vocab[item])\n",
    "    # Trả về 1 danh sách output chứa các id hoặc các tokens tương ứng với các phần tử trong item\n",
    "    return output \n",
    "\n",
    "# Xây dựng phuuwong thức  chuyển đổi các token thành dạng token_ids \n",
    "# nhận đầu vào là bộ từ điển chứa dnah sách các cặp (token , id) \n",
    "# và tokens là danh sách các tokens\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    # trả về output chứa các id tương ứng với các tokens trong tokens \n",
    "    return convert_by_vocab(vocab , tokens)\n",
    "\n",
    "# xây dựng phương thức chuyển đổi các id thành tokens \n",
    "# nhận đầu vào là bộ từ điển chưa các cặp toke , id và danh sách ids chưa các ids \n",
    "# sau đó trả về 1 danh sách chứa các tokens theo id tương ứng \n",
    "def convert_ids_to_tokens(inv_vocab, ids):\n",
    "    return convert_by_vocab(inv_vocab, ids)\n",
    "\n",
    "\n",
    "# Xây dựng phương thức dọn dẹp các khoảng trằng cơ bản \n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\" Chayj tinhs nanwg donj depj vaf phaan tachs canr khoangr trawngs \n",
    "    trong ddonj vawn banr .\n",
    "    Run basic whitespace clearning and pslitting on a piece of text. \n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    # kiểm tra xem có tồn tại text không nếu không trả về 1 dnah sách dỗng \n",
    "    if not text: \n",
    "        return []   \n",
    "    # Trường hợp còn lại \n",
    "    tokens = text.split()\n",
    "    return tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng lớp kiểu cha ký tự khoảng trắng \n",
    "def _is_whitespace(char):\n",
    "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "  # as whitespace since they are generally considered as such.\n",
    "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return True\n",
    "  # gọi hàm unicodedata \\.categorical để lấy một chuỗi mã hóa lại ký tự của char \n",
    "  cat = unicodedata.category(char)\n",
    "  if cat == \"Zs\": # so sánh cat vs Zs một mã hóa cho ký tự khoảng trắng nếu = trả về True \n",
    "    # nếu char = \" \" => Zs\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "# kiểm tra xem liệu char có phải là một ký tự điều khiển \n",
    "def _is_control(char):\n",
    "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "  # These are technically control characters but we count them as whitespace\n",
    "  # characters.\n",
    "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return False\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat in (\"Cc\", \"Cf\"): # cc , cf là 2 mã hóa cho ký tự điều khiển \n",
    "    return True\n",
    "  return False\n",
    "\n",
    "# kiểm xem kiệu các ký tự có là ký tự đặc biệt \n",
    "def _is_punctuation(char):\n",
    "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "  cp = ord(char)\n",
    "  # We treat all non-letter/number ASCII as punctuation.\n",
    "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "  # Punctuation class but we treat them as punctuation anyways, for\n",
    "  # consistency.\n",
    "  # So sánh cp với một số khoảng giá trị,\n",
    "  #  tương ứng với các ký tự không phải là chữ cái hay số trong bảng mã ASCII1\n",
    "  # Nếu cp nằm trong một trong các khoảng đó trả về True . \n",
    "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat.startswith(\"P\"):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựn lớp Tokenize cơ bản "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng lớp tokenize cơ bản cho việc xử lý văn bản \n",
    "# đưa văn bản về dạng tiêu chuẩn \n",
    "class BasicTokenize(object):\n",
    "\n",
    "    \"\"\"Runs basic tokenizetion (punctuation splitting , lower casing , etc...)\n",
    "    Chạy mã hóa mã thông báo cơ bản như xóa kỹ tự đặc biệt , biến đổi thường ... \n",
    "    \"\"\"\n",
    "    # Xây dựng phuuwong thức khởi tạo \n",
    "    def __init__(self, do_lower_case=True):\n",
    "        \"\"\" Xây dựng một BasicTokenizer.\n",
    "        Args : \n",
    "            do_lower_case: Whether to lower case the input.   \n",
    "             Có viết thường chữ thường đầu vào hay không . \n",
    "        \"\"\"\n",
    "\n",
    "    # xây dựng phương thức mã hóa nhận đầu vào là text đoạn văn abnr \n",
    "    def tokenize(self, textr):\n",
    "        \"\"\"Tokenizes a piece of text.\"\n",
    "            Token hóa 1 đoạn văn bản .\n",
    "           \"\"\"\n",
    "        # Chuyển đổi văn abnr về dịnh dạng unicode \n",
    "        text = convert_to_unicode(text)\n",
    "        # làm sạch văn bản bằng cách loại bỏ các ký tự không hợp lệ và khoảng trắng thừa \n",
    "        # sử dụng phương thức self.__clean_text của lớp \n",
    "        text = self._clean_text(text)\n",
    "        # thêm khoảng trắng xung quanh các ký tự JCK sử dụng phương thức self.__tokenize_chines_\n",
    "        text = self._tokenize_chinese_chars(text)\n",
    "\n",
    "        # Tách text bằng các tokens dựa trên khoảng trắng giữ chúng \n",
    "        origi_tokens = whitespace_tokenize(text)\n",
    "        # tạo một danh sách lưu trữ các tokens được tách \n",
    "        split_tokens = []\n",
    "        # duyệt qua dnah sách các tokens đã được chuẩn hóa qua các lớp chuẩn háo ở trên \n",
    "        for token in origi_tokens: \n",
    "            # kiểm tra xem do_lower_case có bằng True không \n",
    "            if self.do_lower_case:\n",
    "                # chuấn hóa tất cả dữ liệu về dạng thường \n",
    "                token = token.lower()\n",
    "                # laoij bỏ dấu trọng âm có trong từ \n",
    "                token = self._run_strip_accents(token)\n",
    "            # Loại bỏ các dấu câu . \n",
    "            split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "        # Sau đó thêm các khoẳng trắng cho đọa văn abnr \n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "    \n",
    "    # Xây dựng phương thức loại bỏ đi các dấu từ nhận đầu vào là đoạn văn bản \n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text. \n",
    "            Tách các dấu của từ trong đoạn văn bản . \n",
    "        \"\"\"\n",
    "        # sử dụng thư viện unicodedata để chuẩn háo văn bản theo dạng NFD (Normalization Form Decomposittion)\n",
    "        # trong đó mỗi ký tự có dấu được phân tách thành một ký tự không dấu và một ký tự biể thị dấu \n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            #   Thêm các ký tự vào dnah sách output \n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "    \n",
    "    # Xây dựng phương thưcs loại bỏ dấu câu trong đoạn văn bản \n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        # tạo một biến char lưu trữ dnah sách văn abnr \n",
    "        chars = list(text)\n",
    "        # một biế  đếm i để đếm ác kỹ tự trong văn bản \n",
    "        i = 0\n",
    "        # Tạo 1 biến start New_word  = True biến cho biết bắt đầu từ mới .  \n",
    "        start_new_word = True\n",
    "        # một dnah sách dỗng để lưu trữ các ký tự trong đoạn văn bản sau khi được loại bỏ dấu câu\n",
    "        output = []\n",
    "        # kiểm tra xem i là số lượng ký tự hiện tại có nhỏ hơn độ dài chars tring list \n",
    "        while i < len(chars):\n",
    "            # gán một biến char = ký tự char tị chỉ số i \n",
    "            char = chars[i]\n",
    "            # ,iểm tra xem biến char có phải dấu câu nếu phải \n",
    "            if _is_punctuation(char):\n",
    "                # danh sách chứa char vào output\n",
    "                output.append([char])\n",
    "                # và đặt start_new_word bằng True.\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                # trường hợp còn lại ta kiểm tra xem star_new_word có = true \n",
    "                # nếu có ta thêm vào 1 phần tử = 1 khoảng không tồn tại \n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                    # đặt lại start)word  = false\n",
    "                start_new_word = False\n",
    "                # đặt chỉ số của ma trận output -1  = char thêm char vào danh sách cuối cùng \n",
    "                output[-1].append(char)\n",
    "            # tăng i lên 1\n",
    "            i += 1\n",
    "        # trả về 1 dnah sách và thêm khoảng trắng vào cho nó . \n",
    "        return [\"\".join(x) for x in output]\n",
    "    \n",
    "    # xây dựng phương thức  kiểm tra xem CP là điểm mã của lý tự CJK không \n",
    "    def is_chinese_char(self, cp):\n",
    "        \"\"\" Check whether CP is the codepoint of a CJK character.\"\"\"\n",
    "        # Điều này xác định một ký tượng tiếng trung như bất kỳ mọi thứ trong khối CJK Unicode. \n",
    "        \n",
    "        # Sau đó kiểm tra xem điểm CP có nằm trong một trong các khoảng của khối CJK hay không ,\n",
    "        # bằng cách so sánh cp với các gái trị hexa tương ứng \n",
    "        # Nếu CP nằm trong bất kỳ khoảng nào đoạn mã sẽ trả về True , => CP là một mã điểm của ký tự \n",
    "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "            (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "            (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "            (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "            (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "            (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "            (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "            (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "\n",
    "            return True\n",
    "        # trường hợp còn lại False \n",
    "        return False \n",
    "    \n",
    "    # xây dựng phương thức chuẩn hóa loại bỏ ký tự không hợp lệ hặc khoảng trắng \n",
    "    # trong text \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Thực hiện loại bỏ các ký tự không hợp lệ và xóa bỏ các khoảng trắng trong văn bản.\"\"\"\n",
    "        # loại bỏ các ký tự không hợp lệ và các khoảng trắng thừa trong text\n",
    "        output = []\n",
    "        # duyệt qua đoạn văn bản \n",
    "        for char in text: \n",
    "            # sử dụng hàm ord để lấy điểm củ mỗi ký tự trong text và sử dụng hai hàm khác \n",
    "            # là _is_control và __is_whitesape để kiểm tra xem ký tự đó có phải là ký tự điều khiển \n",
    "            # hoặc khoảng trắng hay không . \n",
    "            cp = ord(char)\n",
    "            # kiểm tra xem ký tự đó có phải ký tự điều khiển không \n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue \n",
    "            # thêm khoảng trắng vào văn bản . \n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            \n",
    "            else: \n",
    "                output.append(char)\n",
    "        # Cuối cùng , hàm nối các phần tử trong danh sách output lại trả về một chuỗi ký tự mới    \n",
    "        return \"\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng lớp mã hóa Wordpiece \n",
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokeniziation.\"\"\"\n",
    "    # thiết lập phương thức khởi tạo \n",
    "    def __init__(self, vocab , unk_token='[UNK]', max_input_chars_per_word = 200):\n",
    "        self.vocab = vocab \n",
    "        self.unk_token = unk_token \n",
    "        self.max_input_chars_per_word = max_input_chars_per_word \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a piece of text ints its wword pieces. Token hóa một đoạn văn abnr thành các \n",
    "        phần từ của nó. \n",
    "\n",
    "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "        using the given vocabulary.\n",
    "\n",
    "         Điều này sử dụng thuật toán khớp đầu tiên dài nhất để thực hiện mã thông báo bằng cách sử dụng \n",
    "         từ vựng đã cho . \n",
    "\n",
    "        For example : \n",
    "            input = \"unaffable\"\n",
    "            output = [\"un\", \"##aff\", \"##able\"]\n",
    "\n",
    "        ARGS: \n",
    "            text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer.\n",
    "            \n",
    "            Văn bản :Một mã thông báo đơn hoặc mã thông báo được phân tách thành khoảng trắng \n",
    "            điều này lẽ ra phải được thông qua BasicTokenizer. \n",
    "\n",
    "        Returns: \n",
    "            A list of wordpiece tokens. \n",
    "            Một dnah sách các thẻ từ . \n",
    "\n",
    "        \"\"\"\n",
    "        # chuyển tất acr về định dạng unicode \n",
    "        text = convert_to_unicode(text)\n",
    "\n",
    "        # tạo một list output để lưu trữ danh sách đầu ra \n",
    "        output_tokens = []\n",
    "        # duyệt qua 1 danh sách văn bản đã được loại bỏ đi các khoảng trắng. \n",
    "        for token in whitespace_tokenize(text):\n",
    "            # tạo một list char  = danh sách các tokens \n",
    "            chars = list(token)\n",
    "            # kiểm tra xem độ dài trong list char có lớn hơn maxtokens \n",
    "            if len(chars) > self.max_input_chars_per_word: \n",
    "                # Thêm ký tự phân cách [UNK] và vị trí cuối cùng của list \n",
    "                # và add nó vào danh sách lưu trữ \n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "            \n",
    "            # tạo 3 biến is_bad  , start = 0 \n",
    "            is_bad = False # có ý nghĩa là có hay không mội chuỗi có trong chars mà không nămd trong tập từ điển \n",
    "            start = 0 \n",
    "            sub_tokens = []\n",
    "            \n",
    "            # kiểm tra xem trong khi chỉ số start hiện tại có nhỏ hơn độ dài list chars \n",
    "            while start < len(chars):\n",
    "                # đặt chỉ số kết thúc  = độ dài của list \n",
    "                end = len(chars)\n",
    "                cur_substr = None \n",
    "                # xét chỉ số start hiện tại có nhỏ hơn chỉ số kết thúc hay không \n",
    "                while start < end: \n",
    "                    # thêm các khoẳng trắng vào trong list chars[start:end]\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    # nếu start >0 thêm tiền tố ## vào substr \n",
    "                    if start > 0 :\n",
    "                       substr = \"##\" + substr\n",
    "                    # nếu substr thuộc bộ từ điển \n",
    "                    if substr in self.vocab: \n",
    "                        # ta gans cur_substr = substr \n",
    "                        cur_substr = substr\n",
    "                        break \n",
    "                    # giamr chỉ số kết thúc đi 1 để thu hẹp đi khoăng cách giữa start và end \n",
    "                    # và tạo ra các chuỗi ngắn hơn từ chars mục đích tìm ra chuỗi con ngắn hơn \n",
    "                    # mà vẫn nằm trong từ điển self.vocab \n",
    "                    end -= 1  \n",
    "\n",
    "                # nếu cur_sub vẫn  = None\n",
    "                if cur_substr is None:\n",
    "                    # gán is_bad = true và thoát \n",
    "                    is_bad = True\n",
    "                    break\n",
    "                # nếu không thêm thêm cur_sub vào dnah sách subtokens\n",
    "                sub_tokens.append(cur_substr)\n",
    "                # và đăth chỉ số start = end \n",
    "                start = end\n",
    "            # nếu ibad = True\n",
    "            if is_bad:\n",
    "                # thì thêm một token không xác định self.unk_token vào danh sách output_tokens.\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else: # thì mở rộng danh sách output_tokens bằng các phần tử trong sub_tokens.\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng lớp Xử lý văn bản đầy đủ bằng việc kết hợp các phương thức đã xây dựng trươc đó \n",
    "class FullTokenizer(object):\n",
    "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "  # Thiết lập phương thức khởi tạo và định dạng một số phương thức , lớp xử lý dữ liệu tiêu chuẩn \n",
    "  def __init__(self, vocab_file, do_lower_case=True):\n",
    "    # Load data từ vocab_file gán nó vào biến vocab \n",
    "    self.vocab = load_vocab(vocab_file)\n",
    "    # tạo một biến lưu trữ các phần tử trong từ điển đã được truy xuất \n",
    "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    # Mã hóa tiêu chuẩn văn bản cơ bản với bỏ khoảng trắng và các định dnagj cũng như ký tự đặc biệt \n",
    "    self.basic_tokenizer = BasicTokenize(do_lower_case=do_lower_case)\n",
    "    # Mã háo tokens với WordpieceTokenizer \n",
    "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "  # Thiết lập lớp xử lý với các lớp đã được xây dựng trước đó \n",
    "  def tokenize(self, text):\n",
    "    split_tokens = []\n",
    "    for token in self.basic_tokenizer.tokenize(text):\n",
    "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "        split_tokens.append(sub_token)\n",
    "\n",
    "    # Trả về danh sách các tokens đã được trích xuất \n",
    "    return split_tokens\n",
    "\n",
    "  # lấy ra IDS của theo chỉ số tokens \n",
    "  def convert_tokens_to_ids(self, tokens):\n",
    "    return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "  # lấy  ra chỉ số tokens từ chỉ số ids \n",
    "  def convert_ids_to_tokens(self, ids):\n",
    "    return convert_by_vocab(self.inv_vocab, ids)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
