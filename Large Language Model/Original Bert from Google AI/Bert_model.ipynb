{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import six \n",
    "import numpy as np \n",
    "import math \n",
    "import copy \n",
    "import re \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng lớp bert cấu hình \n",
    "class BertConfig(object):\n",
    "    \"\"\"Configuration for 'Bert model' \"\"\"\n",
    "    # thiết lập phương thức khởi tạo \n",
    "    def __init__(self, \n",
    "                vocab_size , hidden_size=768, \n",
    "                num_hidden_layers =12, num_attention_heads=12, \n",
    "                intermediate_size=3072, hidden_act=\"gelu\",\n",
    "                hidden_dropout_prob=0.1, attention_probs_dropout_prob = 0.1,\n",
    "                max_position_embeddings = 512, type_vocab_size = 16,\n",
    "                initializer_range=0.2):\n",
    "        \"\"\"Constructs BertConfig. Xây dựng cấu hình cho Bert.\n",
    "        \n",
    "        Args : vocab_size : Kích thước của đầu vào mô hình bert \n",
    "             hidden_size : Kích thước lớp mã hóa và lớp tổng hợp . \n",
    "             num_hidden_layers : Số lượng đầu chú ý cho mỗi lớp attention trong bộ phân mã hóa Transformer \n",
    "             intermediate_size : Kích thước trung gian của lớp chuyển tiếp trong bộ mã hóa Transformer\n",
    "             hiddent_act : Hàm kích hoạt biến đổi đầu vào trong lớp mã hóa và lớp tổng hợp. \n",
    "             max_position_embedding : Độ dài tối đa mà mô hình này có thể được sử dụng . \n",
    "             type_vocab_size : Kích thước của từ vựng \"token_type_ids\" đã được chuyển vào trong BertModel.\n",
    "             initializer_range: Giá trị chuẩn để khởi tạo tất cả các , ma trận trọng số . \n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "\n",
    "    # xây dựng một phương thức lớp nhận một đối tượng python là một từ điển các tham số cấu hình \n",
    "    # và trả về môtj thể hiwnwj của lơp Bert Congfig với các thuộc tính tương ứng với các kháo và giá trị \n",
    "    # trong từ điển.\n",
    "    @ classmethod \n",
    "    def from_dict(cls, json_object):\n",
    "        \"\"\"Xây dựng a 'BertConfig' từ một từ điển tham số python \"\"\"\n",
    "        config = BertConfig(vocab_size=None)\n",
    "        # sử dụng hàm six.iteritems để duyệt qua các cặp khóa và giá trị trong từ điển \n",
    "        for (key , value) in six.iteritems(json_object):\n",
    "            # gán chúng vào __dict__ của thể hiện cấu hình \n",
    "            config.__dict__[key] = value \n",
    "        return config \n",
    "    \n",
    "    # Xây dựng thêm 1 phương thức lớp để đọc các file json \n",
    "    # và trích suất các đoạn văn bản từ file nguồn \n",
    "    @classmethod\n",
    "    def from_json_file(cls , json_file):\n",
    "        \"\"\"Xây dựng một BerConfig từ một tệp tham số json. \"\"\"\n",
    "        # tải danh sách file json và cho phép đọc gán vào reader \n",
    "        with tf.gfile.GFile(json_file, \"r\") as reader: \n",
    "            # gán biến text = laod reader \n",
    "            text = reader.read()\n",
    "        # sử dụng hàm json.loads để chuyển đổi nó thahf một đối tượng Python là một từ điển\n",
    "        # sử dụng from_dict để trả về một thể hiện của lớp BertConfig từ từ điển đó \n",
    "        return cls.from_dict(json.loads(text))\n",
    "    \n",
    "    # xy dựng phương thứ to_dict \n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        # tạo một bản sao sâu từ một từ điển thể hiện trong bertConfifg mà ta đã tríc xuất từ file json\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "    # xây dựng phương thức thể hiện, nhưng nó trả về một chuỗi JSON chứa các thuộc tính\n",
    "    # và giá trị của thể hiện cấu hình. Phương thức này gọi phương thức to_dict để lấy từ điển Python từ thể hiện cấu hình, \n",
    "    # và sau đó sử dụng hàm json.dumps để chuyển đổi nó thành một chuỗi JSON. \n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng mô hình Bert \n",
    "class BertModel(object):\n",
    "    \"\"\"Bert model (\"Bidirectional Encoder Representation from Transformer.)\n",
    "\n",
    "    \n",
    "    Chuyển đổi thành id mã thông báo với WordPiece \n",
    "    Input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "    Input_mask = tf.constant([[1, 1 , 1][1, 1, 0]])\n",
    "    token_type_ids = tf.constant([[0, 0, 1][0, 2, 0]])\n",
    "\n",
    "    config = modeling.BertConfig(vocab_size= 32000, hidden_size =512 , \n",
    "        num_hidden_layers=8 , num_attention_heads=6 , intermediate_size =1024)\n",
    "\n",
    "    model = modeling.BertModel(config = config , is_training =True , input_ids= input_ids , \n",
    "        inpuut_mask = input_mask , token_type_ids = token_type_ids)\n",
    "\n",
    "    label_embedding = tf.get_variable(..)\n",
    "    pooled_output = model.get_pooled_output()\n",
    "    logits = tf.matmul(pooled_output , labels_embeddings)\n",
    "\n",
    "    \"\"\"\n",
    "    # Thiết lập phương thức khởi tạo \n",
    "    def __init__(self, config , is_training , input_ids , \n",
    "                 input_mask = None , token_type_ids=None , \n",
    "                 use_one_hot_embeddings=False, scope=None):\n",
    "        \"\"\"Constructor for BertModel.\n",
    "\n",
    "        Args:\n",
    "        config: `BertConfig` instance.\n",
    "        is_training: bool. true for training model, false for eval model. Controls\n",
    "            whether dropout will be applied.\n",
    "        input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
    "        input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "        token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "        use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
    "            embeddings or tf.embedding_lookup() for the word embeddings.\n",
    "        scope: (optional) variable scope. Defaults to \"bert\".\n",
    "\n",
    "        Raises:\n",
    "        ValueError: The config is invalid or one of the input tensor shapes\n",
    "            is invalid.\n",
    "        \"\"\"\n",
    "        # tạo một biến config = giá trị sao chép của config \n",
    "        config = copy.deepcopy(config)\n",
    "        # kiểm tra xem có đang khoong owr trong trạng thái huấn luyện không \n",
    "        if not  is_training: \n",
    "            # gán cho biến rời bở cho các lớp trạng thái ẩn  = 0.0\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            # và tham số rời bỏ của lớp attention = 0.0\n",
    "            config.attemtion_probs_ropout_prob = 0.0\n",
    "\n",
    "        # truwowngf hopwj conf laij tuwcs đang ở trong trạng thái đào tạo \n",
    "        # lấy ra hình dạng đầu vào input_shape  từ input_ids theo 2 chiều \n",
    "        input_shape = get_shape_list(input_ids , expected_rak =2)\n",
    "        # lấy ra kích thước lô và độ dài chuỗi đầu vào từ input_shape\n",
    "        batch_size =input_shape[0]\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        # kiểm tả trường hợp có tồn tại mặt nạ tokens không \n",
    "        if input_mask is None: \n",
    "            # nếu không tồn tại gán input_mask = tensor one shape[batch_size , seq_length]\n",
    "            input_mask = tf.one(shape=[batch_size , seq_length] , stype=tf.int32)\n",
    "\n",
    "        # kiểm tra xem danh sách token_type_ids có tồn tại không \n",
    "        # token_type_ids là 1 danh sách chứa nhãn cho các câu theo thứ tự bắt đầu từ 0-> n câu \n",
    "        if token_type_ids is None: \n",
    "            # nếu không tồn tại gán token_type_ids = tensor zeros[batch_size seq_length]\n",
    "            token_type_ids = tf.zeros(shape =[batch_size , seq_length] , dtype=tf.int32)\n",
    "        \n",
    "        # tạo một biến phạm vi ngữ cảnh cho mô hình bert \n",
    "        # sử dụng with để đặt tất cả thuộc phạm vi \n",
    "        with tf.variable_creator_scope(scope , default_name=\"bert\"):\n",
    "            # tiếp tục đặt một biến nhúng phạm vi \n",
    "            with tf.variable_creator_scope(\"embeddings\"):\n",
    "                \"\"\"Thực hiện tra cứu nhúng trên phạm vi từ.\"\"\"\n",
    "                # lấy ra kêt quả nhúng và bảng kết quả giá trị nhúng bằng cách truy suất qua \n",
    "                # phương thức nhúng tra cứu \n",
    "                (self.embedding_output , self.embedding_tabel) = embedding_lookup(\n",
    "                    input_ids= input_ids , \n",
    "                    vocab_size = config.vocab_size, \n",
    "                    embedding_size = config.hidden_size , \n",
    "                    word_embedding_name = \"wword_embeddings\", \n",
    "                    use_one_hot_embeddings= use_one_hot_embeddings\n",
    "                )\n",
    "\n",
    "                # Thêm nhúng vị trí và kiểu nhúng mã thông báo , sau đó bình thường hóa lớp \n",
    "                # và biểu diễn rời bỏ \n",
    "                self.embedding_output = embedding_postprocessor(\n",
    "                    input_tensor=self.embedding_output,\n",
    "                    use_token_type=True,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    token_type_vocab_size=config.type_vocab_size,\n",
    "                    token_type_embedding_name=\"token_type_embeddings\",\n",
    "                    use_position_embeddings=True,\n",
    "                    position_embedding_name=\"position_embeddings\",\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    max_position_embeddings=config.max_position_embeddings,\n",
    "                    dropout_prob=config.hidden_dropout_prob)\n",
    "\n",
    "            # Tạo một biến phạm vi cho bộ phận mã hóa của Bert \n",
    "            with tf.variable_creator_scope(\"encoder\"):\n",
    "                # điều này chuyển đổi một mặt ạn 2D  với hình dạng shape [batch_size , seq_length]\n",
    "                # thành một mặt nạ 3D với hình dạng shape [batch_size , seq_length , seq_length] \n",
    "                # để khi sử dụng cho lớp attention (điểm chú ý của lớp attention)\n",
    "                attention_mask = create_attention_mask_from_input_mask(\n",
    "                    input_ids, input_mask)\n",
    "                \n",
    "                # chạy máy biến áp xếp trồng chuỗi đầu ra seq_length shape = [batch_size , seq_length , hidded_size]\n",
    "                self.all_encoder_layers = tranformer_model(\n",
    "                    input_tensor=self.embedding_output,\n",
    "                    attention_mask=attention_mask,\n",
    "                    hidden_size=config.hidden_size,\n",
    "                    num_hidden_layers=config.num_hidden_layers,\n",
    "                    num_attention_heads=config.num_attention_heads,\n",
    "                    intermediate_size=config.intermediate_size,\n",
    "                    intermediate_act_fn=get_activation(config.hidden_act),\n",
    "                    hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    do_return_all_layers=True\n",
    "\n",
    "                )\n",
    "\n",
    "                # lấy ra kích thước theo chiều cuối cùng của all_encoder_layers \n",
    "                # và gán kích thước này vào biến sequence_output\n",
    "                self.sequence_output = self.all_encoder_layers[-1]\n",
    "                # chuyển đổi các lớp gộp mã hóa chuỗi 1 tensor shape = [batch_size , seq_length, hidden_size]\n",
    "                # thành 1 tensor shape [batch_size , hidden_size]. Điều này cần thiết cho cấp bận phân đoạn \n",
    "                # hay hoặc cấp độ cặp phân khúc các nhiệm vụ phân loại trong đó cần biểu diễn chiều cố định \n",
    "                # của phân loại \n",
    "                # tạo một biến phạm vi ngữ cảnh cho các lớp gộp \n",
    "                with tf.variable_creator_scope('pooler'):\n",
    "                    # gộp mô hình bằng cách lấy trạng thái ẩn tương ứng cho token đầu tiên . \n",
    "                    # cho rằng điều này đã được đào tạo trước . \n",
    "                    # self.sequence_output[:, 0:1, :]: Đoạn mã này sử dụng cắt tỉa trên các chiều của\n",
    "                    #  self.sequence_output. Trong trường hợp này, nó giữ lại tất cả các chiều từ 0 đến 1 \n",
    "                    # trên chiều thứ hai (axis 1), và toàn bộ các giá trị trên các chiều khác (chiều thứ nhất và chiều thứ ba). \n",
    "                    # Do đó, đầu ra của nó sẽ có kích thước là [batch_size, 1, hidden_size].\n",
    "                    first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :] , axis=1)\n",
    "                    # ánh xạ đầu ra pooler layer đầu ra của lớp gộp qua một mạng dense \n",
    "                    self.pooled_output = tf.keras.layers.Dense(\n",
    "                        first_token_tensor,\n",
    "                        config.hidden_size,\n",
    "                        activation=tf.tanh,\n",
    "                        kernel_initializer=create_initializer(config.initializer_range)\n",
    "                    )\n",
    "\n",
    "    # xây dựng phương thức trả về kết quả của lớp ẩn \n",
    "    def get_pooled_output(self):\n",
    "        return self.pooled_output\n",
    "    \n",
    "    # tương tự xây dựng một phương thức trả về đầu ra tuần tự là trạng thái ẩn của khối encoder \n",
    "    def get_sequence_ouput(self):\n",
    "        \"\"\"Nhận lớp ẩn cuối cùng của bộ phận encoder\n",
    "        \n",
    "        Return : \n",
    "            Một tensor type = float có dạng [batch_size , seq_length,  hidden_size] tương ứng \n",
    "            với lớp ẩn cuối cùng của khối encoder Transformer. \n",
    "        \"\"\"\n",
    "\n",
    "        return self.sequence_output \n",
    "    \n",
    "    # xây dựng một phương thức trả về một loạt các lớp encoder trong của bert \n",
    "    def get_all_encoder_layers(self):\n",
    "        return self.all_encoder_layers\n",
    "    \n",
    "    # xây dựng phương thức trả về kết quả của lớp nhúng \n",
    "    def get_embedding_output(self):\n",
    "        \"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n",
    "            Nhận đầu ra của tra cứu nhúng . đây là đầu vào của Transformer Encoder \n",
    "        Returns:\n",
    "            float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
    "            to the output of the embedding layer, after summing the word\n",
    "            embeddings with the positional embeddings and the token type embeddings,\n",
    "            then performing layer normalization. This is the input to the transformer.\n",
    "        \"\"\"\n",
    "        return self.embedding_output\n",
    "\n",
    "    # xây dựng phươnh thức trả về bảng tra cứu nhúng văn bản \n",
    "    def get_embedding_table(self):\n",
    "        return self.embedding_table\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    Args:\n",
    "        x: float Tensor to perform activation.\n",
    "\n",
    "    Returns:\n",
    "        `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "\n",
    "\n",
    "def get_activation(activation_string):\n",
    "    \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
    "\n",
    "    Args:\n",
    "        activation_string: String name of the activation function.\n",
    "\n",
    "    Returns:\n",
    "        A Python function corresponding to the activation function. If\n",
    "        `activation_string` is None, empty, or \"linear\", this will return None.\n",
    "        If `activation_string` is not a string, it will return `activation_string`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: The `activation_string` does not correspond to a known\n",
    "        activation.\n",
    "    \"\"\"\n",
    "\n",
    "    # We assume that anything that\"s not a string is already an activation\n",
    "    # function, so we just return it.\n",
    "    if not isinstance(activation_string, six.string_types):\n",
    "        return activation_string\n",
    "\n",
    "    # \n",
    "    if not activation_string:\n",
    "        return None \n",
    "\n",
    "    act = activation_string.lower()\n",
    "    if act == \"linear\":\n",
    "        return None\n",
    "    elif act == \"relu\":\n",
    "        return tf.nn.relu\n",
    "    elif act == \"gelu\":\n",
    "        return gelu\n",
    "    elif act == \"tanh\":\n",
    "        return tf.tanh\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation: %s\" % act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng phương thức có chức năng lấy một phần bản đồ gán giữa các biến hiện tại\n",
    "# và các biến trong một checkpoint \n",
    "def get_assigment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "    \"\"\"Compute the union of the current variables checkpoint variables.\n",
    "        Tính toán sự khác biệt của các biến hiện tại và các biến điểm kiểm tra \n",
    "    \n",
    "    \"\"\"\n",
    "    # khưởi tạo 2 từ điển assigment dẽ chứa các cặp key và value là tên của biến đã được khởi tạo \n",
    "    # từ checkpoint . \n",
    "    assignment_map = {}\n",
    "    # và từ điển init này sẽ chứa các khóa là tên của các biến trong mô hình hiện tại và các giá \n",
    "    # trị là đối tượng biến tương ứng . \n",
    "    initialized_variable_names = {}\n",
    "\n",
    "    # tạo một từ điển có thứ tự có các mục được sắp xếp theo thứ tự chúng được thêm vào \n",
    "    name_to_variable = collections.OrderedDict()\n",
    "    # duyệt qua dạn sách tvar là một danh sách các biến huấn luyện trong mô hình hiện tại , \n",
    "    # được truyền vào hàm như một tham số đầu vào \n",
    "    for var in tvars: \n",
    "        # gán biến name = var.name tức là thuộc tính name của biến var \n",
    "        name = var.name \n",
    "        # gán biến m bằng kết quả của hàm re.match với tham số đầu vào là một biểu thức chính quy \n",
    "        # ((.*):\\d+) . có nghĩa là bắt đầu bằng bất kỳ thứ tự nào , theo sau là dấu 2 chấm , rồi kết thúc \n",
    "        # băng một hiawcj nhiều chữ số vd : \"weight:0\", nếu name khớp trả về 1 đối tượng match \n",
    "        # còn lại trả về None 4\n",
    "        m = re.match(\"^(.*):\\\\d+$\", name)\n",
    "        # kiểm tra xem m có tồn tại không \n",
    "        if m is not None:\n",
    "            # nếu có ta nhóm m lại thành nhóm thứ nhất \n",
    "            name = m.group(1)\n",
    "        # thêm các giá trị name và giá trị tương ứng vào từ điển theo thứ tự \n",
    "        name_to_variable[name] = var\n",
    "\n",
    "    # tạo một danh sách các biến từ init_checkpoint là một chuỗi chỉ định đường dẫn đến tập checkpoint \n",
    "    # được truyền vào hàm như một tham số đầu vào . Hàm list_variable dùng để lấy một tập tên và kích \n",
    "    # thước của các biến trong cjeckpoint\n",
    "    init_vars = tf.train.list_variables(init_checkpoint)\n",
    "\n",
    "    # tạo một từ điển mà các mục được sắp xếp theo thứ tự mà chúng được thêm vào \n",
    "    assignment_map = collections.OrderedDict()\n",
    "    # duyệt qua danh sách init_vars\n",
    "    for x in init_vars:\n",
    "        # tríc xuất ra tên và giá trị của các biến tương ứng \n",
    "        (name , var) = (x[0], x[1])\n",
    "        # kiểm tra xem tên biến có tồn tại trong từ điển name_to_variable không \n",
    "        if name not in name_to_variable: \n",
    "            # nếu không tồn tại thì tiếp tục \n",
    "            continue \n",
    "        # trường hợp tức là có tồn tại \n",
    "        # tạo một từ điển map chứa tên của các biến được huân luyện trong mô hình \n",
    "        assignment_map[name] = name\n",
    "        # Thêm hai mục vào initialized_variable_names, có khóa là name và name + “:0”,\n",
    "        # và giá trị đều là 1. \n",
    "        # Điều này có nghĩa là đánh dấu biến name đã được khởi tạo từ checkpoint.\n",
    "        initialized_variable_names[name] = 1\n",
    "        initialized_variable_names[name + \":0\"] = 1\n",
    "\n",
    "    return (assignment_map, initialized_variable_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng phương thức rời bỏ dropout \n",
    "def dropout(input_tensor, dropout_prob):\n",
    "    \"\"\"Perform dropout.\n",
    "\n",
    "    Args:\n",
    "        input_tensor: float Tensor.\n",
    "        dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
    "        *keeping* a dimension as in `tf.nn.dropout`).\n",
    "\n",
    "    Returns:\n",
    "        A version of `input_tensor` with dropout applied.\n",
    "    \"\"\"\n",
    "    # kiểm tra xem có tồn tại tỷ lệ dropout hoặc tywr lệ này = 0\n",
    "    if dropout_prob is None or dropout_prob == 0.0:\n",
    "        # thì trả về nguyên kết quả là tensor đầu vào lớp này \n",
    "        return input_tensor\n",
    "    # trường hợp có tồn tại tỷ lệ bỏ học \n",
    "    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
    "    # ta trả về kết quả cho đầu vào được biến đổi \n",
    "    return output\n",
    "\n",
    "# Xây phương thức khởi tạo cho lớp layernormalization \n",
    "def layer_norm(input_tensor, name=None):\n",
    "    \"\"\"Run layer normalization on the last dimension of the tensor\"\"\"\n",
    "    # trả về kết quả của tensor sau khi chuyển qua lớp layernorm\n",
    "    return tf.contrib.layers.layer_norm(\n",
    "        inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name\n",
    "    )\n",
    "# xây dựng một phương thức kết hợp của lớp drôput và layernorm \n",
    "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
    "    \"\"\"Runs layer normalization followed by dropout.\"\"\"\n",
    "    # add layernorma\n",
    "    output_tensor = layer_norm(input_tensor, name)\n",
    "    # add dropout layer\n",
    "    output_tensor = dropout(output_tensor, dropout_prob)\n",
    "    return output_tensor\n",
    "\n",
    "# xây dựng một phương thức khởi tạo các trọng số cho mô hình \n",
    "def create_initializer(initializer_range=0.02):\n",
    "    \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
    "    # khởi tạo các trọng số với tỷ lệ được cắt bớt bởi độ lệch chuẩn . \n",
    "    return tf.truncated_normal_initializer(stddev=initializer_range)\n",
    "\n",
    "\n",
    "# Xây dựng một lớp tra cứu nhúng \n",
    "def embedding_lookup(input_ids , vocab_size, embedding_size=128 , \n",
    "                     initializer_range= 0.02 , word_embedding_name =\"word_embeddings\",\n",
    "                     use_one_hot_embeddings=False\n",
    "                     ):\n",
    "    \"\"\"Looks up words embeddings for id tensor.\n",
    "        Tra cứu các từ nhúng cho id tensor . \n",
    "\n",
    "    Args: \n",
    "        input_size: int . Size of the embedding vocabulary . \n",
    "        vocab_size: int. Size of the embedding vocabulary.\n",
    "        embedding_size: int. Width of the word embeddings.\n",
    "        initializer_range: float. Embedding initialization range.\n",
    "        word_embedding_name: string. Name of the embedding table.\n",
    "        use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
    "\n",
    "        embeddings. If False, use `tf.gather()` vocab_size: int. Size of the embedding vocabulary.\n",
    "        embedding_size: int. Width of the word embeddings.\n",
    "        initializer_range: float. Embedding initialization range.\n",
    "        word_embedding_name: string. Name of the embedding table.\n",
    "        use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
    "        embeddings. If False, use `tf.gather()`\n",
    "\n",
    "    Returns: \n",
    "        float Tensor of shape [batch_size , seq_length , embedding_size].\n",
    "    \"\"\"\n",
    "    # This function assumes that the input is of shape [batch_size, seq_length,\n",
    "    # num_inputs].\n",
    "    # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
    "    # reshape to [batch_size, seq_length, 1].\n",
    "    \n",
    "    # kiểm tra xem hình dạng của tensor input_ids hiện tại có số chiều n_dims == 2 hay không \n",
    "    if input_ids.shape.ndims == 2:\n",
    "        # thêm vào tensor input_ids 1 chiều shape = 1 vào vị trí cuối cùng của tensor \n",
    "        # kết quả ta nhận được 1 tensor input_ids shape = [batch_size , seq_length , 1]\n",
    "        input_ids = tf.expand_dims(input_ids , axis=[-1])\n",
    "    \n",
    "    # gán biến embedding cho kết quả của hàm tf.get_variable hàm này trả về một kết quả là một\n",
    "    # bảng nhúng từ một bộp từ vựng có kích thước vocab_size từ kích thước nhúng = embedding_size \n",
    "    # \n",
    "    embedding_table = tf.get_variable(\n",
    "        name = word_embedding_name , \n",
    "        shape = [vocab_size , embedding_size],\n",
    "        initializer = create_initializer(initializer_range)\n",
    "    )\n",
    "\n",
    "    # tạo một ma trận flat_input có shape = [batch_size , seq_length], là chiều cuối cùng của tensor input_ids\n",
    "    # với giá trị - 1 nghĩa là chiều còn lại sẽ được tính tự động dựa vào số chiều còn lại \n",
    "    flat_input_ids = tf.reshape(input_ids , [-1])\n",
    "    # kiểm tra xem use_one_hot_embeddings có tồn tại không \n",
    "    if use_one_hot_embeddings: \n",
    "        # tạo một biến là use_one_hot_ids = 1 ma trận one_hot shape = [batch_size , sqg_length, vocab_size ]\n",
    "        one_hot_input_ids = tf.one_hot(flat_input_ids , depth=vocab_size)\n",
    "        # Tạo một tensor ouput bằng kết quả của phép nhân tensor on_hot_input_ids , embedding_table \n",
    "        # shape [batch_size ,seq_length, vocab_size] * [vocab_size , embeddinh_size]\n",
    "        # trả về shape = [batch_size ,seq_length, embeding_size]\n",
    "        output = tf.matmul(one_hot_input_ids , embedding_table)\n",
    "\n",
    "    # trường hợp còn lại tức không tồn tại one_hot_embeddings \n",
    "    else: \n",
    "        # sử dụng hàm tập trung tf.gather để kết hợp 2 tensor embedding_tabel và flat_input\n",
    "        # shape = [vocab_size , embedding_size] và [batch_size * seq_length]\n",
    "        # giá trị mới tensor kết quả là các phần tử của params được lấy theo các chỉ số của indices\n",
    "        # shape = [batch_size ,seq_length, embeding_size] \n",
    "        # ở đây ta đặt mặc định axis = 0 tức là sẽ bỏ đi chiều đầu của tensor prams và lấy \n",
    "        # kích thước theo ma trận indices \n",
    "        output = tf.gather(prams=embedding_table , indices=flat_input_ids)  \n",
    "\n",
    "        # sử dụng phương thức get_shape_list để lấy ra tất acr hình dnagj của 1 tensor dưới dạng số nguyên \n",
    "        input_shape = get_shape_list (input_shape)\n",
    "\n",
    "        # định hình lại kích thước đầu ra với hình dnagj input_shape \n",
    "        # = [batch_size, seq_length ,num_inputs].\n",
    "        # và shape = [batch_size ,seq_length, embeding_size] \n",
    "        output = tf.reshape(output , input_shape[0:, -1] # lấy 2 kích thước đầu \n",
    "                            # kết quả sẽ được 1 tensor shape  = (batch_size, seq_length, num_inputs * embedding_size)\n",
    "                            + [input_shape[-1] * embedding_size])\n",
    "        \n",
    "        # trả về kết quả và bản tra cứu nhúng \n",
    "        return (output, embedding_table)\n",
    "    \n",
    "\n",
    "# xây dựng phương thức embedding_posprocessor (phương thức xử lý nhúng hậu kỳ)\n",
    "# Thực hiện các bước xử lý sau cùng trên một tensor nhúng từ \n",
    "# trả về 1 tensor có cùng kích thước với tensor đầu vào \n",
    "\n",
    "\n",
    "def embedding_postprocessor(input_tensor,\n",
    "                            use_token_type=False,\n",
    "                            token_type_ids=None,\n",
    "                            token_type_vocab_size=16,\n",
    "                            token_type_embedding_name=\"token_type_embeddings\",\n",
    "                            use_position_embeddings=True,\n",
    "                            position_embedding_name=\"position_embeddings\",\n",
    "                            initializer_range=0.02,\n",
    "                            max_position_embeddings=512,\n",
    "                            dropout_prob=0.1):\n",
    "    \"\"\"Thực hiện quá trình sử lý hậu kỳ trên một tensor nhúng từ. \n",
    "       Performs various post-processor on a word embedding tensor. \n",
    "\n",
    "    Args: \n",
    "        input_tensor: float Tensor of shape [batch_size, seq_length,\n",
    "            embedding_size].\n",
    "        use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
    "        token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "            Must be specified if `use_token_type` is True.\n",
    "\n",
    "        token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
    "        token_type_embedding_name: string. The name of the embedding table variable\n",
    "            for token type ids.\n",
    "\n",
    "        use_position_embeddings: bool. Whether to add position embeddings for the\n",
    "            position of each token in the sequence.\n",
    "        position_embedding_name: string. The name of the embedding table variable\n",
    "            for positional embeddings.\n",
    "\n",
    "        initializer_range: float. Range of the weight initialization.\n",
    "        max_position_embeddings: int. Maximum sequence length that might ever be\n",
    "            used with this model. This can be longer than the sequence length of\n",
    "        input_tensor, but cannot be shorter.\n",
    "        dropout_prob: float. Dropout probability applied to the final output tensor.\n",
    "\n",
    "    Returns:\n",
    "        float tensor with same shape as `input_tensor`.\n",
    "\n",
    "    Raises: \n",
    "        ValueError : One of the tensor shapes or input values is invalid.\n",
    "    \"\"\"  \n",
    "    # lấy ra hình dạng của input_tensor với số chiều mong đợi = 3\n",
    "    input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "    # gán 3 biến batch_size , seq_length , width với giá trị là \n",
    "    # kích thước của 3 chiều tương ứng \n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "    width = input_shape[2]\n",
    "\n",
    "    # gán tensor đâu ra = tensor đâu vào \n",
    "    output = input_tensor\n",
    "\n",
    "    # kiểm tra xem biến bool use_token_type là biến cho biết có thêm phần nhúng cho token_type_id \n",
    "    # hay không\n",
    "    if use_token_type: \n",
    "        # đồng thời kiểm tra xem token_type_id tensor này có tồn tại không \n",
    "        if token_type_ids is None: # nếu không \n",
    "            # trả về một cảnh báo \n",
    "            raise ValueError (\"`token_type_ids` must be specified if\"\n",
    "                       \"`use_token_type` is True.\")\n",
    "    \n",
    "    # tạo một biến token_type_table có chức năng dùng để tra cứu bộ nhúng cho token_type_id\n",
    "    # kết quả trả về  tensor có chứa các vector nhúng cho các loại token khác nhau \n",
    "    # kết qyar này sẽ được sử dụng để thêm vào tensor nhúng từ cho loại tokens của mỗi token trong \n",
    "    # chuỗi đầu vào \n",
    "    token_type_table = tf.get_variable(\n",
    "        name = token_type_embedding_name, \n",
    "        shape =[token_type_vocab_size, width],\n",
    "        initializer = create_initializer(initializer_range)\n",
    "    )\n",
    "\n",
    "    # từ điển điển này sẽ nhỏ nên cần thực hiện one-hot ở đây vì nó luôn nhanh hơn đối \n",
    "    # với từ vựng nhỏ \n",
    "    # tạo một tensor flat_token có hình dạng bằng với tensor token_type_ids \n",
    "    # với tham số - được hiểu là chiều cuối cùng sẽ được suy ra dựa vào 2 chiều còn lại \n",
    "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
    "    # tạo một tensor one_hot shape = [batch_size , seq_length, token_type_vocab_size ]\n",
    "    one_hot_ids = tf.one_hot(flat_token_type_ids, dept=token_type_vocab_size)\n",
    "    \n",
    "    # nhân 2 ma trận là one_hot_ids và token_type_table với phép nhân hàng và cột matmul \n",
    "    # [batch_size , seq_length, token_type_vocab_size ] * [token_type_vocab_size, width]\n",
    "    # kết quả là 1 tensor mới shape = [batch_size * seq_length , width]\n",
    "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
    "\n",
    "    # định hình lại tensor này với hình dạng batch_Size , seq_length , width \n",
    "    token_type_embeddings = tf.reshape(token_type_embeddings,\n",
    "                                       [batch_size, seq_length, width])\n",
    "    # sau đó cộng ma trận đầu ra với am trận này \n",
    "    output += token_type_embeddings\n",
    "\n",
    "    # kiểm tra xem use_position_embedding có tồn tại không \n",
    "    # đây là ma trận nhúng vị trí các tokens \n",
    "    if use_position_embeddings: \n",
    "        # tạo một phép kiểm tra để đảm bảo rằng seq_length của 1 tensor đầu vào không vượt quá \n",
    "        # max_position_len chỉ số tối đa của 1 đoạn đầu vào được đánh dấu \n",
    "        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
    "        # tạo một ngữ cảnh quản lý để đảm bảo răng các hoạt động trong khối này chỉ được \n",
    "        # thực hiện sau khi assert_op được thực hiện \n",
    "        with tf.control_dependencies([assert_op]):\n",
    "            # tạo một biên mới full_position nhận kết quả chứa danh sách các vector nhúng cho các\n",
    "            # tensor đầu vào \n",
    "            full_position_embeddings = tf.get_variable(\n",
    "                name=position_embedding_name,\n",
    "                shape=[max_position_embeddings, width],\n",
    "                initializer=create_initializer(initializer_range))\n",
    "\n",
    "            # khởi tạo mọt tensor  nhúng bắt đầu  0-> seq_length - 1 với hàm slice\n",
    "            position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
    "                                     [seq_length, -1])\n",
    "            # Gán giá trị cho biến num_dims bằng cách lấy độ dài của danh sách chứa kích thước của tensor output\n",
    "            num_dims = len(output.shape.as_list())\n",
    "\n",
    "            # tạo một danh sách position_broadcast để chứa kích thước thước của ma trận \n",
    "            # position embeddings  danh sách này có số phần tử bằng num_dims là số chiều \n",
    "            # của ma trận position_embedding\n",
    "            position_broadcast_shape = []\n",
    "            # lặp 1 lần từ 0 -> số num_dim -1 \n",
    "            for _ in range(num_dims - 2):\n",
    "                # gán cho các phần tử đầu tiên của danh sách = 1\n",
    "                position_broadcast_shape.append(1)\n",
    "            # còn 2 phần  tử cuối cùng của dnah sách được gán bằng  seq_length và width \n",
    "            position_broadcast_shape.extend([seq_length, width])\n",
    "            # định hình lại ma trận position_emnbeddings theo kích thước của ma trận position_boardcast\n",
    "            # mục đích là để có thể cộng được nó với ma trận output \n",
    "            position_embeddings = tf.reshape(position_embeddings,\n",
    "                                            position_broadcast_shape)\n",
    "            # công ma trận output với ma trận position_embedding \n",
    "            # kết quả là tensor shape = [batch_size , seq_lengh ,width]\n",
    "            output += position_embeddings\n",
    "    \n",
    "    # áp dụng phương thức chuẩn hóa cho lớp đầu ra \n",
    "    output = layer_norm_and_dropout(output, dropout_prob)\n",
    "    # trả về kết quả sau khi được chuẩn hóa \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dưng một phương thức tạp mặt nạ chú ý từ đầu vào \n",
    "def create_attention_mask_from_input_mask(from_tensor , to_mask):\n",
    "    \"\"\"Create 3d Attention mask from 2D tensor mask. \n",
    "        Tạo ra một mặt nạ chú ý 3 chiều từ tensor mask 2 chiều. \n",
    "\n",
    "    Args: \n",
    "        from_tensor : 2D or 3D Tensor of shape [batch_size , from_seq_length,...].\n",
    "        to_mask : Int32 Tensor of shape [batch_size, to_seq_length].\n",
    "\n",
    "\n",
    "    Returns: \n",
    "        Float tensor of shape [batch_size, from_seq_length , to_seq_length].\n",
    "    \"\"\"\n",
    "    # tạo một biến from_shape = hình dạng của tensor đầu vào from_tensor là một ma trận có 2\n",
    "    # hoặc 3 chiều ta sẽ lấy toàn bộ số chiều của nó\n",
    "    from_shape = get_shape_list(from_tensor, expected_rank=[2 ,3])\n",
    "    # gán batch_size  = kích thước from_shape theo chiều 0\n",
    "    batch_size = from_shape[0]\n",
    "    # tương tự lấy ra seq_length \n",
    "    from_seq_length = from_shape[1]\n",
    "\n",
    "    # lấy ra hình dạng của tensor mask là 1 tensor 2 chiều \n",
    "    to_shape = get_shape_list(to_mask , expected_rank=2)\n",
    "    # gán cho biến to_seq_length = chiều thứ 2 được lấy bởi to_shape\n",
    "    to_seq_length = to_shape[1]\n",
    "\n",
    "    # tạo một tensor là to_mask dtype = float 32\n",
    "    # shape = [batch_size , seq_length] mục đích để khi nhân tensor này với \n",
    "    # tensor boardcasr_one để có được tensor đầu ra mong muồn \n",
    "    to_mask = tf.cast(\n",
    "        tf.reshape(to_mask , [batch_size, 1 , to_seq_length] , dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    # ta tạo một tensor boadcast_one tươ ng tụ \n",
    "    broadcast_ones = tf.ones(\n",
    "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
    "\n",
    "    # Here we broadcast along two dimensions to create the mask.\n",
    "    mask = broadcast_ones * to_mask\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# xây dựng lớp chú ý cho khối transformer \n",
    "\n",
    "def attention_layer(\n",
    "        from_tensor ,to_tensor, attention_mask=None , num_attention_heads = 1, \n",
    "        size_per_head = 512 , query_act =None , key_act =None , value_act =None ,\n",
    "        attention_probs_dropout_prob = 0.0, initializer_range=0.02, \n",
    "        do_return_2d_tensor =False , batch_size =None , from_seq_length=None ,\n",
    "        to_seq_length=None\n",
    "    ):\n",
    "\n",
    "    \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
    "\n",
    "        This is an implementation of multi-headed attention based on \"Attention\n",
    "        is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
    "        this is self-attention. Each timestep in `from_tensor` attends to the\n",
    "        corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
    "\n",
    "        This function first projects `from_tensor` into a \"query\" tensor and\n",
    "        `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
    "        of tensors of length `num_attention_heads`, where each tensor is of shape\n",
    "        [batch_size, seq_length, size_per_head].\n",
    "\n",
    "        Then, the query and key tensors are dot-producted and scaled. These are\n",
    "        softmaxed to obtain attention probabilities. The value tensors are then\n",
    "        interpolated by these probabilities, then concatenated back to a single\n",
    "        tensor and returned.\n",
    "\n",
    "        In practice, the multi-headed attention are done with transposes and\n",
    "        reshapes rather than actual separate tensors.\n",
    "\n",
    "    Args: \n",
    "        from_tensor : float multi_headed attention of shape [batch_size , sqe_length , from_width]\n",
    "        to_tensor: Float tensor of shape [batch_size , to_seq_length , to_width].\n",
    "        attention_mask: (không bắt buộc) : int32 Tensor of shape [batch_size , seq_length , to_seq_length].\n",
    "            The values should 0 or 1. The attention scores will effectively be set to -infinity for any \n",
    "            positions in the mask tha are 0 , and will be unchangeed for positions that are 1. \n",
    "        num_attention_heads : int . Number of attention heads . \n",
    "        size_per_head : Int . Size of each attention head. \n",
    "        query_act: (optional) Activation function for the query transform.\n",
    "        key_act: (optional) Activation function for the key transform.\n",
    "        value_act: (optional) Activation function for the value transform.\n",
    "        attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
    "            attention probabilities.\n",
    "\n",
    "        initializer_range: float. Range of the weight initializer. \n",
    "        do_return 2d_tensor: bool .If True , the output will be of shape [batch_size * from_seq_length,\n",
    "            num_attention_heads * size_per_head]. If false, the output will be of shape [batch_size , from_seq_length,\n",
    "            num_attention_heads * size_per_head].\n",
    "        \n",
    "        \n",
    "        batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
    "            of the 3D version of the `from_tensor` and `to_tensor`.\n",
    "        from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "            of the 3D version of the `from_tensor`.\n",
    "        to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "            of the 3D version of the `to_tensor`.\n",
    "    \n",
    "    Retuns : \n",
    "        Float Tensor of shape [batch_size , from_seq_length, num_attentiom_heads * size_per_head].\n",
    "            If do return 2d Tensor is True , this will be of shape [batch_size * from_seq_length, \n",
    "             num_attention_heads * size_per_head]\n",
    "\n",
    "     \n",
    "    Raises:\n",
    "        ValueError: Any of the arguments or tensor shapes are invalid.   \n",
    "    \"\"\"\n",
    "\n",
    "    # Xaay duwngj phuwowng thuwcs chuyển đổi hình dnagj cho ma trận score\n",
    "    # là các ma trận q k , v \n",
    "    def transpose_for_scores(input_tensor , batch_size, num_attention_heads, \n",
    "                             seq_length , width):\n",
    "        # ddinhj hinhf lại input tensor shape = [batch_size , seq_length , num_attention, width]\n",
    "        output_tensor = tf.reshape(\n",
    "             input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
    "\n",
    "        # Chuyển vị lại  output_tensor shape = [batch_size ,num_attention , seq_length , width]\n",
    "        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "        # trả về tensor đã được chuyển vị \n",
    "        return output_tensor\n",
    "    \n",
    "    # lấy ra hình dnahj cuảt 2 tensor là from_tensor và to_tensor với kích thước mong đợi\n",
    "    # gán cho nó 2 biến ;à from_shape  và to_shape \n",
    "    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "    to_shape = get_shape_list(to_tensor, expected_rank=[2,3])\n",
    "\n",
    "    # kiểm tra xem số chiều của to_shape và from_shape có bằng nhau không\n",
    "    if len(from_shape) != len(to_shape):\n",
    "        # trả về một cảnh báo lỗi \n",
    "        raise ValueError(\n",
    "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
    "    \n",
    "    # trường hợp nếu số chiều được lấy from_shape = 3\n",
    "    if len(from_shape) == 3:\n",
    "        # lấy ra 2 kích thước 0 , và và gá cho 3 biến là batcj_size , seq_length ,to_seq_length\n",
    "        batch_size = from_shape[0]\n",
    "        from_seq_length = from_shape[1]\n",
    "        to_seq_length = from_shape[1]\n",
    "\n",
    "    # trường hợp < 3 thì ta kiểm tra dàng buộc và đưa ra một cảnh báo lỗi \n",
    "    elif len(from_shape) == 2:\n",
    "        if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
    "            raise ValueError(\n",
    "                \"When passing in rank 2 tensors to attention_layer, the values \"\n",
    "                \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
    "                \"must all be specified.\")\n",
    "\n",
    "     # Scalar dimensions referenced here:\n",
    "    #   B = batch size (number of sequences)\n",
    "    #   F = `from_tensor` sequence length\n",
    "    #   T = `to_tensor` sequence length\n",
    "    #   N = `num_attention_heads`\n",
    "    #   H = `size_per_head`\n",
    "\n",
    "    # reshape 2 tensor from_tensor và to_tensor thành ma trận 2 chiều \n",
    "    from_tensor_2d = reshape_to_matrix(from_tensor)\n",
    "    to_tensor_2d = reshape_to_matrix(to_tensor)\n",
    "\n",
    "    # `query_layer` = [B*F, N*H] shape = [batch_size, seq_length , num_heads , head_dim(size_per_head)]\n",
    "    query_layer = tf.layers.dense(\n",
    "        from_tensor_2d,\n",
    "        num_attention_heads * size_per_head,\n",
    "        activation=query_act,\n",
    "        name=\"query\",\n",
    "        kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "    # `key_layer` = [B*T, N*H] shape = [batch_size, seq_length , num_heads , head_dim(size_per_head)]\n",
    "    key_layer = tf.layers.dense(\n",
    "        to_tensor_2d,\n",
    "        num_attention_heads * size_per_head,\n",
    "        activation=key_act,\n",
    "        name=\"key\",\n",
    "        kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "    # `value_layer` = [B*T, N*H] shape = [batch_size, seq_length , num_heads , head_dim(size_per_head)]\n",
    "    value_layer = tf.layers.dense(\n",
    "        to_tensor_2d,\n",
    "        num_attention_heads * size_per_head,\n",
    "        activation=value_act,\n",
    "        name=\"value\",\n",
    "        kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "    # `query_layer` = [B, N, F, H] chuyển vị tensor q thành tensor có dạng [batch_size ,num_heads , seq_length, nem_per_h ]\n",
    "    # để có thể nhân tensor q với tensor k kết quả nhận được 1 tensor score\n",
    "    query_layer = transpose_for_scores(query_layer, batch_size,\n",
    "                                        num_attention_heads, from_seq_length,\n",
    "                                        size_per_head)\n",
    "\n",
    "    # `key_layer` = [B, N, T, H] chuyển vị tensor q thành tensor có dạng [batch_size ,num_heads , seq_length, nem_per_h ] \n",
    "    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
    "                                    to_seq_length, size_per_head)\n",
    "\n",
    "    # Tính tích số q . K.T để có điuwocj điểm chú ý \n",
    "    # âttantion_score shape = [ batch_size , num_heads , seq_lengt , num_per_head]\n",
    "    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "    # thực hiện phép tính chia điểm score  cho căn bậc 2 dk tức là số chiều biểu diễn nhúng \n",
    "    # kết quả ở đây là 25 \n",
    "    attention_scores = tf.multiply(attention_scores,\n",
    "                                 1.0 / math.sqrt(float(size_per_head)))\n",
    "    \n",
    "    # kiểm tra xem có tồn tại tensor mặt nạ tự chú ý \n",
    "    # attention_mask là 1 tensor int32  shape [batch_size, from_seq_length , to_seq_length]\n",
    "    if attention_mask is not None: \n",
    "        # Thêm 1 chiều có size = 1 vào  vị trí thứ 2 của tensor \n",
    "        # kết quả là một tensor shape = [batch_size ,1 , num_heads , seq_length]\n",
    "        attention_mask = tf.expand_dims(attention_mask , axis=[1])\n",
    "        # mặt nạ chú ý mang các giá trị 1.0 cho các vị trí được giữ lại và 0.0 \n",
    "        # cho vị trí các tokens bị che đi . Thao tác này sẽ tạo ra một tensor\n",
    "        # 0.0 cho các vị trí được giữ lại và -10000 cho các vị trí được che đi \n",
    "        adder = (1.0 -tf.cast(attention_mask, tf.float32)) *- 10000.0\n",
    "\n",
    "        # sau đó cộng ma trận scores là kết quả của phép nhân ma trận q.k \n",
    "        # với mặt nạ chú ý để che đi những chỉ số có điểm score quá nhỏ \n",
    "        attention_scores += adder \n",
    "\n",
    "    # Bình thường hóa attention_scores bằng xác suất với một hàm softmax\n",
    "    attention_probs = dropout(attention_scores)\n",
    "\n",
    "    # Thêm một lớp dropout cho attention \n",
    "    attention_probs = dropout(attention_probs ,attention_probs_dropout_prob )\n",
    "\n",
    "    # định hình lại vector v với kích thước [batch_size , seq_length , num-heads , num_per_head]\n",
    "    value_layer = tf.reshape(\n",
    "        value_layer , [batch_size , to_seq_length , num_attention_heads , size_per_head]\n",
    "    )\n",
    "    # chuyển vị vector V này về hình dạng [batch_size , num_heads , seq_length , num_per_head]\n",
    "    # để có thể thực hiện nhân tensor Attention_probs với vector v như đúng công thức định nghĩa \n",
    "    value_layer = tf.transpose(\n",
    "        value_layer , [0,2,1,3]\n",
    "    )\n",
    "\n",
    "    # Tạo một vector ngữ cản context_layer là kết quả của phép nhân tensor v với kết quả của Q.K.T /^ DK \n",
    "    # kết quả là một tensor mới shape = [ batch_size , num_heads , seq_length, num_per_head]\n",
    "    context_layer = tf.matmul(attention_probs, value_layer)    \n",
    "\n",
    "    # chuyển vị tensor ngữ cảnh về hình dạng ban đầu \n",
    "    # shape = [batch_size , seq_length , num_head , num_per_head]\n",
    "    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
    "\n",
    "    # kiểm tra xem biến do_return _tensor có = True không \n",
    "    # do return tensor là một biến boolen \n",
    "    if do_return_2d_tensor:\n",
    "    # `context_layer` = [B*F, N*H]\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer,\n",
    "            [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
    "    # trường hợp còn lại tức do_retunr  = false \n",
    "    else:\n",
    "        # `context_layer` = [B, F, N*H]\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer,\n",
    "            [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
    "\n",
    "    # Trả về Tensor context \n",
    "    return context_layer\n",
    "\n",
    "\n",
    "\n",
    "# Xây dựng phương thức khởi tạo cho một mô hình Transformer \n",
    "def transformer_model(input_tensor, attention_mask=None, \n",
    "                      hidden_size = 768 , num_hidden_layers= 12, \n",
    "                      num_attention_heads =12, intermediate_size = 3072,\n",
    "                      intermediate_act_fn=gelu,\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False            \n",
    "        ):\n",
    "    \n",
    "    \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
    "\n",
    "        This is almost an exact implementation of the original Transformer encoder.\n",
    "\n",
    "        See the original paper:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "\n",
    "        Also see:\n",
    "        https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
    "\n",
    "    Args:\n",
    "\n",
    "        Input_tensor : Float Tensor of shape [batch_size , seq_length , hidden_size]\n",
    "        attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n",
    "            seq_length], with 1 for positions that can be attended to and 0 in\n",
    "            positions that should not be.\n",
    "        hidden_size: int. Hidden size of the Transformer.\n",
    "        num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n",
    "        num_attention_heads: int. Number of attention heads in the Transformer.\n",
    "        intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n",
    "            forward) layer.\n",
    "        intermediate_act_fn: function. The non-linear activation function to apply\n",
    "            to the output of the intermediate/feed-forward layer.\n",
    "        hidden_dropout_prob: float. Dropout probability for the hidden layers.\n",
    "        attention_probs_dropout_prob: float. Dropout probability of the attention\n",
    "            probabilities.\n",
    "        initializer_range: float. Range of the initializer (stddev of truncated\n",
    "            normal).\n",
    "        do_return_all_layers: Whether to also return all layers or just the final\n",
    "            layer.\n",
    "\n",
    "    Returns:\n",
    "        float Tensor of shape [batch_size, seq_length, hidden_size], the final\n",
    "        hidden layer of the Transformer.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: A Tensor shape or parameter is invalid.\n",
    "    \"\"\"\n",
    "    # kiểm tra xem hidden_size có chia hết cho num_attention_heads \n",
    "    if hidden_size % num_attention_heads != 0:\n",
    "        # nếu không chia hết đưa ra cảnh báo \n",
    "        raise ValueError(\n",
    "            \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "            \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "    \n",
    "    # khởi tạo một biến là attention_head_size biểu thị cho kích thước của mỗi đầu chú ý \n",
    "    attention_head_size = int(hidden_size / num_attention_heads)\n",
    "    # sử dụng hàm get_shape_list để lấy ra kích thước của tensor input với kích thước mong đợi \n",
    "    # là 3 chiều \n",
    "    input_shape = get_shape_list(input_tensor , expected_rank=3)\n",
    "    # gán các chiều lần lượt được lấy bởi input_shape \n",
    "    # lần lượt cho các biến batch_size  , seq_length , input_width . \n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "    input_width = input_shape[2]\n",
    "\n",
    "    # Transformer thực hiện tổng phần dư trên tất cả các lớp đầu vào nên cần có cùng kích thước với \n",
    "    # hidden_size \n",
    "    # kiểm tra xem đầu vào input_width có cùng kích thước với hidden_size \n",
    "    if input_width != hidden_size: \n",
    "        # nếu không có cùng kích thước lập tức đưa ra cnahr báo \n",
    "        raise ValueError (\"The width of input tensor (%d) != hidden size (%d)\" % (input_width , hidden_size))\n",
    "    \n",
    "    # cần giữ biểu diễn dưới dạng tensor 2D để tránh định hình lại nó từ tensor 3D thành tensor 2D\n",
    "    # tạo một biến prev_output  = kết quả của phương thức reshape_to_maxtrix biến đổi 1 tensor 3d thành 2d\n",
    "    prev_output = reshape_to_matrix(input_tensor)\n",
    "\n",
    "    # tạo một danh sách để lưu chữ tất cả các lớp outputs \n",
    "    all_layer_outputs = []\n",
    "    # duyệt qua 1 dnah scahs 0- > num_hidden_layers\n",
    "    for layer_idx in range(num_hidden_layers):\n",
    "        # tạo một biến variable_scope là một lớp theo chỉ số index và\n",
    "        # với with ta đặt tất cả vào phạm vi của nó \n",
    "        with tf.variable_creator_scope(\"layer_%d\", layer_idx):\n",
    "            # gán biến layer_input = prev_output \n",
    "            layer_input = prev_output\n",
    "            # tiếp tục tạo một biến phạm vi attention sử dụng with để xác định phạm vi chủa nó  \n",
    "            with tf.variable_creator_scope(\"attention\"):\n",
    "                # tạo một danh scahs attention_head để lưu trữ danh sách đầu ra cho các lớp attention \n",
    "                attention_heads = []\n",
    "                # tiếp tục tạo một biến phạm vi tự chú ý sử dụng with để xác định phạm vi của nó \n",
    "                with tf.variable_creator_scope(\"self\"):\n",
    "                    # khởi tạo một lớp head_attention gán nó bằng phương thức attention_layer \n",
    "                    attention_head = attention_layer(\n",
    "                        from_tensor=layer_input,\n",
    "                        to_tensor=layer_input,\n",
    "                        attention_mask=attention_mask,\n",
    "                        num_attention_heads=num_attention_heads,\n",
    "                        size_per_head=attention_head_size,\n",
    "                        attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                        initializer_range=initializer_range,\n",
    "                        do_return_2d_tensor=True,\n",
    "                        batch_size=batch_size,\n",
    "                        from_seq_length=seq_length,\n",
    "                        to_seq_length=seq_length\n",
    "                    )      \n",
    "                    # sau đó thêm kết quả của lơp attention này vào dnah sách chứa kết quả của các lớp attention\n",
    "                    attention_heads.append(attention_head)\n",
    "\n",
    "                # tạo một biến attention_output sau đó ta gá đầu ra của lớp attention = None \n",
    "                attention_output = None \n",
    "                # kiểm tra xem số phần tử trong danh sách attention_heads có == 1 \n",
    "                if len(attention_heads) == 1:\n",
    "                    # gán giá trị đầu tiên trong dnah sách attention_heads cho biến attention_output \n",
    "                    attention_output = attention_heads[0]\n",
    "                \n",
    "                else: \n",
    "                    # trường hợp có dãy khác thì ta chỉ ghép vào đầu tự chú ý trước khi chiếu \n",
    "                    # tức là ta gán biến attention_output = kết quả của dnah sách attention_mask \n",
    "                    # theo chiều cuối cùng \n",
    "                    attention_output = tf.concat(attention_heads, axis=-1)\n",
    "\n",
    "                # chạy phép chiếu tuyến tính của hidden_size sau khi thêm phần dư với lớp đầu ra \n",
    "                # tạo một biến phạm vi đầu ra sử dụng with để xác định phạm vi của nó \n",
    "                with tf.variable_creator_scope(\"output\"):\n",
    "                    # chiều tuyến tính kết quả đầu ra của khối attention \n",
    "                    attention_output = tf.keras.layers.Dense(\n",
    "                        attention_output, hidden_size, \n",
    "                        kernel_initializer= create_initializer(initializer_range)\n",
    "                    )\n",
    "                    # thêm 1 lớp xử lý dropout \n",
    "                    attention_output = dropout(attention_output , hidden_dropout_prob)\n",
    "                    # và cuỗi cùng là lớp bình thường hóa \n",
    "                    attention_output = layer_norm(attention_output + layer_input)\n",
    "\n",
    "                # The activation is only applied to the \"intermediate\" hidden layer.\n",
    "                # tạo một biến intermediate (ffn) là một biến phạm vi \n",
    "                # sử dụng with để xác định phạm vi của biến này \n",
    "            with tf.variable_scope(\"intermediate\"):\n",
    "                # gán biến intermediate_ output = kết quả của phéo chiếu tuyến tính \n",
    "                intermediate_output = tf.keras.layers.Dense(\n",
    "                    attention_output,\n",
    "                    # chiều biểu diễn \n",
    "                    intermediate_size,\n",
    "                    # hàm kích hoạt \n",
    "                    activation=intermediate_act_fn,\n",
    "                    # khởi tạo tham só \n",
    "                    kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "            # Down-project back to `hidden_size` then add the residual.\n",
    "            # tương tự tạo mọt biến phạm vi đầu ra sử dụng with để quyết định phạm vị biến \n",
    "            with tf.variable_scope(\"output\"):\n",
    "                #gán cho biến layer_output là kết quả phéo chiếu tuyến tính cho kết qyar của lớp \n",
    "                # ffn ở trê\n",
    "                layer_output = tf.keras.layers.Dense(\n",
    "                    intermediate_output,\n",
    "                    hidden_size,\n",
    "                    kernel_initializer=create_initializer(initializer_range))\n",
    "                # add 1 lớp dropout layers \n",
    "                layer_output = dropout(layer_output, hidden_dropout_prob)\n",
    "                # + add layer_norm\n",
    "                layer_output = layer_norm(layer_output + attention_output)\n",
    "                # gán lại biến prev_output = layer_norm để vòng lặp thực hiện lại từ đầu \n",
    "                prev_output = layer_output\n",
    "                # thêm kết quả này vào danh sách chưá các lớp đầu ra .\n",
    "                all_layer_outputs.append(layer_output)\n",
    "\n",
    "        # kiểm tra xem giá trị của do_return_all_layer có băng True: \n",
    "        if do_return_all_layers: \n",
    "            # tạo một dnah sách lưu trữ đầu ra cuối cùng \n",
    "            final_outputs = []\n",
    "            # duyệt qua dnah cách lưu trữ các đầu ra của các lớp layer_attention trước đó \n",
    "            for layer_output in all_layer_outputs: \n",
    "                # định hình lại thành tensor có hình dạng ban đầu [batch_size, seq_length, hidden_size]\n",
    "                final_output = reshape_from_matrix(layer_output, input_shape)\n",
    "                # thêm nó vào danh sách final_outputs\n",
    "                final_outputs.append(final_output)\n",
    "                # trả về dnah sách đầu ra chứa các tensor với shape [batch_size, seq_length, hidden_size]\n",
    "            return final_outputs\n",
    "        else:\n",
    "            # trườn hợp còn lại tức do_return tensor = False\n",
    "            # thì ta phải lấy kết quả từ biến prev_output và thực hiện tương tự \n",
    "            final_output = reshape_from_matrix(prev_output, input_shape)\n",
    "            # trả về dnah sách đầu ra chứa các tensor với shape [batch_size, seq_length, hidden_size]\n",
    "            return final_output\n",
    "        \n",
    "\n",
    "# xây dựng phương thức trả về hình dạnh mong đợi cho 1 tensor \n",
    "# get_shape_list nhận đầu vào là 1 tensor , số chiều mong đợi , name \n",
    "def get_shape_list (tensor, expected_rank=None , name=None):\n",
    "    \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
    "\n",
    "    Args:\n",
    "        tensor: A tf.Tensor object to find the shape of.\n",
    "        expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
    "        specified and the `tensor` has a different rank, and exception will be\n",
    "        thrown.\n",
    "        name: Optional name of the tensor for the error message.\n",
    "\n",
    "    Returns:\n",
    "        A list of dimensions of the shape of tensor. All static dimensions will\n",
    "        be returned as python integers, and dynamic dimensions will be returned\n",
    "        as tf.Tensor scalars.\n",
    "    \"\"\"\n",
    "    # kiểm tra xem có tồn tại tên của tensor không \n",
    "    if name is None: \n",
    "        # ta gán name = tensor.name \n",
    "        name = tensor.name \n",
    "    \n",
    "    # sau đó kiểm tra xem số chiều mong muốn có = None \n",
    "    if expected_rank  is not None: \n",
    "        # nếu tồn tại giá trị này \n",
    "        # ta kiểm tra 3 giá trị đâu vào \n",
    "        assert_rank (tensor, expected_rank , name)\n",
    "    \n",
    "    # lấy ra hình dnagj của tensor \n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    # tạo ra một dnah scahs none_static_indexes để lưu chỉ các chỉ số kích thước \n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        # kiểm tra xem dim là giá trị nghĩa là kichs thước chiều của tensor có = None\n",
    "        if dim is None: \n",
    "            # thêm các chỉ số index vào dnah sách none_static \n",
    "            non_static_indexes.append(index)\n",
    "            # nếu danh sách none_static không tồn tại \n",
    "    if not non_static_indexes:\n",
    "        # trả về shape \n",
    "        return shape\n",
    "\n",
    "    # lấy ra hình dnagj của tensor \n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    # duyệt qua danh sách các chỉ số của tensor \n",
    "    for index in non_static_indexes:\n",
    "        # gán số kích thước chiều theo chỉ số của tensor shape bằng kích thước chiều \n",
    "        # theo chỉ số của tdyn_shape\n",
    "        shape[index] = dyn_shape[index]\n",
    "        # trả về kết quả \n",
    "    return shape\n",
    "\n",
    "\n",
    "# xây dựng phương thức chuyển đổi hình dnagj tensor thành ma trận \n",
    "def reshape_to_matrix(input_tensor):\n",
    "    \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
    "    # lấy ra số chiều của tensor \n",
    "    ndims = input_tensor.shape.ndims\n",
    "    # nếu tensor < 2 chiều \n",
    "    if ndims < 2:\n",
    "        # đưa ra cảnh báo đầu vào phải có ít 2 chiều \n",
    "        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
    "                        (input_tensor.shape))\n",
    "    # trường hợp có 2 chiều \n",
    "    if ndims == 2:\n",
    "        # trả về kích thước mặt định của đầu vào \n",
    "        return input_tensor\n",
    "\n",
    "    # lấy chiều cuối cùng gán vào biến width\n",
    "    width = input_tensor.shape[-1]\n",
    "    # định hình lại tensor input và bỏ đi 1 chiều với tham số  -1 nghĩa là nó sẽ tự suy ra dựa vào width\n",
    "    output_tensor = tf.reshape(input_tensor, [-1, width])\n",
    "    # trả về kết quả \n",
    "    return output_tensor\n",
    "\n",
    "\n",
    "# xây dựng phuuwong thức chuyển 1 ka trận 2d thành 1 tensor 3D \n",
    "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
    "    \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
    "    if len(orig_shape_list) == 2:\n",
    "        return output_tensor\n",
    "\n",
    "    # lấy ra các chiều của tensor output_tensor \n",
    "    output_shape = get_shape_list(output_tensor)\n",
    "\n",
    "    orig_dims = orig_shape_list[0:-1] # lấy ra 2 chiều đầu tiên của tensor \n",
    "    # lấy ra kích thước theo chiều cuối cùng từ  output_shape\n",
    "    width = output_shape[-1]\n",
    "    # thêm chiều cuối cùng có shape = width để nó thành 1 tensor 3 chiều \n",
    "    return tf.reshape(output_tensor, orig_dims + [width])\n",
    "\n",
    "\n",
    "# xây dựng phương thức để kiểm tra số chiều của tensor \n",
    "def assert_rank(tensor, expected_rank, name=None):\n",
    "    \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
    "\n",
    "    Args:\n",
    "        tensor: A tf.Tensor to check the rank of.\n",
    "        expected_rank: Python integer or list of integers, expected rank.\n",
    "        name: Optional name of the tensor for the error message.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the expected shape doesn't match the actual shape.\n",
    "    \"\"\"\n",
    "    # kiểm tra name có tồn tại\n",
    "    if name is None:\n",
    "        # gán name tensor cho name\n",
    "        name = tensor.name\n",
    "\n",
    "    # tạo ra một từ điển để lưu trữ các giá trị Bool \n",
    "    expected_rank_dict = {}\n",
    "    if isinstance(expected_rank, six.integer_types):\n",
    "        expected_rank_dict[expected_rank] = True\n",
    "    else:\n",
    "        for x in expected_rank:\n",
    "            expected_rank_dict[x] = True\n",
    "    # lấy ra số chiều \n",
    "    actual_rank = tensor.shape.ndims\n",
    "    # kiểm tra xem số chiều này có nằm trong từ điển không \n",
    "    # nếu không đưa ra cảnh báo \n",
    "    if actual_rank not in expected_rank_dict:\n",
    "        scope_name = tf.get_variable_scope().name\n",
    "        raise ValueError(\n",
    "            \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
    "            \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
    "            (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.28946578, 0.31157443, 0.27321728, 0.40430227, 0.31906998],\n",
       "        [0.13888947, 0.14949749, 0.13109322, 0.19398952, 0.15309395],\n",
       "        [0.13774609, 0.14826678, 0.13001402, 0.19239254, 0.15183363],\n",
       "        [0.26689513, 0.28727989, 0.25191358, 0.37277742, 0.29419099],\n",
       "        [0.32523142, 0.35007176, 0.30697529, 0.4542568 , 0.35849344]]),\n",
       " array([[0.48853087, 0.5258436 , 0.4611083 , 0.6823402 , 0.53849382]]),\n",
       " array([[0.59252302],\n",
       "        [0.28430029],\n",
       "        [0.28195984],\n",
       "        [0.54632194],\n",
       "        [0.66573361]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "a = np.random.rand(1,5)\n",
    "b = np.random.rand(5,1)\n",
    "a *b  , a , b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35007175812339597"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.66573361 *  0.5258436 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
