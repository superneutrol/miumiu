{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import Bert_model\n",
    "import Optimization\n",
    "import Tokenization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "## Required parameters\n",
    "flags.DEFINE_string(\n",
    "    \"data_dir\", None,\n",
    "    \"The input data dir. Should contain the .tsv files (or other data files) \"\n",
    "    \"for the task.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"task_name\", None, \"The name of the task to train.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 128,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_predict\", False,\n",
    "    \"Whether to run the model in inference mode on the test set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simplr sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid , text_a, text_b=None, label=None):\n",
    "        \"\"\"\"Constructs a InputExample.\n",
    "        \n",
    "        Args: \n",
    "            guid: Unique id for the example. \n",
    "            text_a : String. The untokenized text of the first sequence. For sigle sequence tasks, \n",
    "                only this sequence must be specified . \n",
    "            text_b : (Optional) string.The untokenized text of the second sequence . Only must be specified \n",
    "                for sequence pair tasks. \n",
    "            label: (Optional) string. The label of the example. This should be specified for train and dev examples, \n",
    "                but not for test examples. \n",
    "        \"\"\"\n",
    "        self.guid = guid \n",
    "        self.text_a = text_a \n",
    "        self.text_b = text_b \n",
    "        self.label = label \n",
    "\n",
    "class PaddingInputExample(object):\n",
    "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids , input_mask, segment_ids, label_id,\n",
    "                is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_example(self, data_dir):\n",
    "        \"\"\"Gets a collection of \"InputExample\" for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @classmethod \n",
    "    def _read_tsv(cls , input_file , qoutechar=None):\n",
    "        \"\"\"Reads a tab sepqrated value file.\"\"\"\n",
    "        with tf.gfile.Open(input_file, \"r\") as f :\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", qoutechar=qoutechar) \n",
    "            lines = []\n",
    "            for line in reader: \n",
    "                lines.append(line)\n",
    "            return  lines \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the XNLI data set.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.language = \"zh\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        lines = self._read_tsv(\n",
    "            os.path.join(data_dir, \"multinli\",\n",
    "                        \"multinli.train.%s.tsv\" % self.language))\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"train-%d\" % (i)\n",
    "            text_a = Tokenization.convert_to_unicode(line[0])\n",
    "            text_b = Tokenization.convert_to_unicode(line[1])\n",
    "            label = Tokenization.convert_to_unicode(line[2])\n",
    "            if label == Tokenization.convert_to_unicode(\"contradictory\"):\n",
    "                label = Tokenization.convert_to_unicode(\"contradiction\")\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        lines = self._read_tsv(os.path.join(data_dir, \"xnli.dev.tsv\"))\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"dev-%d\" % (i)\n",
    "            language = Tokenization.convert_to_unicode(line[0])\n",
    "            if language != Tokenization.convert_to_unicode(self.language):\n",
    "                continue\n",
    "            text_a = Tokenization.convert_to_unicode(line[6])\n",
    "            text_b = Tokenization.convert_to_unicode(line[7])\n",
    "            label = Tokenization.convert_to_unicode(line[1])\n",
    "            examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "\n",
    "class MnliProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
    "        \"dev_matched\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, Tokenization.convert_to_unicode(line[0]))\n",
    "        text_a = Tokenization.convert_to_unicode(line[8])\n",
    "        text_b = Tokenization.convert_to_unicode(line[9])\n",
    "        if set_type == \"test\":\n",
    "            label = \"contradiction\"\n",
    "        else:\n",
    "            label = Tokenization.convert_to_unicode(line[-1])\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "\n",
    "# \n",
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "    \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"see base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")) , \"train\"\n",
    "        )\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"see base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")) , \"dev\"\n",
    "        )\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")) , \"test\"\n",
    "        )\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\" , \"1\"]\n",
    "    \n",
    "    def _create_examples(self, lines , set_type):\n",
    "        \"\"\"Create examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for(i , line) in enumerate(lines):\n",
    "            if i == 0: \n",
    "                continue \n",
    "            guid = \"%s-%s\" % (set_type , i)\n",
    "            text_a = Tokenization.convert_to_unicode(line[3])\n",
    "            text_b = Tokenization.convert_to_unicode(line[4])\n",
    "            if set_type == \"test\":\n",
    "                label = \"0\"\n",
    "            else : \n",
    "                label = Tokenization.convert_to_unicode(line[0])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid , text_a=text_a , text_b =text_b , label=label)\n",
    "            )\n",
    "        return examples \n",
    "    \n",
    "\n",
    "    \n",
    "class ColaProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")) , \"train\"\n",
    "        )\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "        # Only the test set has a header\n",
    "            if set_type == \"test\" and i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            if set_type == \"test\":\n",
    "                text_a = Tokenization.convert_to_unicode(line[1])\n",
    "                label = \"0\"\n",
    "            else:\n",
    "                text_a = Tokenization.convert_to_unicode(line[3])\n",
    "                label = Tokenization.convert_to_unicode(line[1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index , example , label_list, max_seq_length, \n",
    "                    tokenizer):\n",
    "    \"\"\"Converts a single 'InputExample' into a single 'InputFeatures'.\n",
    "        Chuyển đổi một InputExample thành một InputFeatures.\n",
    "    \"\"\"\n",
    "    if isinstance(example , PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "            input_ids = [0] * max_seq_length, \n",
    "            input_mask = [0] * max_seq_length, \n",
    "            segment_ids= [0] * max_seq_length\n",
    "            label_id = 0, \n",
    "            is_real_example=False\n",
    "        )\n",
    "    \n",
    "    label_map = {}\n",
    "    for (i , label) in enumerate(label_list):\n",
    "        label_map[label] = i \n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = None \n",
    "    if example.text_b: \n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "    \n",
    "    if tokens_b:\n",
    "        # sửa đổi tokens_a và tokens_b tại chỗ sao cho tổng độ dài nhỏ hơn độ dài được chỉ định\n",
    "        _truncate_seq_pair(tokens_a , tokens_b, max_seq_length - 3)\n",
    "    \n",
    "    else: \n",
    "        # Account for [CLS] and [SEP] with \"-2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length -2)]\n",
    "    \n",
    "     # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens .only real tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length \n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    label_id = label_map[example.label]\n",
    "    if ex_index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "            [Tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "    \n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids, \n",
    "        input_mask=input_mask, \n",
    "        segment_ids=segment_ids, \n",
    "        label_id=label_id , is_real_example=True\n",
    "    )\n",
    "\n",
    "    return feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_convert_examples_to_features(\n",
    "        examples , labels_list , max_seq_length , tokenizer , output_file\n",
    "    ):\n",
    "    \"\"\"Convert a set of 'InputExample' to a TFRecord file.\"\"\"\n",
    "    # định dạng một bản ghi TFRecord là một định dạng đơn giản để lưu trữ một \n",
    "    # chuỗi các bản ghi nhị phân \n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (ex_index , example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d \" % (ex_index , len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index , example , labels_list, max_seq_length, tokenizer)\n",
    "    \n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list =tf.train.Int64List(value=list(values)))\n",
    "            return f \n",
    "        \n",
    "        features =collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
    "        features[\"is_real_example\"] = create_int_feature(\n",
    "            [int(feature.is_real_example)])\n",
    "\n",
    "        # tạo một đối tượng tf.train.Example với tham số features là một đối tượng tf.train.Feature\n",
    "        # có tham số features là một đối tượng tf.train.Features  có tham số features là một từ điển\n",
    "        # chứa các cặp \"string\": tf.train.feature \n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        # ghi mội đối tượng tf.train.Example vào tệp tf.record bằng phương thức write phương thức này nhận \n",
    "        # vào một chuỗi nhị phân \n",
    "        # sử dụng serializeToString để chuyển đổi tf_example thành 1 chuỗi nhị phân \n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def file_based_input_fn_builder(input_file , seq_length , is_training, \n",
    "                    drop_remainder):\n",
    "    \"\"\"Creates an 'input_fn' closure to be passed to TPUEstimator.\"\"\"\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "\n",
    "    def _decode_record(record , name_to_features):\n",
    "        \"\"\"Decodes a record to a Tensorflow example \"\"\"\n",
    "\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "        # tf.example only supports tf.int64 , but the TPU only support tf.int32\n",
    "        #so cast all int64 to int32\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t \n",
    "\n",
    "        return example \n",
    "    \n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        # For training, we want a lot of parallel reading and shuffling.\n",
    "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "        d = tf.data.TFRecordDataset(input_file)\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                lambda record: _decode_record(record, name_to_features),\n",
    "                batch_size=batch_size,\n",
    "                drop_remainder=drop_remainder))\n",
    "\n",
    "        return d\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a , tokens_b , max_length):\n",
    "    \"\"\"\"Truncates q sequence pair in palce to the maximum length.\"\"\"\n",
    "    \n",
    "    while True: \n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break \n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def create_model(bert_config , is_training, input_ids , input_mask ,segment_ids, \n",
    "                labels , num_labels , use_one_hot_embeddings):\n",
    "    \"\"\"Create a classification model .\"\"\"\n",
    "    model = Bert_model.BertModel(\n",
    "        config = bert_config, is_training=is_training,\n",
    "        input_ids = input_ids , input_mask = input_mask , \n",
    "        token_type_ids = segment_ids , \n",
    "        use_one_hot_embeddings = use_one_hot_embeddings , \n",
    "    )\n",
    "\n",
    "    # In the demo , we are doing a simplr classification task on the entire segment . \n",
    "    # if you want to use the token-level output , use model.get_sequence_output() instead\n",
    "\n",
    "    output_layer = model.get_pooled_output()\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\" , [num_labels , hidden_size],\n",
    "        initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "    )\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\" , [num_labels] , initializer=tf.zeros_initializer()\n",
    "    )\n",
    "\n",
    "    with tf.variable_creator_scope(\"loss\"):\n",
    "        if is_training: \n",
    "            # I.e 0.1 dropout \n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer , output_weights , transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits , output_bias)\n",
    "    probabilities = tf.nn.softmax(logits , axis=-1)\n",
    "    log_probs = tf.nn.log_softmax(logits , axis=-1)\n",
    "\n",
    "    one_hot_labels = tf.one_hot(labels , dpth=num_labels , dtype=tf.float32)\n",
    "\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs , axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "    return (loss , per_example_loss, logits , probabilities)\n",
    "\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config , num_labels , init_checkpoint, learning_rate,\n",
    "                    num_train_steps , num_warmup_steps , use_tpu, use_one_hot_embeddings):\n",
    "    \"\"\"Returns 'model_fn' closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features , labels , mode , params):\n",
    "        \"\"\"The 'model_fn' for TPUEstimator.\"\"\"\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "        input_ids = features['input_ids']\n",
    "        input_mask = features['input_mask']\n",
    "        segment_ids = features['segment_ids']\n",
    "        label_ids = features['label_ids']\n",
    "        is_real_example =None \n",
    "        if \"is_real_example\" in features:\n",
    "            is_real_example = tf.cast(features[\"is_real_example\"] , dtype=tf.float32)\n",
    "        \n",
    "        else: \n",
    "            is_real_example = tf.ones(tf.shape(label_ids) , dtype=tf.float32)\n",
    "\n",
    "        is_training = (mode ==tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        (total_loss , per_example_loss , logits , probabilities) = create_model(\n",
    "            bert_config , is_training ,input_ids , segment_ids , label_ids , num_labels , \n",
    "            use_one_hot_embeddings\n",
    "        )\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        initialized_variable_names = {}\n",
    "        scaffold_fn = None \n",
    "        if init_checkpoint: \n",
    "            (assignment_map , initialized_variable_names) = Bert_model.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            if use_tpu:\n",
    "                 \n",
    "                def tpu_scaffold():\n",
    "                    tf.train.init_from_checkpoint(init_checkpoint , assignment_map)\n",
    "                    return tf.train.Scaffold()\n",
    "                \n",
    "                scaffold_fn = tpu_scaffold \n",
    "            else: \n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "        \n",
    "        tf.logging.info(\"**** Trainable Variables ****\")\n",
    "        for var in tvars:\n",
    "            init_string = \"\"\n",
    "            if var.name in initialized_variable_names:\n",
    "                init_string = \", *INIT_FROM_CKPT*\"\n",
    "            tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                            init_string)\n",
    "\n",
    "        output_spec = None \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN: \n",
    "            \n",
    "            train_op = Optimization.create_optimizer(\n",
    "                total_loss , learning_rate , num_train_steps, num_warmup_steps , use_tpu\n",
    "            )\n",
    "\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode = mode , \n",
    "                loss = total_loss, \n",
    "                train_op = train_op, \n",
    "                scaffold_fn = scaffold_fn ,\n",
    "            )\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "            def metric_fn (per_example_loss , label_ids ,logits , is_real_example):\n",
    "                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                accuracy = tf.metrics.accuracy(\n",
    "                    labels=label_ids, predictions=predictions, weights=is_real_example\n",
    "                )\n",
    "                loss = tf.metrics.mean(values=per_example_loss , weights=is_real_example)\n",
    "\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy, \n",
    "                    \"eval_loss\": loss , \n",
    "                }\n",
    "            \n",
    "            eval_metrics = (metric_fn, [per_example_loss , label_ids , logits , is_real_example])\n",
    "\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode = mode, predictions ={\"probabilities\": probabilities}, \n",
    "                scaffold_fn=scaffold_fn\n",
    "            )\n",
    "        else: \n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode = mode ,\n",
    "                predictions = {\"probabilities\": probabilities}, \n",
    "                scaffold_fn = scaffold_fn,\n",
    "            )\n",
    "        return output_spec\n",
    "    return model_fn\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length , is_training , drop_remainder):\n",
    "    \"\"\"Create an 'Input_fn' closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    all_label_ids = []\n",
    "\n",
    "    for feature in features: \n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_segment_ids.append(feature.segment_ids)\n",
    "        all_label_ids.append(feature.label_id)\n",
    "\n",
    "    \n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        num_examples = len(features)\n",
    "\n",
    "        # this is for demo purpose and does Not scale to large data sets. We do not use Dataset\n",
    "        # We donot use Dataset.from_generator() because that uses tf.py_func which is\n",
    "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "\n",
    "        d = tf.data.Dataset.from_tensor_slices({\n",
    "            \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
    "        })\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "        return d\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, label_list , max_seq_length, tokenizer):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index, example, label_list,\n",
    "                                        max_seq_length, tokenizer)\n",
    "\n",
    "        features.append(feature)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  processors = {\n",
    "      \"cola\": ColaProcessor,\n",
    "      \"mnli\": MnliProcessor,\n",
    "      \"mrpc\": MrpcProcessor,\n",
    "      \"xnli\": XnliProcessor,\n",
    "  }\n",
    "\n",
    "  Tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\n",
    "                                                FLAGS.init_checkpoint)\n",
    "\n",
    "  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n",
    "    raise ValueError(\n",
    "        \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n",
    "\n",
    "  bert_config = Bert_model.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "  if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "  task_name = FLAGS.task_name.lower()\n",
    "\n",
    "  if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "  processor = processors[task_name]()\n",
    "\n",
    "  label_list = processor.get_labels()\n",
    "\n",
    "  tokenizer = Tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "  tpu_cluster_resolver = None\n",
    "  if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master=FLAGS.master,\n",
    "      model_dir=FLAGS.output_dir,\n",
    "      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "          num_shards=FLAGS.num_tpu_cores,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  train_examples = None\n",
    "  num_train_steps = None\n",
    "  num_warmup_steps = None\n",
    "  if FLAGS.do_train:\n",
    "    train_examples = processor.get_train_examples(FLAGS.data_dir)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      num_labels=len(label_list),\n",
    "      init_checkpoint=FLAGS.init_checkpoint,\n",
    "      learning_rate=FLAGS.learning_rate,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size=FLAGS.train_batch_size,\n",
    "      eval_batch_size=FLAGS.eval_batch_size,\n",
    "      predict_batch_size=FLAGS.predict_batch_size)\n",
    "\n",
    "  if FLAGS.do_train:\n",
    "    train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
    "    file_based_convert_examples_to_features(\n",
    "        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    train_input_fn = file_based_input_fn_builder(\n",
    "        input_file=train_file,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "  if FLAGS.do_eval:\n",
    "    eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n",
    "    num_actual_eval_examples = len(eval_examples)\n",
    "    if FLAGS.use_tpu:\n",
    "      # TPU requires a fixed batch size for all batches, therefore the number\n",
    "      # of examples must be a multiple of the batch size, or else examples\n",
    "      # will get dropped. So we pad with fake examples which are ignored\n",
    "      # later on. These do NOT count towards the metric (all tf.metrics\n",
    "      # support a per-instance weight, and these get a weight of 0.0).\n",
    "      while len(eval_examples) % FLAGS.eval_batch_size != 0:\n",
    "        eval_examples.append(PaddingInputExample())\n",
    "\n",
    "    eval_file = os.path.join(FLAGS.output_dir, \"eval.tf_record\")\n",
    "    file_based_convert_examples_to_features(\n",
    "        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n",
    "\n",
    "    tf.logging.info(\"***** Running evaluation *****\")\n",
    "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
    "                    len(eval_examples), num_actual_eval_examples,\n",
    "                    len(eval_examples) - num_actual_eval_examples)\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
    "\n",
    "    # This tells the estimator to run through the entire set.\n",
    "    eval_steps = None\n",
    "    # However, if running eval on the TPU, you will need to specify the\n",
    "    # number of steps.\n",
    "    if FLAGS.use_tpu:\n",
    "      assert len(eval_examples) % FLAGS.eval_batch_size == 0\n",
    "      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)\n",
    "\n",
    "    eval_drop_remainder = True if FLAGS.use_tpu else False\n",
    "    eval_input_fn = file_based_input_fn_builder(\n",
    "        input_file=eval_file,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=eval_drop_remainder)\n",
    "\n",
    "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "\n",
    "    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n",
    "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "      tf.logging.info(\"***** Eval results *****\")\n",
    "      for key in sorted(result.keys()):\n",
    "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "  if FLAGS.do_predict:\n",
    "    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n",
    "    num_actual_predict_examples = len(predict_examples)\n",
    "    if FLAGS.use_tpu:\n",
    "      # TPU requires a fixed batch size for all batches, therefore the number\n",
    "      # of examples must be a multiple of the batch size, or else examples\n",
    "      # will get dropped. So we pad with fake examples which are ignored\n",
    "      # later on.\n",
    "      while len(predict_examples) % FLAGS.predict_batch_size != 0:\n",
    "        predict_examples.append(PaddingInputExample())\n",
    "\n",
    "    predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
    "    file_based_convert_examples_to_features(predict_examples, label_list,\n",
    "                                            FLAGS.max_seq_length, tokenizer,\n",
    "                                            predict_file)\n",
    "\n",
    "    tf.logging.info(\"***** Running prediction*****\")\n",
    "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
    "                    len(predict_examples), num_actual_predict_examples,\n",
    "                    len(predict_examples) - num_actual_predict_examples)\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
    "\n",
    "    predict_drop_remainder = True if FLAGS.use_tpu else False\n",
    "    predict_input_fn = file_based_input_fn_builder(\n",
    "        input_file=predict_file,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=predict_drop_remainder)\n",
    "\n",
    "    result = estimator.predict(input_fn=predict_input_fn)\n",
    "\n",
    "    output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
    "    with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
    "      num_written_lines = 0\n",
    "      tf.logging.info(\"***** Predict results *****\")\n",
    "      for (i, prediction) in enumerate(result):\n",
    "        probabilities = prediction[\"probabilities\"]\n",
    "        if i >= num_actual_predict_examples:\n",
    "          break\n",
    "        output_line = \"\\t\".join(\n",
    "            str(class_probability)\n",
    "            for class_probability in probabilities) + \"\\n\"\n",
    "        writer.write(output_line)\n",
    "        num_written_lines += 1\n",
    "    assert num_written_lines == num_actual_predict_examples\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  flags.mark_flag_as_required(\"data_dir\")\n",
    "  flags.mark_flag_as_required(\"task_name\")\n",
    "  flags.mark_flag_as_required(\"vocab_file\")\n",
    "  flags.mark_flag_as_required(\"bert_config_file\")\n",
    "  flags.mark_flag_as_required(\"output_dir\")\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
