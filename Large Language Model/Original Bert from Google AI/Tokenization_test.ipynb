{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Tokenization.ipynb\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import \n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os \n",
    "import tempfile\n",
    "from import_ipynb import *\n",
    "\n",
    "import Tokenization \n",
    "import six \n",
    "import tensorflow as tf \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng lớp tokenization test \n",
    "class TokenizationTest(tf.test.TestCase):\n",
    "\n",
    "    # xaay duwngj phuwowng thuwcs thwur nghieemj ddaayf đủ cho tất cả các tokens . \n",
    "    def test_full_tokenizer(self):\n",
    "        # xây dựng một từ điển các tokens là danh sách các tokens cần đặc biệt \n",
    "        vocab_tokens = [\n",
    "              \"[UNK]\", \"[CLS]\", \"[SEP]\", \"want\", \"##want\", \"##ed\", \"wa\", \"un\", \"runn\",\n",
    "                \"##ing\", \",\"\n",
    "        ]\n",
    "        # sử dụng hàm tempfile để tạo 1 tệp tạm thời có tên và sử dụng cấu trúc with để đảm \n",
    "        # bảo các tệp được đóng lịa khi không cần thiết \n",
    "        # với delete = False nghĩa là tệp tạm thời không bị xóa đi khi đóng \n",
    "        # mà phải được xóa thủ công bằng os.unlink , vocan_write là một đối tượng têoj , có thể \n",
    "        # được sử dụng để ghi nội dung vào tệp tạm thời \n",
    "        with tempfile.NamedTemporaryFile(delete=False) as vocab_writer: \n",
    "            # kiểm tra phiên bản python hiện tại . \n",
    "            if six.PY2: \n",
    "                # neeus nos alf py2 vaf ghi nội dung của tập vocab_tokens vào tập tam thời \n",
    "                # vocab_wite mỗi phần tử được thêm vò một ký tự xuống dòng \n",
    "                vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n",
    "            else :  # Trường hợp với python 3 ta cũng làm tương tự \n",
    "                vocab_writer.write(\"\".join(\n",
    "            [x + \"\\n\" for x in vocab_tokens]).encode(\"utf-8\"))\n",
    "\n",
    "        # sau đó gán tên của tệp tạm thời cho biến vocab_file \n",
    "        vocab_file = vocab_writer.name\n",
    "    \n",
    "        # Truyền file nguồn vào tokenizer  \n",
    "        tokenizer = Tokenization.FullTokenizer(vocab_file)\n",
    "        # xóa tệp vocab_file \n",
    "        os.unlink(vocab_file)\n",
    "        \n",
    "        # Xử lý file nguồn với token hóa với các định dnagj tiêu chuẩn \n",
    "        # với u là chuỗi vaen bản viết theo dnagj unicode \n",
    "        # \\u00E9 là một ký tự đặc biệt, được mã hóa theo định dạng Unicode, tương ứng với ký tự d\n",
    "        # UNwanted và running, mà BERT sẽ tách thành các token nhỏ hơn như sau:\n",
    "        #  [\"un\", \"##want\", \"##ed\", \",\", \"runn\", \"##ing\"].\n",
    "        tokens = Tokenization.tokenize(u\"UNwant\\u00E9d,running\")\n",
    "        # sau đó sử dụng phương thức assertAllEqual của lớp tf.test.testcase để kiểm tra xem \n",
    "        # kết quả của tokenize có đúng với dnah sách các tokens được mong đợi không. \n",
    "        self.assertAllEqual(tokens , [\"un\", \"##want\", \"##ed\", \",\", \"runn\", \"##ing\"])\n",
    "        # chuyển đổi các tokens tương ứng với định dạng thành các id tươnh ứng trong tập \n",
    "        # từ vựng và kiểm tra xem kết quả có đúng với danh sách id mong muốn .\n",
    "        self.assertAllEqual(\n",
    "            tokenizer.convert_tokens_to_ids(tokens), [7, 4, 5, 10, 8, 9])\n",
    "\n",
    "\n",
    "    # xây dựng phương thức thử nghiệm các dạng ký tự tiếng trung \n",
    "    def test_chinese(self):\n",
    "        # Tạo  biến tokenizer với nhiệm vụ thực hiện xử lý các định dạng căn bản \n",
    "        tokenizer = Tokenization.BasicTokenizer()\n",
    "\n",
    "        # sử dụng phương thức assertAllEqual để so sánh kết u=quả của phương thức tokenize với \n",
    "        #  danh sách kỳ vọng Ví dụ với chuỗi u\"ah\\u535A\\u63A8zz\"  sẽ được token hóa thành \n",
    "        # [u\"ah\", u\"\\u535A\", u\"\\u63A8\", u\"zz\"] với u\"ah\\u535A\\u63A8zz\" là  2 ký tự tiếng hán\n",
    "        self.assertAllEqual(\n",
    "            tokenizer.tokenize(u\"ah\\u535A\\u63A8zz\"),\n",
    "                [u\"ah\", u\"\\u535A\", u\"\\u63A8\", u\"zz\"]\n",
    "        )\n",
    "\n",
    "    # xây dựng phương thức thử nghiệm token hóa chữ thường cơ bản \n",
    "    def test_basic_tokenizer_lower(self):\n",
    "        # xuwr lys văn bản với chữ thường các kỹ tự chuyển về thường trước khi token hóa \n",
    "        tokenizer = Tokenization.BassicTokenizer(do_lower_case=True)\n",
    "\n",
    "        # sử dụng phương thức assertAllEqual để so sánh kết quả của phương thức tokenize\n",
    "        # với một danh sách kỳ vọng : Ví dụ chuỗi u\" \\tHeLLo!how  \\n Are yoU?  \" sẽ được token \n",
    "        # hóa thành [\"hello\", \"!\", \"how\", \"are\", \"you\", \"?\"]\n",
    "        self.assertAllEqual(\n",
    "            tokenizer.tokenize(u\" \\tHeLLo!how  \\n Are yoU?  \"),\n",
    "                [\"hello\", \"!\", \"how\", \"are\", \"you\", \"?\"])\n",
    "        self.assertAllEqual(tokenizer.tokenize(u\"H\\u00E9llo\"), [\"hello\"])\n",
    "\n",
    "    # xây dựng phươnh thức thử nghiệm token hóa với định dạng nguyên bản \n",
    "    def test_basic_tokenizer_no_lower(self):\n",
    "        # thực hiện xây dụng tuowng tự như tokenizer lowe\n",
    "        tokenizer = Tokenization.BasicTokenizer(do_lower_case=False)\n",
    "\n",
    "        # Hàm này cũng sử dụng phương thức assertAllEqual để so sánh kết quả của phương thức\n",
    "        #  tokenize với một danh sách kỳ vọng. Ví dụ, chuỗi u\" \\tHeLLo!how  \\n Are yoU?  \n",
    "        # \" sẽ được token hóa thành [\"HeLLo\", \"!\", \"how\", \"Are\", \"yoU\", \"?\"],\n",
    "        #  trong đó các ký tự hoa đã được giữ nguyên và tất cả các khoảng trắng và ký tự xuống dòng đã được loại bỏ.\n",
    "        self.assertAllEqual(\n",
    "            tokenizer.tokenize(u\" \\tHeLLo!how  \\n Are yoU?  \"),\n",
    "                [\"HeLLo\", \"!\", \"how\", \"Are\", \"yoU\", \"?\"])\n",
    "        \n",
    "    # Xây dượng phương thức thử nghiệm mã hóa token vơí WordPiece \n",
    "    def test_wordpiece_tokenizer(self):\n",
    "        # tạo một bộ danh sách từ vựng \n",
    "        vocab_tokens = [\n",
    "            \"[UNK]\", \"[CLS]\", \"[SEP]\" , \"want\" ,\"##want\" ,\"##ed\" , \"wa\" , \n",
    "            \"un\" , \"runn\" , \"##ing\"\n",
    "        ]\n",
    "        # tạo một từ điển rỗng \n",
    "        vocab = {}\n",
    "        # duyệt qua một danh sách các chỉ só  và giá trị của danh sách vocab_tokens đã thiết lập \n",
    "        for (i, token) in enumerate (vocab_tokens):\n",
    "            # đặt các chỉn số i với giá trị tương ứng trong danh sách vocab tokens \n",
    "            # đưa tất cả vào từ điển vocab \n",
    "            vocab[token] = i\n",
    "            # sử dụng phương thức assertAllEqual để so sánh kết của phương thức convert_token_to_ids \n",
    "            # với danh sách kỳ vọng  [\"un\", \"##want\", \"##ed\", \"runn\", \"##ing\"]), [7, 4, 5, 8, 9] \n",
    "            self.assertAllEqual(\n",
    "                Tokenization.convert_tokens_to_ids(\n",
    "                    vocab ,  [\"un\", \"##want\", \"##ed\", \"runn\", \"##ing\"]), [7, 4, 5, 8, 9]\n",
    "                )\n",
    "    \n",
    "    # Xây dựng các phuuwong thức xử lý thử nghiệm với ký tự điều khiển , các định dạng ký tự\n",
    "    # đặc biệt cũng như với các khoảng trắng \n",
    "    def test_is_whitespace(self):\n",
    "        self.assertTrue(Tokenization._is_whitespace(u\" \"))\n",
    "        self.assertTrue(Tokenization._is_whitespace(u\"\\t\"))\n",
    "        self.assertTrue(Tokenization._is_whitespace(u\"\\r\"))\n",
    "        self.assertTrue(Tokenization._is_whitespace(u\"\\n\"))\n",
    "        self.assertTrue(Tokenization._is_whitespace(u\"\\u00A0\"))\n",
    "\n",
    "        self.assertFalse(Tokenization._is_whitespace(u\"A\"))\n",
    "        self.assertFalse(Tokenization._is_whitespace(u\"-\"))\n",
    "\n",
    "    def test_is_control(self):\n",
    "        self.assertTrue(Tokenization._is_control(u\"\\u0005\"))\n",
    "\n",
    "        self.assertFalse(Tokenization._is_control(u\"A\"))\n",
    "        self.assertFalse(Tokenization._is_control(u\" \"))\n",
    "        self.assertFalse(Tokenization._is_control(u\"\\t\"))\n",
    "        self.assertFalse(Tokenization._is_control(u\"\\r\"))\n",
    "        self.assertFalse(Tokenization._is_control(u\"\\U0001F4A9\"))\n",
    "\n",
    "    def test_is_punctuation(self):\n",
    "        self.assertTrue(Tokenization._is_punctuation(u\"-\"))\n",
    "        self.assertTrue(Tokenization._is_punctuation(u\"$\"))\n",
    "        self.assertTrue(Tokenization._is_punctuation(u\"`\"))\n",
    "        self.assertTrue(Tokenization._is_punctuation(u\".\"))\n",
    "\n",
    "        self.assertFalse(Tokenization._is_punctuation(u\"A\"))\n",
    "        self.assertFalse(Tokenization._is_punctuation(u\" \"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.test.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
