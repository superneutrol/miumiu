{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division \n",
    "from __future__ import print_function\n",
    "\n",
    "from import_ipynb import *\n",
    "import Optimization \n",
    "\n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTest(tf.test.TestCase):\n",
    "    def test_adam(self):\n",
    "        # sử dụng phương thức test_session để tạo một phiên TensorFlow để chạy các đoạn mã3.\n",
    "        with self.test_session() as sess: \n",
    "            # Trong phiên, nó tạo một biến w có kích thước 3 và được khởi tạo bằng các giá trị [0.1, -0.2, -0.1].\n",
    "            # Biến này sẽ được tối ưu hóa bằng thuật toán Adam.\n",
    "            w = tf.get_varibale(\n",
    "                \"w\", shape=[3] , initializer = tf.constant_initializer([0.1 ,-0.2, -0.1])\n",
    "            )\n",
    "            #  tạo một hằng số x có giá trị [0.4, 0.2, -0.5]. \n",
    "            # Hằng số này sẽ được sử dụng làm đầu vào cho hàm mất mát\n",
    "            x = tf.constant([0.4 ,0.2, -0.5])\n",
    "            # Hàm mất mát được định nghĩa là trung bình bình phương của sự khác biệt giữa x và w.\n",
    "            # Hàm này sẽ được tối thiểu hóa bằng thuật toán Adam.\n",
    "            loss = tf.reduce_mean(tf.square(x-w))\n",
    "            # lấy danh sách các biến có thể huấn luyện (tvars)\n",
    "            # và tính toán gradient của hàm mất mát theo các biến này (grads).\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads = tf.gradients(loss , tvars)\n",
    "\n",
    "            #  tạo một biến toàn cục global_step để theo dõi số lần huấn luyện\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            # tạo một đối tượng optimizer thuộc lớp AdamWeightDecayOptimizer với tốc độ học là 0.2.\n",
    "            # Đây là một lớp con của lớp tf.train.AdamOptimizer với thêm tính năng suy giảm trọng số.\n",
    "            optimizer = Optimization.AdamWeightDecayOptimizer(learning_rate=0.2)\n",
    "            # tạo một đoạn mã huấn luyện (train_op) bằng cách gọi phương thức apply_gradients của \n",
    "            # optimizer với đầu vào là các cặp gradient và biến, cùng với global_step.\n",
    "            # Phương thức này sẽ cập nhật các biến theo thuật toán Adam5\n",
    "            train_op = optimizer.apply_gradients(zip(grads , tvars), global_step)\n",
    "\n",
    "            # tạo một đoạn mã khởi tạo (init_op) bằng cách gom nhóm các đoạn mã khởi tạo các biến toàn cục và cục bộ.\n",
    "            init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "\n",
    "            # Nó chạy đoạn mã khởi tạo để khởi tạo các biến4.\n",
    "            sess.run(init_op)\n",
    "            # Nó lặp 100 lần, mỗi lần chạy đoạn mã huấn luyện để cập nhật các biến4.\n",
    "            for _ in range(100):\n",
    "                sess.run(train_op)\n",
    "            # Nó lấy giá trị của biến w sau khi huấn luyện (w_np)\n",
    "            w_np = sess.run(w)\n",
    "            # Nó sử dụng phương thức assertAllClose để kiểm tra xem giá trị của w_np có gần bằng với [0.4, 0.2, -0.5] hay không, \n",
    "            # với sai số tương đối là 1e-2 và sai số tuyệt đối là 1e-23\n",
    "            self.asserAllClose(w_np.flat , [0.4,0.2,-0.5], rtol=1e-2 , atol=1e-2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.test.main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
