{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import random \n",
    "from import_ipynb import *\n",
    "\n",
    "import Tokenization \n",
    "import tensorflow as tf \n",
    "\n",
    "# gán biến flag bằng một module trong tensorflow  \n",
    "# cho phép tạo và xử lý các tham số dòng lệnh \n",
    "flags = tf.flags\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa một tham số đầu vào dạng string = None và mô tả của nó.\n",
    "flags.DEFINE_string(\"input_file\", None,\n",
    "                    \"Input raw text file (or comma-separated list of files).\")\n",
    "# Định nghĩa một đầu ra tham số dạng string = None và mô tả. \n",
    "flags.DEFINE_string(\n",
    "    \"output_file\", None , \n",
    "    \"Output TF exemple file (or comma-separated list of files).\"\n",
    ")\n",
    "\n",
    "# định nghãi mộtu tham số dáng tring vocab_file  = None và mô tả của nó .\n",
    "flags.DEFINE_string(\"vocab_file\", None, \n",
    "                    \"The vocabulary file thar the Bert model was trained on.\")\n",
    "\n",
    "# Định nghĩa một tham số dạng bool là do_lower_case  = True và mô tả của nó\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True, \n",
    "    \"Whether to lower case the input text. Should be True for uncased\"\n",
    "    \"models and False for cased models.\"\n",
    ")\n",
    "# Định nghĩa một tham số do_whole_word_mask dạng boolen  gán = False và mô tả của nó . \n",
    "flags.DEFINE_bool(\n",
    "    \"do_whole_word_mask\", False, \n",
    "    \"Whether to use whole word masking rather than per-WordPiece masking.\"\n",
    ")\n",
    "\n",
    "# Tương tự định nghĩa một tham số dạng integer là max_seq_length = 128  \n",
    "flags.DEFINE_integer(\"max_seqg_length\", 128, \"Maximum sequence length.\")\n",
    "# Tương tự như trên với biến max_prediction_per_seq \n",
    "flags.DEFINE_integer(\"max_predictions_per_seq\", 20, \n",
    "                     \"Maximum number if masked LM predictions per sequence.\")\n",
    "\n",
    "flags.DEFINE_integer(\"dupe_factor\", 10, \n",
    "                     \"Probability of creating sequences wwhich are shorter tha the\"\n",
    "                     \"maximum length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng lớp trường hợp huấn luyện \n",
    "class TrainingInstance(object):\n",
    "    \"\"\"A single traing instance (sentence pair).\n",
    "        Một trường hợp huấn luyện duy nhất . \n",
    "    \"\"\"\n",
    "    # Thiết lập phương thức khởi tạo \n",
    "    def __init__(self, tokens, segment_ids , masked_lm_positions, masked_lm_labels, \n",
    "                is_random_next):\n",
    "        self.tokens = tokens \n",
    "        self.segment_ids = segment_ids \n",
    "        self.masked_lm_positions = masked_lm_positions \n",
    "        self.masked_lm_labels = masked_lm_labels \n",
    "        self.is_random_next = is_random_next\n",
    "\n",
    "    # Thiết lập phương thức def _str_ cho lớp trainingInstance \n",
    "    # lớp này được sử dụng để lưu trữ các thông tin liên quan đến một mẫu huấn luyện \n",
    "    # cho mô hình bert \n",
    "    def __str__(self):\n",
    "        #  Khởi tạo một biến s = 1 chuỗi rỗng \n",
    "        # để lưu trữ các thuộc tính của thể hiện huấn luyện \n",
    "        s = \"\"\n",
    "        # cộng biến s với 1 chuỗi giá trị tokens được mã hóa , cách nhau bởi dấu cách \n",
    "        # hàm tokenization.Printable_text chuyển đổi một token thành một chuỗi có thể đọc .\n",
    "        s += \"tokens: %s\\n\" % (\" \".join(\n",
    "            [Tokenization.printable_text(x) for x in self.tokens]\n",
    "        ))\n",
    "        # tuwowng tujw nhuw trên thêm vào một chuỗi s một dòng chứa các segment_ids , cách nhau\n",
    "        # bởi dấu cách, sử dụng str chuyển số nguyên thành  1 chuỗi vd :  nếu self.segment_ids là [0, 0, 0, 0, 0],\n",
    "        #  thì dòng này sẽ thêm vào s chuỗi \"segment_ids: 0 0 0 0 0\\n\".\n",
    "        s += \"segment_ids: %s \\n\" %(\" \".join([str(x) for x in self.segment_ids]))\n",
    "        # Thêm vào chuỗi s một dòng chứa các giá trị của thuộc tính is_trandom_next .\n",
    "        # strt được ngầm định chuyển một giá trị bool thành một chuỗi \n",
    "        s += \"is_random_next: %s \\n\" % self.is_random_next \n",
    "        # Thêm vào chuỗi s một dòng chứa các vị trí bị che đi , cách nhau bỏi dấu cách . \n",
    "        # ví dụ : Ví dụ: nếu self.masked_lm_positions là [3], \n",
    "        # thì dòng này sẽ thêm vào s chuỗi \"masked_lm_positions: 3\\n\"\n",
    "        s += \"masked_lm_positions: %s \\n\" % (\" \".join([str(x) for x in self.masked_lm_positions]))\n",
    "        # Thêm vào chuỗi s một dòng chứa các nhãn đúng cho các vị trí bị che đi, cách nhau bởi dấu cách\n",
    "        #  Ví dụ: nếu self.masked_lm_labels là [2024], \n",
    "        # thì dòng này sẽ thêm vào s chuỗi \"masked_lm_labels: a\\n\"\n",
    "        s += \"masked_lm_labels: %s \\n\" % (\" \".join([Tokenization.printable_text(x) for x in self.masked_lm_labels]))\n",
    "        # thêm vào chuỗi s một dòng trống để tách biệt các thể hiện huấn luyện khác nhau .\n",
    "        s += \"\\n\"\n",
    "        return s \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng phương thức create_int_feature \n",
    "# tạo ra các đặc trưng định dạng int \n",
    "def create_int_feature(values):\n",
    "    # nhận vào một danh sách các số nguyên và trả về một đối tượng tf.train.Feature \n",
    "    # là một lớp được sử dụng để lưu trữ một giá trd dơn hoặc nhiều giá trị cùng kiểu \n",
    "    # dữ liệu \n",
    "    feature = tf.train.Feature(int64_list = tf.train.Int64List(value=list(values))\n",
    "                               )\n",
    "    return feature \n",
    "\n",
    "\n",
    "# xây dựng phương thức create_infloat_feature \n",
    "# tạo ra các đặc trưng định dạng float\n",
    "def create_float_feature(values):\n",
    "  #  sử dụng float_list, là một lớp được sử dụng để lưu trữ một danh sách các số thực 32 bit.\n",
    "  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "  return feature\n",
    "\n",
    "# xây dụng phương thức write instance to example files\n",
    "# đê tạo ra các file mẫu tf từ lớp trường hợp huấn luyện  \n",
    "def write_instance_to_example_files(\n",
    "        instances , tokenizer , max_seq_length , max_predictions_per_seq , \n",
    "        output_files\n",
    "):\n",
    "    \"\"\"Create TF example files from 'TrainingInstance's.\"\"\"\n",
    "    # tạo một danh sách writers để lưu trữ các bản gi \n",
    "    writers = []\n",
    "    # duyêt nqua 1 danh sách các file đầu ra\n",
    "    for output_file in output_files:\n",
    "        # thêm các bản ghi vào danh sách viết \n",
    "        writers.append(tf.python_io.TFRecorWriter(output_file))\n",
    "\n",
    "     # khowir taij mootj biến số nguyên để theo dõi chỉ số của đối tượng TFRecordWriter \n",
    "     # hiện tại trong danh sách writes . \n",
    "    writer_index =0 \n",
    "    # tạo một biến số nguyên để theo dõi số lượng thể hiện huấn luyện đã được ghi vào các \n",
    "    # tệp đầu ra \n",
    "    total_written = 0\n",
    "\n",
    "    # duyệt nqnua một danh sách các thể hiện huấn luyện và lấy ra chỉ số và giá trị của mỗi\n",
    "    # thể hiện \n",
    "    for (inst_index , instance) in enumerate(instances):\n",
    "        # chuyển đổi 1 danh sách các tokens của thể hiện huấn luyện thành một danh sách các\n",
    "        # số nguyên biểu diễn mã hóa của chúng \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "        # tạo một danh sách có cùng độ dài với dnh sách input_ids và gán gái trị 1 cho \n",
    "        # tất cả các phần tử . Đây là một mătj nạ để chỉ ra các vị trí có chứa tokens hợp lệ \n",
    "        # trong đầu vào . \n",
    "        input_mask = [1] * len(input_mask)\n",
    "        # sao chép danh sách các segment_ids của thể hiện huấn luyện thành 1 danh sách mới . \n",
    "        # Đây là một dnah sách các số nguyên chỉ ra thuộc về đoạn nào trong một cặp văn bản đầu vào \n",
    "        segment_ids = list(instance.segment_ids)\n",
    "\n",
    "        # kiểm tra xem độ dìa của danh sa hs input_ids đó có nhỏ hơn hoặc bằng độ dài tối đa của \n",
    "        # một chuỗi đầu vào hay không. Nếu không dừng trương trình và báo lỗ \n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        # trong khi độ dài input_ids  < max_seq length \n",
    "        while len(input_ids) < max_seq_length: \n",
    "            # thêm các giá trị = 0 vào 3 danh sách \n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        # kiểm tra xem độ dài của input_ids , input_mask , segment_ids có \n",
    "        # bằng với max_seq_length không \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        # sao chép danh sách các vị trí bị tre đi của thể hiện huấn luyện thành một danh sách mới \n",
    "        # đây là danh sách các số nguyên chỉ ra các vị trí của tokens bị che để dự đoán bằng mô hình ngôn ngữ \n",
    "        masked_lm_positions = list(instance.masked_lm_possitions)\n",
    "        # chuyển đổi danh sách các nhãn đúng cho các vị trí bị che đi của thể hiện hiaasn luyện thành \n",
    "        # một danh sách các số ngyên biểu diễn mã hóa chúng \n",
    "        masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.mask_lm_labels)\n",
    "        # tạo một danh sách có cùng độ dài với masked_lm_ids và gán giá trị 1.0 cho tất cả các \n",
    "        # phần tử. Đây là một danh sách các số thực biểu diễn trọng số của các vị trị bị tre đi \n",
    "        masked_lm_weights =[1.0] * len(masked_lm_ids)\n",
    "\n",
    "        # lặp lại cho đến khi len(masked_lm_positions) < max_predictions_per_seq \n",
    "        # tức alf đến khi độ dài của danh sách masked_lm_positions bằng với số lượng dự đoán \n",
    "        # tối đa của một chuỗi đầu vào. \n",
    "        while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "            masked_lm_positions.append(0)\n",
    "            masked_lm_ids.append(0)\n",
    "            masked_lm_weights.append(0.0)\n",
    "\n",
    "        # gán cho biến next_sentence_label giá trị = 1 nếu thuộc tính is_random_next của thể hiện \n",
    "        # huấn luyện là True ,ngược lại là 0. đây là 1 giá trị nguyên chỉ ra xem liệu văn bản thứ 2\n",
    "        # có được chọn ngẫu nhiên hay không . \n",
    "        next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "        # khởi tạo một đối tượng OrderedDict để lưu trữ các đặc trưng của thể hiện huấn luyện . \n",
    "        # sử dụng để tạo ra một từ điển có thứ tự \n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "        features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "        features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "        features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "        features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "        # khởi tạo biến tf_example  bằng các đặc trưng của thể hiện huấn luyện được xử ký \n",
    "        # tạo một đối tuowngj tf.train.Examople là một lớp được sử dụng để lưu trữ một bản ghi\n",
    "        # TFRecord từ đối tượng features\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(features=features))\n",
    "\n",
    "        # ghi đối tượng tf_example vào tập đầu ra tương ứng với chỉ số wi=rite_index trong danh sách \n",
    "        # đối tượng tf_example được chuyển đổi thành một chuỗi nhị phân với SerializeToString()\n",
    "        writers[writer_index].write(tf_example.SerializeToString())\n",
    "        # caapj nhật giá trị của biến writer_index bằng cách tăng nó lên 1 và lấy phần dư chi độ dài \n",
    "        # của danh sách write nhằm mục đích phân bổ các thể hiện huấn luyện đều cho cácteepj đầu ra khác nhau \n",
    "        writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "        total_written += 1\n",
    "\n",
    "        # kiểm tra xem inst_index  có nhỏ hơn 20 tức chỉ số index theo gái trị trong danh sách \n",
    "        # các thể hiện \n",
    "        if inst_index < 20:\n",
    "            # ghi một thông báo vào nhật ký nhằm đánh dấu một thể hiện huấn luyện \n",
    "            tf.logging.infor(\"*** Example ***\")\n",
    "            # ghi một thông báo vào nhật ký chứa danh sách các tokens của thể hiện huấn luyện \n",
    "            # cách nhau bởi dấu cách . và chuyển dổi một tokens thành 1 chuỗi đọc được \n",
    "            tf.logging.info(\"tokens: %s\" % \" \".join([\n",
    "                Tokenization.printable_text(x) for x in instance.tokens]))\n",
    "            # duyệt qua các khóa của đối tượng features là một OrderedDict chứa các đặc trưng của thể hiện huấn luyện.\n",
    "            for feature_name in features.keys():\n",
    "                # láy ra giá trị đặc trưng ứng với khóa feature_name.\n",
    "                # OrderedDict chứa các đặc trưng của thể hiện huấn luyện.\n",
    "                feature = features[feature_name]\n",
    "                # tạo 1 danh sách rỗng để luuw trữ các giá trị \n",
    "                values = []\n",
    "                # kiểm tra xem các giá trị trong dah sách giá trị đặc trưng của thể hiện huấn luyện \n",
    "                # có thuộc kiểu int.64 \n",
    "                if feature.int64_list.value: \n",
    "                    # gán danh sách values = các giá trị dạng list_int trong feature \n",
    "                    alues = feature.int64_list.value\n",
    "                # trường hợp là kiểu float ta làm tương tự \n",
    "                elif feature.float_list.value:\n",
    "                    # và cũng gán dnah sách value\n",
    "                    values = feature.float_list.value\n",
    "                # ghi một thông báo vào nhật ký chứa một danh sachs các tên đặc trưng trong features và \n",
    "                # giá trị của nó được cách nhau bởi khoảng trắng. \n",
    "                tf.logging.info(\n",
    "                        \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "                \n",
    "        # duyệt qua danh sách các bản ghi \n",
    "        for writer in writers:\n",
    "            # đóng các bản ghi đó lại \n",
    "            writer.close()\n",
    "\n",
    "        # ghi một mã thông báo vào nhật ký cho biết tổng số bản ghi đã được đọc \n",
    "        tf.logging.info(\"Wrote %d total instances\", total_written)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng phương thức khởi tạo các trường hợp huấn luyện \n",
    "def create_training_instances(input_files , tokenizer, max_seq_length , \n",
    "        dupe_factor , short_seq_prob, masked_lm_prob , \n",
    "        max_predictions_per_seq, rng                      \n",
    "            ):\n",
    "    \"\"\"Create \"TrainingInstance\"s from raw text.\"\"\"\n",
    "    all_documents = [[]]\n",
    "    # Input file format: \n",
    "    # (1) One sentence per lien. These should ideally be actual sentences , not entire paragraphs \n",
    "    #   or arbitrary spans of text . (Because we use the sentences bounderies for the \" next sentences\n",
    "    #   prediction task\").\n",
    "\n",
    "        # Mỗi dòng một câu . Lý tưởng nhất đây phải là những câu thực tế . Không phải toàn bộ đoạn văn \n",
    "        # các đoạn văn bản tùy ý . (Bởi vì sử dụng ranh giới câu cho nhiệm vụ dự đoán câu tiếp theo)\n",
    "    # (2) Blank lines between documents. Document bounderies are needed so that the \" next sentences prediction\" task \n",
    "    # doesn't span between documents . \n",
    "\n",
    "        # Dòng Trống giữa các tài liệu , cần có danh giới tài liệu để có thể dự đoán câu tiếp theo \n",
    "        # không trải dài các dữ liệu \n",
    "\n",
    "    # duyệt qua dnah sách các file đầu vào \n",
    "    for input_file in input_files: \n",
    "        # mở các file đầu vào từ danh sachs các file đầu vào \n",
    "        with tf.gfile.GFile(input_file , \"r\") as reader:\n",
    "            # với điều kiện = True \n",
    "            while True : \n",
    "                # dọc các dòng văn bản có trong file dưới định dnagj unicode \n",
    "                line = Tokenization.convert_to_unicode(reader.readline())\n",
    "                # nếu không tồn tại bất kỳ giá trị nào trong line \n",
    "                if not line: \n",
    "                    # thoát khỏi vòng lặp \n",
    "                    break \n",
    "                # trường hợp còn lại gán line = line.strip để chuẩn hóa tachs các chuỗi rồng \n",
    "                line = line.strip()\n",
    "\n",
    "                # các dòng trống sẽ được sử dụng làm dấu phân cách tài liệu \n",
    "                # kiểm tra xem line có tồn tại \n",
    "                if not line :\n",
    "                    # thêm một khoẳng trắng vào danh sách chưa tất cả tài liệu \n",
    "                    all_documents.append([])\n",
    "                # Thực hiện ãm hóa tiêu chuẩn các dòng văn bản bằng Tokenizer.tokenize\n",
    "                tokens = tokenizer.tokenize(line)\n",
    "                # nếu có tồn tại tokens \n",
    "                if tokens: \n",
    "                    # thêm token vào vị trí của cùng của danh sách chưa tài liệu hiện tại \n",
    "                    all_documents[-1].append(tokens)\n",
    "\n",
    "        # xóa đi các tài liệu trống \n",
    "        # tạo một danh sách chưa tài liệu mới bằng cách lặp qua danh sách ban đầu và lưu các \n",
    "        # tài liệu không rỗng có trong danh sách vào danh sách mới\n",
    "        all_documents = [x for x in all_documents if x]\n",
    "        # xáo trộn các tài liệu trong danh sách \n",
    "        rng.shuffle(all_documents)\n",
    "\n",
    "        # lấy ra danh scahs ácc từ trong từ điển của tokenizer à lưu vào biến vocab_word \n",
    "        vocab_word = list(tokenizer.vocab.keys())\n",
    "        instances = []\n",
    "\n",
    "        # duyệt qua dupe_factor lần mỗi lần duyêth qua tất cả cac tài liệu trong all_documents \n",
    "        for _ in range(dupe_factor):\n",
    "            for document_text in range(len(all_documents)):\n",
    "                # sử dụng hàm extend để thêm các tài liệu vừa tạo từ một tài liệu vào danh sách \n",
    "                # instances \n",
    "                instances.extend(\n",
    "                    create_instances_from_document(\n",
    "                        all_documents , document_text, max_seq_length , short_seq_prob, \n",
    "                        masked_lm_prob, max_predictions_per_seq , vocab_word, rng\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Trộn các thể hiện của instances và trả về kết quả của đoạn mã \n",
    "        rng.shuffle(instances)\n",
    "        return instances \n",
    "    \n",
    "\n",
    "# Xây dựng phương thức khởi tại các trường hợp từ tài liệu \n",
    "def create_instances_from_document(\n",
    "        all_documents , document_index, max_seq_length , short_seq_prob, \n",
    "        masked_lm_prob , max_predictions_per_seq , vocab_words , rng\n",
    "):\n",
    "    # Tạo ra các trường hợp huấn luyện cho một tài liệu đơn độc . \n",
    "    # tạo một tài liệu document = cách ánh xạ các chỉ số index_document cho danh sách \n",
    "    # chứa tất cả tìa liệu \n",
    "    document = all_documents[document_index]\n",
    "\n",
    "    # tạo một biến số lượng tokens trong 1 câu hay một chuỗi đầu vào \n",
    "    # bằng cách trừ đi 3 chỉ số là các tokens đặc biệt \n",
    "    max_num_tokens = max_seq_length - 3\n",
    "\n",
    "    # Chúng ta thường muốn phủ lên toàn bộ chuỗi vì dù sao chúng at cũng đệm vào max_seq_length \n",
    "    # vì vậy các chuỗi ngắn thường  lãng phí tính toán \n",
    "    # khởi tạo một biến mục tiêu là độ dài mục tiêu cho số lượng tokens của 1 đoạn đầu vào \n",
    "    target_seq_length = max_num_tokens\n",
    "\n",
    "    # sử dụng hàm random.random để tạo ra 1 giá trị [0->1] và so sánh nó với \n",
    "    # xác suất sinh ra chuỗi ngắn và kiểm tra xem giá trị ngẫu nhiên cod nhỏ hơn \n",
    "    # xác suất sinh ra chuỗi ngắn không \n",
    "    if rng.random() < short_seq_prob: \n",
    "        # gán biến độ dài cho đoạn văn bản mục tiêu bằng giá trị ngẫu nhiên \n",
    "        # từ 2 đến số tokens tối đa của một chuỗi văn bản \n",
    "        target_seq_length = rng.randint(2 , max_num_tokens)\n",
    "\n",
    "    # tạo da nh sách instances để lưu trữ các thể hiện \n",
    "    instances = []\n",
    "    # một danh sách current_chunk để lưu trữ các đoạn văn bản hiện tại \n",
    "    # mỗi đoạn văn bản là dnah scahs các câu đã được mã hóa thành dnah sách các \n",
    "    # mã thông báo \n",
    "    current_chunk = []\n",
    "    # một danh scahs current để lưu trữ độ dài hiện tại của đoạn văn bản đang xử lý \n",
    "    # từ currrent chunk \n",
    "    current_length = []\n",
    "    # và biến i đếm số đoạn văn bản trong current_chunk \n",
    "    i = 0 \n",
    "    # kiểm tra xem số lượng đoạn hiện tại có nhỏ hơn số đoạn trong tài liệu \n",
    "    while i < len(document):\n",
    "        # gán biến segment đoạn văn bản hiện tại bằng đoạn văn bản thứ i trong document \n",
    "        segment = document[i]\n",
    "        # và sau đó thêm đoạn văn bản hiện tại vào danh sách lưu trữ current_chunk \n",
    "        current_chunk.append(segment)\n",
    "        # cộng vào current_length một tham số là độ dài của đoạn hiện tại\n",
    "        current_length += len(segment)\n",
    "        # kiểm tra xem số đoạn hiện tại trong current_chunk có bằng với số đoạn trong tài liệu chưa \n",
    "        # hay độ dài của các đoạn hiện tại , current_length >=  độ dài của đoạn văn bản mục tiêu \n",
    "        if i == len(document)  or current_length >= target_seq_length:\n",
    "            # và kiểm tra xem current_chunk có tồn tại không \n",
    "            if current_chunk: \n",
    "                # tạo một biến a_end là giá trị cho biết số lượng đoạn đã được sử dụng làm câu A để dự đoán câu b \n",
    "                # từ dnah sách chứa các đoạn hiện tại \n",
    "                a_end = 1 \n",
    "                # ssau đó kiểm tra xem số lượng đoạn hiện tại từ danh sách current chunk có \n",
    "                # lớn hơn = 2 không \n",
    "                if len(current_chunk) >= 2 : \n",
    "                    # đặt lại a_end = môth giá trị ngẫu nhiên từ 1 đến số lượng đoạn hiện tại - 1\n",
    "                    a_end = rng.randint(1 , len(current_chunk) - 1)\n",
    "\n",
    "                # tạo một danh sách tokens để lưu trữ tokens của câu a  \n",
    "                tokens_a = []\n",
    "                # lặp qua một danh sách từ 0-> a_end cho biết ố đoạn hiện tại trong current_chunk \n",
    "                for j in range(a_end):\n",
    "                    # thêm danh cách tokens chứa trong mỗi đoạn từ 0 đến a_end trong danh sách current_chunk \n",
    "                    # sử dụng hàm extend để nối các đoạn lại với nhau \n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                \n",
    "                # Tạo một dnah sách tokenS_b để lưu trữ các tokens của câu B \n",
    "                tokens_b = []\n",
    "                # kiểm tra xem current_chunk == 1 tức là chỉ chứa duy nhất 1 đoạn \n",
    "                # hoặc xác suất ngẫu nhiên < 0.5\n",
    "                # khởi tạo một biến là random next = False \n",
    "                is_random_next = False\n",
    "                if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                    # đặt biến is_random_next = True tức là B là một câu theo sau câu A \n",
    "                    # trong document Điều này để taọ nhãn cho Tác vụ NSP \n",
    "                    is_random_next = True \n",
    "                    # khởi tạo một giá trị là độ dài mục tiêu cho B bằng độ dài đoạn mục tiêu \n",
    "                    # trừ độ dài của dnah sách tokens chứa câu a \n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "                    for _ in range(10):\n",
    "                        # tạo một biến random_document_index = giá trị ngẫu nhiên từ 0  đến \n",
    "                        # số lượng các đoạn văn abnr trong bộ tài liệu document \n",
    "                        random_document_index = rng.randint(0 , len(all_documents) -1)\n",
    "                        # kiểm tra xem chỉ số số chỉ số tài liệu ngẫu nhiên random_document_index \n",
    "                        # có khác với chỉ số của bộ tài liệu hay không , nếu khác thì lập tức thoát \n",
    "                        # và thực hiện tiếp trương trình .tức là đoạn mã này sẽ chọn 1 bộ tài liệu bắt kỳ nếu trùng chọn lại \n",
    "                        # Mục đích của đoạn mã này để đảm bảo rằng tại liệu ngẫu nhiên không \n",
    "                        # Trùng với tài liệu hiện tại đoạn mã sẽ kết thúc khi đạt được kết quả là \n",
    "                        # một bộ chỉ số khác nhau tức là khi đạt được mục đích \n",
    "                        if random_document_index != document_index: \n",
    "                            break \n",
    "\n",
    "                    # đặt biến random_documnet bằng các giá trị theo chỉ số ngẫu nhiên \n",
    "                    # bằng cách ánh xạ chỉ số này vào danh sách chứa các đoạn văn bản từ bộ tài liệu \n",
    "                    random_document = all_documents[random_document_index]\n",
    "                    # Tạo một giá trị random_start chỉ số bắt đầy ngẫu nhiên  là một giá trị bất kỳ \n",
    "                    #  khoảng 0 -> len(random_document) - 1\n",
    "                    random_start = rng.random(0, len(random_document) -1)\n",
    "                    # duyệt qua một dnah sách các đoạn ngẫu nhiên từ random satrt đến random_document\n",
    "                    for j in range(random_start, len(random_document) -1):\n",
    "                        # thêm danh sách các đoạn ngẫu nhiên từ random_document - 1 \n",
    "                        # vào danh sách tokens_b sử dụng hàm extend để nối các câu này lại '\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        # kiểm tra xem độ dài của danh sách tokens_b > target_b_length \n",
    "                        if len(tokens_b) >= target_b_length: \n",
    "                            # lập tức thoát khỏi vòng vặp \n",
    "                            break \n",
    "                    # đếm số lượng các đoạn văn bản không được sử dụng trong danh sách hiện tại \n",
    "                    # Giảm giá trị của i đi num_unused_segments để quay lại các đoạn chưa được xử lý\n",
    "                    num_unsed_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unsed_segments\n",
    "\n",
    "                # trường hợp còn lain tức là số đoạn hiện tại củadnah sách current chunk != 1 \n",
    "                # và giá trị sinh ngẫu nhiên >0.5\n",
    "                else: \n",
    "                    # đặt is_random_next = False tức là B là câu ngẫu nhiên không phải câu theo sau A\n",
    "                    is_random_next = False \n",
    "                    # lặp qua danh ách từ chỉ số a_end đến hết số lượng đoạn hiện tại trong current_chunk \n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        # thêm nó vào dnah sách tokens_b \n",
    "                        # tức là thêm các đoạn trong danh sách current_chunk từ đoạn hiện tại đến hết danh \n",
    "                        # sách chứa các đoạn hiện tại \n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "                    \n",
    "                # Sau đó từ danh sách tokens_a , tokens_b ta đư vào phương thức truncate_seq_pair \n",
    "                # để cắt ngẵn các cặp câu trong trong danh sách tokens_a , tokens_b nếu tổng số \n",
    "                # tokens của cúng quá số tokens tối đa \n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "                # kiểm tra và đưa ra cảnh báo cho độ dài 2 danh sách tokens_a , tokens_b \n",
    "                assert len(tokens_a) >= 1 \n",
    "                assert len(tokens_b) >= 1\n",
    "\n",
    "                # tạo một dnah sách tokens để lưu trữ các tokens sau khi xử lý \n",
    "                tokens = []\n",
    "                # và một danh sách segment_ids để lưu trữ các nhãn cho các tokens\n",
    "                # 0 cho câu a và 1 cho câu b \n",
    "                segment_ids = []\n",
    "                # Thêm một tokens [CLS] vào danh sách tokens \n",
    "                tokens.append(\"[CLS]\")\n",
    "                # Tươnh tự thêm 1 token 0 vào dnah sách chứa nhãn \n",
    "                segment_ids.append(0)\n",
    "\n",
    "                # duyệt qua dnah scahs tokens a\n",
    "                for token in tokens_a:\n",
    "                    # thêm các tokens vào dnah sách tokens\n",
    "                    tokens.append(token)\n",
    "                    # thêm nhãn tương ứng \n",
    "                    segment_ids.append(0)\n",
    "                # thêm nhãn và tokens đặc biệt đánh dấu kết thúc câu \n",
    "                tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(0)\n",
    "                \n",
    "                # duyệt qua dnah sách tokens_b \n",
    "                for token in tokens_b:\n",
    "                    # thêm tokens vào dnah scahs tokens\n",
    "                    tokens.append(token)\n",
    "                    # thêm nhãn tương ứng vào danh sách \n",
    "                    segment_ids.append(1)\n",
    "                # thêm nhãn và tokens đặc biệt đánh dấu kết thúc câu \n",
    "                tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(1)\n",
    "\n",
    "                # THỰC HIỆN LỚP TIỀN XỬ LÝ mặt nạ tokens cho mô hình dự đoán \n",
    "                (tokens, masked_lm_positions,\n",
    "                    masked_lm_labels) = create_masked_lm_predictions(\n",
    "                    tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "                \n",
    "                # khởi tạo instance = TrainingIntance \n",
    "                # để lưu trữ các thông tin cần thiết cho một thể hiện huấn luyện của kô hình Bert\n",
    "                # motpj thể hiện huấn luyện là một cặp câu có độ dài ngẫu nhiên \n",
    "                instance = TrainingInstance(\n",
    "                    tokens=tokens,\n",
    "                    segment_ids=segment_ids,\n",
    "                    is_random_next=is_random_next,\n",
    "                    masked_lm_positions=masked_lm_positions,\n",
    "                    masked_lm_labels=masked_lm_labels)\n",
    "                # thêm instance vào dnah sách chứa các trường hợp huấn luyện \n",
    "                instances.append(instance)\n",
    "            # đặt lại curent chunk và current_length = rỗng \n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        # tăng i lên 1 \n",
    "        i += 1\n",
    "    # trả về dnah sách chứa các trường hợp được huấn luyện \n",
    "    return instances\n",
    "\n",
    "# Lớp MaskedLmInstance được sử dụng trong đoạn mã tiền xử lý dữ liệu cho mô hình BERT1,\n",
    "#  để lưu trữ các vị trí và nhãn của các token bị che mặt trong một chuỗi. \n",
    "# Các token bị che mặt là những token bị thay thế bằng token [MASK], token ngẫu nhiên,\n",
    "# hoặc giữ nguyên, với một số xác suất nhất định. Mục đích của việc che mặt các token là \n",
    "# để huấn luyện mô hình ngôn ngữ có khả năng hiểu được ngữ cảnh của các token trong một chuỗi1\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng lớp che mặt nạ cho mô hình dự đoán mặt nạ ngôn ngữ Bert \n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob, \n",
    "                max_predictions_per_seq , vocab_words , rng):\n",
    "    \"\"\"Create the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    # khởi tạo một danh scahs để lưu trữ các chỉ số của tokens ứng viên để có thể che \n",
    "    cand_indexes = []\n",
    "    # duyệt qua danh sách chưa tokens lấy ra chỉ só và giá trị các tokens từ danh sách \n",
    "    for (i , token) in enumerate(tokens):\n",
    "        # kiểm tra xem các tokes có phải là các giá trị đặc biệt không phải thì tiếp tục \n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue \n",
    "\n",
    "        # kiểm trã xem có nên sử dụng kỹ thuật tre toàn bộ các tokens hay không, \n",
    "        # nếu có thì kiểm tra xem tokens hiện tại có bắt đầu bằng ## hay không \n",
    "        # ## là 2 token phụ được mã hóa thêm vào 1 từ \n",
    "        if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        token.startswith(\"##\")):\n",
    "            # nếu có thêm lập tức chỉ số i vào danh sách chứa chỉ số ứng viên \n",
    "            cand_indexes[-1].append(i)\n",
    "        # trường hợp còn lại tạo cho i là một danh sách mới rồi thêm vào danh sách \n",
    "        # chứa chỉ số ứng viên\n",
    "        else:\n",
    "            cand_indexes.append([i])\n",
    "    # xáo trộn các chỉ số ứng viên một cacgs ngẫu nhiên\n",
    "    rng.shuffle(cand_indexes)\n",
    "    # tạo một danh sách output_tokens bằng cách sao chép lain danh sách tokens\n",
    "    output_tokens = list(tokens)\n",
    "    # tính số lượng tokens cần dự đoán được tính bằng cách lấy giá trị nhỏ nhất giữa max_predictions_per_seq,\n",
    "    # là một tham số để giới hạn số lượng token tối đa được che trong mỗi câu,\n",
    "    # và giá trị lớn nhất giữa 1 và số lượng token trong câu nhân với masked_lm_prob, \n",
    "    # là một tham số để chỉ xác suất che một token trong câu.\n",
    "    num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "    # tạo một danh sách masked_lm để lưu trữ các dối tượng của maskedLmInstance mỗi đối tượng \n",
    "    # này chứa thông tin về một tokens bị che đi , bao gồm chỉ số , tokens gốc , tokens thay thế\n",
    "    masked_lms =[]\n",
    "    # tạo một danh sách convered_indexes là một tập hợp các chỉ số của ácc tokens đẫ được \n",
    "    # che hoặc bỏ qua . để đảm bảo rằng không có tokens nào được tre nhiều hơn 1 lần \n",
    "    covered_indexes = set()\n",
    "    # duyệt qua danh sách các chỉ số ứng viên trong danh sách cand_indexes \n",
    "    for index_set in cand_indexes:\n",
    "        # kiểm tra xem đọ dài danh sách masked_lm đã lớp hơn số tượng tokens cần dự đoán chưa \n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            # nếu lớn hơn thoát khỏi vòng lặp \n",
    "            break \n",
    "\n",
    "        # trường hợp nếu như việc thêm mặt nạ toàn từ sẽ vượt quá số lượng dự đoán tối đa \n",
    "        # thì cần bỏ qua ứng viên này\n",
    "        # kiểm tra nếu tổng số phần tử của của masked_lms và chỉ số ứng viên hiện tại \n",
    "        # mà vượt quá số lượng cần dự đoán \n",
    "        if len(masked_lms) + len(index_set) > num_to_predict: \n",
    "            # tiếp tục lặp \n",
    "            continue \n",
    "\n",
    "        # khởi tạo một biến boolen gọi là is_any_index_convered = False\n",
    "        # biến này sẽ được sử dụng để kiểm tra xem có bất kỳ tokens nào trong tâp hợp indexes\n",
    "        # hay danh sách ứng viên đã được tre đi hoặc bỏ qua hay không . \n",
    "        is_any_index_convered = False \n",
    "        # duyệt qua dnah sách các phần tử trong danh sách các token ứng viên để được tre được \n",
    "        # trích xuất từ danh sách ứng viên ban đầu\n",
    "        for index in index_set: \n",
    "            # kiểm tra xem phần tử ứng viên trong danh sách này đã có trong tập convered_indexes \n",
    "            # thì gán = True và thoát khỏi vòng lặp . Điều này có nghĩa là tập hơn index_set đã bị trùng với \n",
    "            # mọt tập hợp khác và không nên được tre \n",
    "            is_any_index_convered = True \n",
    "            break \n",
    "        \n",
    "        # kiểm tra xem is_any có  = True không\n",
    "        if is_any_index_convered :\n",
    "            # nếu có ta tiếp tục lặp \n",
    "            continue \n",
    "\n",
    "        # duyệt qua danh sách ứng viên một lần nữa \n",
    "        for index in index_set: \n",
    "            # thêm chúng avof danh sách convered_indexes \n",
    "            # điều này có nghĩa là đánh dấu các tokens index này đã được che hoặc bỏ qua \n",
    "            covered_indexes.add(index)\n",
    "\n",
    "        # khởi tạo một biến gọi là masked_token = None biến này sẽ được sử dụng để lưu trữ \n",
    "        # token thay thế cho tokens bị che đi \n",
    "        masked_token = None \n",
    "        # kiểm tra xem giá trị ngẫu nhiên < 80 % không \n",
    "        # tức là ta thay thế 80% số trường hợp bị che đi = MASK \n",
    "        if rng.random() < 0.8:\n",
    "            masked_token = \"[MASK]\"\n",
    "        # trường hợp còn lại \n",
    "        else: \n",
    "            # kiểm tra xem tỷ lệ ngẫu nhiên < 0.5 \n",
    "            if rng.random() < 0.5: \n",
    "                # ta giữ nguyên 10 % số trường hợp ban đầu từ danh sách bị che đi \n",
    "                # bằng cách ánh xạ các chỉ số ứng viên vào danh sách tokens và gán cho \n",
    "                # danh sách lưu trữ tokens thay thế \n",
    "                masked_token = tokens[index]\n",
    "            # trưuowngf hợp còn lại ta chọn 10% trường hợpn\n",
    "            # và thay thế nó bởi 1 tokens ngẫu nhiên trong bộ từ điển \n",
    "            else:\n",
    "                masked_token = vocab_words[rng.ranint(0 , len(vocab_words) -1)]\n",
    "\n",
    "        # gán giá trị của tokens bị thay thế cho token bị che đi , vào vị trí tương ứng trong danh sách \n",
    "        # output_tokens . \n",
    "        # tokens bị thay thế có thể là cac loại token đặc biệt hoặc token ngẫu nhiên khác . \n",
    "        output_tokens[index] = masked_token\n",
    "\n",
    "        # thay thế một đối tượng maskedLanguageModelInstance vào danh sách masked_lms đối tượng này \n",
    "        # chưa chỉ só và tokens gốc của token bị tre đi \n",
    "        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "        # kiểm tra xem độ dài của dnah scahs masked_lm có nhỏ hơn số lượng tokens cần dự đoán \n",
    "        assert len(masked_lms) <= num_to_predict\n",
    "        # sắp xếp danh sachs token bị che đi theo thứ tự của chỉ số điều này giúp cho việc dự đoán các \n",
    "        # token bị che đi dễ dàng hơn \n",
    "        masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "        # tạo 2 danh scahs mới \n",
    "        # 1 danh sách sẽ chứa vị trí accs tokens bị che \n",
    "        masked_lm_positions = []\n",
    "        # và 1 danh scahs chứa nhãn cho các token bị tre\n",
    "        masked_lm_labels = []\n",
    "        # duyệt qua dnah sách chưa các token bị tre \n",
    "        for p in masked_lms: \n",
    "            # thêm vị trí và nhãn của nó vào danh sách tương ứng \n",
    "            masked_lm_positions.append(p.index)\n",
    "            masked_lm_labels.append(p.label)\n",
    "\n",
    "    # trả về 3 giá trị gòm danh scahs token được tre , vị trí của các token đuwocj tre , \n",
    "    # nhãn của ácc tokens được tre \n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
    "\n",
    "\n",
    "# Xây dựng phương thức cắt ngắn các cặp câu trong danh sách tokens a và b \n",
    "def truncate_seq_pair(tokens_a , tokens_b , max_num_tokens, rng):\n",
    "    \"\"\"cắt ngắn một cặp chuỗi đến một độ dài tối đa\"\"\"\n",
    "    # trong khi điều kiện đúng \n",
    "    while True: \n",
    "        # tính tổng độ dài chuỗi từ 2 chuỗi câu a , b \n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        # kiểm tra xem tỏng độ dài có nhỏ hơn số lượng tokens tối đa \n",
    "        if total_length <= max_num_tokens:\n",
    "            # có thì thoát không cần cắt \n",
    "            break\n",
    "\n",
    "        # trường hợp khác ta chọn đoạn alf tokens_a hoặc b\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        # kiểm tra xem điều kiện và trả về cảnh báo \n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        # tạo ra một giá trị ngẫu nhiên [0 ->1]\n",
    "        # để ta co thể chọn việc cắt bớt trước hay sau chuỗi văn bản \n",
    "        # để tạo tính ngẫu nhiên \n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # đặt mức độ ghi nhật ký alf ìnffor để hiển thị thông tị quan trọng trongq úa trình chạy chương trình \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    # tạo một đối tượng tokenizer với đươngf dâbx đếb file \n",
    "    tokenizer = Tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "    # tạo một dnah scahs để lưu trữ file đầu vào là một chuỗi chứa một hoặc nhiều đường dẫn  được \n",
    "    # phân tahcs bởi 1 dấu phẩy \n",
    "    input_files = []\n",
    "    for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "        input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "\n",
    "    # Ghi nhật ký thông tin “*** Reading from input files ***” \n",
    "    # để báo hiệu bắt đầu đọc các tệp đầu vào. Vòng lặp for sẽ duyệt qua các \n",
    "    # phần tử trong danh sách input_files và ghi nhật ký thông tin về đường dẫn của mỗi tệp\n",
    "    tf.logging.info(\"*** Reading from input files ***\")\n",
    "    for input_file in input_files:\n",
    "        tf.logging.info(\"  %s\", input_file)\n",
    "\n",
    "\n",
    "    # tạo một đối tượng rng là một đối tượng random.random sử dụng để tạo ra một xác suấ ngẫu nhiên\n",
    "    rng = random.Random(FLAGS.random_seed)\n",
    "    # gọi hàm create_training_instances là một lớp để lưu trữ các thông tin về một mẫu huấn luyện\n",
    "    instances = create_training_instances(\n",
    "        input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "         FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "      rng)\n",
    "    # Tạo một danh sách gọi là output_files, là một danh sách các đường dẫn đến các tệp đầu ra\n",
    "    #  để lưu trữ dữ liệu huấn luyện. Tham số output_file là một chuỗi chứa một hoặc nhiều đường dẫn, \n",
    "    # được phân tách bằng dấu phẩy. Hàm split sẽ tách chuỗi này thành một danh sách các đường dẫn.\n",
    "    output_files = FLAGS.output_file.split(\",\")\n",
    "    tf.logging.info(\"*** Writing to output files ***\")\n",
    "    #  Vòng lặp for sẽ duyệt qua các phần tử trong danh sách output_files và ghi nhật ký thông tin về đường dẫn của mỗi tệp\n",
    "    for output_file in output_files:\n",
    "        tf.logging.info(\"  %s\", output_file)\n",
    "\n",
    "    #ghi cc\n",
    "    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                  FLAGS.max_predictions_per_seq, output_files)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  flags.mark_flag_as_required(\"input_file\")\n",
    "  flags.mark_flag_as_required(\"output_file\")\n",
    "  flags.mark_flag_as_required(\"vocab_file\")\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
