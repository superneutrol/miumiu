{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os \n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "from import_ipynb import * \n",
    "import Tokenization  \n",
    "import Optimization \n",
    "import Run_classifier \n",
    "\n",
    "flags = tf.flags \n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string(\n",
    "    \"bert_hub_module_handle\", None, \n",
    "    \"Handle for the BERT TF-Hub module.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_training, input_ids , input_mask , segment_ids , labels,\n",
    "                num_labels , bert_hub_module_handel):\n",
    "    \n",
    "    \"\"\"Create a classification model.\"\"\"\n",
    "    tags = set()\n",
    "    if is_training:\n",
    "        tags.add(\"train\")\n",
    "    bert_module = hub.Module(bert_hub_module_handel, tags=tags, trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids, \n",
    "        input_mask = input_mask , segment_ids = segment_ids, \n",
    "    )\n",
    "\n",
    "    bert_outputs = bert_module(\n",
    "        inputs = bert_inputs , signature=\"tokens\",\n",
    "        as_dict=True\n",
    "    )\n",
    "\n",
    "    # In the demo , we are doing a simple classification task on the entire segment \n",
    "    # If you want to use the tokn-level output , use bert_outputs[\"sequence_output\"] instead. \n",
    "\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\" ,[num_labels , hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02)\n",
    "    )\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\" , [num_labels], initializer=tf.zeros_initializer()\n",
    "    )\n",
    "\n",
    "    with tf.variable_creator_scope(\"loss\"):\n",
    "        if is_training:\n",
    "            #I.e , 0.1 dropout \n",
    "            output_layer = tf.nn.dropout(\n",
    "                output_layer , keep_prob=0.9\n",
    "            )\n",
    "        \n",
    "        logits = tf.matmul(output_layer, output_weights , transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits , output_bias)\n",
    "        probabilities = tf.nn.softmax(logits , axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits,axis=-1)\n",
    "\n",
    "        one_hot_labels = tf.one_hot(labels , depth=num_labels , dtype=tf.float32)\n",
    "\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs , axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        return (loss , per_example_loss , logits , probabilities)\n",
    "    \n",
    "\n",
    "def model_fn_builder(num_labels , learning_rate , num_train_steps, \n",
    "                num_warmup_steps , use_tpu , bert_hub_module_handle):\n",
    "    \"\"\" Returns 'model_fn' closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features , labels , mode , params):\n",
    "        \"\"\"The 'model_fn' for TPUEstimator.\"\"\"\n",
    "\n",
    "        tf.logging.info(\"** Features **\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\" name =%s , shape = %s\" % (name , features[name].shape))\n",
    "        \n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids =features[\"label_ids\"]\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        (total_loss , per_example_loss , logits , probabilities) = create_model(\n",
    "            is_training , input_ids , input_mask , segment_ids , label_ids , num_labels , bert_hub_module_handle\n",
    "        )\n",
    "        output_spec = None \n",
    "        if mode  == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = Optimization.create_optimizer(\n",
    "                total_loss , learning_rate , num_train_steps , num_warmup_steps , use_tpu\n",
    "            )\n",
    "\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode = mode , \n",
    "                loss = total_loss , \n",
    "                train_op = train_op\n",
    "            )\n",
    "        elif mode == tf.estimator.ModeKeys.TRAIN: \n",
    "            \n",
    "            def metric_fn(per_example_loss , label_ids, logits):\n",
    "                predictions = tf.argmax(logits , axis=-1 , output_type=tf.int32)\n",
    "                accuracy = tf.metrics.accuracy(label_ids , predictions)\n",
    "                loss = tf.metrics.mean(per_example_loss)\n",
    "\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy, \n",
    "                    \"eval_loss\": loss,\n",
    "                }\n",
    "            \n",
    "            eval_metrics = (metric_fn , [per_example_loss,label_ids , logits])\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metrics=eval_metrics)\n",
    "        \n",
    "        elif mode == tf.estimator.ModeKeys.PREDICT: \n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode =mode , predictions ={\"probabilities\": probabilities}\n",
    "            )\n",
    "        else: \n",
    "            raise ValueError(\n",
    "                \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" %(mode)\n",
    "            )\n",
    "        return output_spec\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "def create_tokenizer_from_hub_module(bert_hub_module_handle):\n",
    "    \"\"\"Get the vocab file and casing infor from the hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_hub_module_handle)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "\n",
    "        with tf.Session() as sess: \n",
    "            vocab_file , do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                        tokenization_info[\"do_lower_case\"]])\n",
    "            \n",
    "    return Tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file , do_lower_case=do_lower_case \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    processors = {\n",
    "        \"cola\": Run_classifier.ColaProcessor,\n",
    "        \"mnli\": Run_classifier.MnliProcessor,\n",
    "        \"mrpc\": Run_classifier.MrpcProcessor,\n",
    "    }\n",
    "\n",
    "    if not FLAGS.do_train and not FLAGS.do_eval:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "    tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "    task_name = FLAGS.task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "\n",
    "    tokenizer = create_tokenizer_from_hub_module(FLAGS.bert_hub_module_handle)\n",
    "\n",
    "    tpu_cluster_resolver = None\n",
    "    if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        master=FLAGS.master,\n",
    "        model_dir=FLAGS.output_dir,\n",
    "        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "            iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "            num_shards=FLAGS.num_tpu_cores,\n",
    "            per_host_input_for_training=is_per_host))\n",
    "\n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    num_warmup_steps = None\n",
    "    if FLAGS.do_train:\n",
    "        train_examples = processor.get_train_examples(FLAGS.data_dir)\n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "    model_fn = model_fn_builder(\n",
    "        num_labels=len(label_list),\n",
    "        learning_rate=FLAGS.learning_rate,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        use_tpu=FLAGS.use_tpu,\n",
    "        bert_hub_module_handle=FLAGS.bert_hub_module_handle)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=FLAGS.use_tpu,\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        train_batch_size=FLAGS.train_batch_size,\n",
    "        eval_batch_size=FLAGS.eval_batch_size,\n",
    "        predict_batch_size=FLAGS.predict_batch_size)\n",
    "\n",
    "    if FLAGS.do_train:\n",
    "        train_features = Run_classifier.convert_examples_to_features(\n",
    "            train_examples, label_list, FLAGS.max_seq_length, tokenizer)\n",
    "        tf.logging.info(\"***** Running training *****\")\n",
    "        tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "        tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "        tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "        train_input_fn = Run_classifier.input_fn_builder(\n",
    "            features=train_features,\n",
    "            seq_length=FLAGS.max_seq_length,\n",
    "            is_training=True,\n",
    "            drop_remainder=True)\n",
    "        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "    if FLAGS.do_eval:\n",
    "        eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n",
    "        eval_features = Run_classifier.convert_examples_to_features(\n",
    "            eval_examples, label_list, FLAGS.max_seq_length, tokenizer)\n",
    "\n",
    "        tf.logging.info(\"***** Running evaluation *****\")\n",
    "        tf.logging.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
    "\n",
    "        # This tells the estimator to run through the entire set.\n",
    "        eval_steps = None\n",
    "        # However, if running eval on the TPU, you will need to specify the\n",
    "        # number of steps.\n",
    "        if FLAGS.use_tpu:\n",
    "        # Eval will be slightly WRONG on the TPU because it will truncate\n",
    "        # the last batch.\n",
    "            eval_steps = int(len(eval_examples) / FLAGS.eval_batch_size)\n",
    "\n",
    "        eval_drop_remainder = True if FLAGS.use_tpu else False\n",
    "        eval_input_fn = Run_classifier.input_fn_builder(\n",
    "            features=eval_features,\n",
    "            seq_length=FLAGS.max_seq_length,\n",
    "            is_training=False,\n",
    "            drop_remainder=eval_drop_remainder)\n",
    "\n",
    "        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "\n",
    "        output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n",
    "        with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "            tf.logging.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    if FLAGS.do_predict:\n",
    "        predict_examples = processor.get_test_examples(FLAGS.data_dir)\n",
    "        if FLAGS.use_tpu:\n",
    "        # Discard batch remainder if running on TPU\n",
    "            n = len(predict_examples)\n",
    "            predict_examples = predict_examples[:(n - n % FLAGS.predict_batch_size)]\n",
    "\n",
    "            predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
    "            Run_classifier.file_based_convert_examples_to_features(\n",
    "                predict_examples, label_list, FLAGS.max_seq_length, tokenizer,\n",
    "                predict_file)\n",
    "\n",
    "            tf.logging.info(\"***** Running prediction*****\")\n",
    "            tf.logging.info(\"  Num examples = %d\", len(predict_examples))\n",
    "            tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
    "\n",
    "            predict_input_fn = Run_classifier.file_based_input_fn_builder(\n",
    "                input_file=predict_file,\n",
    "                seq_length=FLAGS.max_seq_length,\n",
    "                is_training=False,\n",
    "                drop_remainder=FLAGS.use_tpu)\n",
    "\n",
    "            result = estimator.predict(input_fn=predict_input_fn)\n",
    "\n",
    "            output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
    "            with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
    "                tf.logging.info(\"***** Predict results *****\")\n",
    "                for prediction in result:\n",
    "                    probabilities = prediction[\"probabilities\"]\n",
    "                    output_line = \"\\t\".join(\n",
    "                        str(class_probability)\n",
    "                        for class_probability in probabilities) + \"\\n\"\n",
    "                    writer.write(output_line)\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        flags.mark_flag_as_required(\"data_dir\")\n",
    "        flags.mark_flag_as_required(\"task_name\")\n",
    "        flags.mark_flag_as_required(\"bert_hub_module_handle\")\n",
    "        flags.mark_flag_as_required(\"output_dir\")\n",
    "        tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
