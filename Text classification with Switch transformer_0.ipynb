{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 # Only consider the top 20k words \n",
    "num_tokens_per_example = 200 # Only consider the first 200 words of each movie review \n",
    "(x_train , y_train) ,(x_val , y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), 'Training sequence')\n",
    "print(len(x_val), 'Validation sequence')\n",
    "\n",
    "x_train = keras.utils.pad_sequences(\n",
    "    -1 , maxlen=num_tokens_per_example\n",
    ")\n",
    "x_val = keras.utils.pad_sequences(-1 , maxlen= num_tokens_per_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32 # Embedding size for each token \n",
    "num_heads = 2 # Number of attention heads \n",
    "ff_dim = 32 # Hidden layers size in feed for ward network \n",
    "num_experts = 10 # Num experts used in the Switch Transformer .\n",
    "batch_size = 50 # Batch size \n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.25\n",
    "num_epochs = 3 # Num of epochs\n",
    "num_tokens_per_batch = (\n",
    "    batch_size * num_tokens_per_example\n",
    ") # Total number of tokens per batch \n",
    "print(f\"Number of token per batch: {num_tokens_per_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement toke & Position embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen , vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size , output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen , output_dim=embed_dim)\n",
    "\n",
    "    def call (self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        position = tf.range(start=0 , limit=maxlen, delta=1)\n",
    "        position = self.pos_emb(position)\n",
    "        x = self.token_emb(x)\n",
    "        return x + position\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedforward_network(ff_dim , name=None):\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ], \n",
    "        name = name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the load-balance loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balance_loss(router_probs ,expert_mask):\n",
    "    # router_probs [token_per_batch , num_experts] là xác xuất được chỉ định cho mỗi chuyên gia mã thông báo \n",
    "    # expert_mask [token_per_batch , num_experts] chứa chuyên gia có xác xuất định tuyến cao nhất dạng one - hot \n",
    "    num_experts = tf.shape(expert_mask)[-1]\n",
    "# lấy số lượng chuyên gia từ size của ma trận expert_mask \n",
    "# là một ma trân nhị phân có giá trị 1 nếu token được gửi đến chuyên gia tương ứng và 0 nếu không .\n",
    "    density = tf.reduce_mean(expert_mask, axis=0)\n",
    "# tính tỷ lệ toke được gửi đến mỗi chuyên gia , bằng cách lấy trung bình theo trục 0 của ma trận expert_mask \n",
    "# là một vector density có độ dài bằng số lượng chuyên gia (10)  và tổng = 1 (tổng tỷ lệ của các chuyên gia)\n",
    "    density_proxy = tf.reduce_mean(router_probs, axis=0)\n",
    "# Tính tỷ lệ xác xuất được gán cho mỗi chuyên từ bộ định tuyến \n",
    "# bằng cách lấy trung bình  theo trục 0 của ma trận router_probs , là một ma trận có giá trị 0 -> 1 cho biết xác xuất của token\n",
    "# được gửi đến chuyên gia tương ứng kết quả là 1 vector density_proxy có độ dài bằng số lượng chuyên gia và tổng bằng 1.\n",
    "    loss = tf.reduce_mean(density_proxy * density) *tf.cast(\n",
    "        (num_experts **2), tf.dtypes.float32 \n",
    "    )\n",
    "# Tính hàm mất mát bằng cách lấy tích vô hướng của 2 vector rồi nhân num_expertexpert^2 hàm mất mát này có gía trị nhỏ nhất \n",
    "# khi 2 vector này đồng nhất tức là đều có giá trị = 1/  num_expert \n",
    "# Điều này có nghĩa là token và xác xuất được phân phối đều cho các chuyên gia \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(layers.Layer):\n",
    "    def __init__(self, num_experts , expert_capacity):\n",
    "        self.num_experts = num_experts\n",
    "        self.router = layers.Dense(units=num_experts)\n",
    "        self.expert_capacity = expert_capacity\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, inputs , training=False):\n",
    "        # Input shape : [token_per_batch , embed_dim]\n",
    "        # router_logits shape : [token_per_batch , num_experts]\n",
    "        router_logits = self.router(inputs)\n",
    "        if training:\n",
    "            # Thêm nhiễu vào router_logits để khuyến khích việc khám phá của các chuyên gia \n",
    "            # Nhiễu ngẫu nhiên có giá trị từ 0.9 đến 1.1\n",
    "            router_logits += tf.random.uniform(\n",
    "                shape= router_logits.shape , minval=0.9 , maxval=1.1\n",
    "            )\n",
    "        # Tính router_probs là một tensor có shape [token_per_batch , num_exoperts]\n",
    "        # chứa xác suất của mỗi token được gửi đến mỗi chuyên gia \n",
    "        # Xác suất được tính bằng hàm softmax trên trục  -1 của router_logits\n",
    "        router_probs = keras.activations.softmax(router_logits, axis=-1)\n",
    "        # lấy chuyên gia có xác xuất cao nhất cho mỗi token tương ứng\n",
    "        # expert_gate là một tensor shape [token_per_batch , 1]\n",
    "        # chứa xác xuất cao nhất từ router_probs cho mỗi token . \n",
    "        # expert_index là một tensor có kích thuuwocs [toén_per_batch ,1]\n",
    "        # chứa các chỉ số của chuyên gia tương ứng với xác suất cao nhất cho mỗi token \n",
    "        expert_gate , expert_index = tf.math.top_k(router_probs ,k=1)\n",
    "        # Tính expert_mask là một tensor có kích thước [tokens_per_batch , num_experts]\\\n",
    "        # chứa giá trị nhị phân cho biết token nào được gửi đến chuyên gia nào \n",
    "        # Giá trị này đuơcj tính bằng cách sử dụng hàm tf.one_hot với expert_index và depth là num_exp\n",
    "        expert_mask = tf.one_hot(expert_index , depth=self.num_experts)\n",
    "        # Tính hàm mất mát cân bằng tải với router_probs và num_experts làm đầu vào \n",
    "        aux_loss = load_balance_loss(router_probs , expert_mask)\n",
    "        self.add_loss(aux_loss)\n",
    "        # Tính position_indx_expert là một tensor shape :[token_per_batch , num_experts]\n",
    "        # cho biết vị trí của token trong hàng đợi mỗi chuyên gia \n",
    "        # Giá trị này được tính bằng cách lấy tổng tích lũy theo trục 0 của experts_mask \n",
    "        # rồi nhân với expert_mask sau đó ép kiển anyf sang int32.\n",
    "        position_in_expert = tf.cast(\n",
    "            tf.math.cumsum(expert_mask ,axis=0) *expert_mask , tf.dtypes.int32\n",
    "        )\n",
    "        # Lọc ra các token có vị trí trong hàng đợi của chuyên gia nhỏ hơn expert_capacity,\n",
    "        # tức là các token không vượt quá khả năng xử lý của chuyên gia. \n",
    "        # Điều này được thực hiện bằng cách sử dụng hàm tf.math.less để so sánh \n",
    "        # position_in_expert và expert_capacity, \n",
    "        # rồi ép kết quả sang kiểu float32 và nhân với expert_mask. \n",
    "        # Kết quả là expert_mask được cập nhật lại để loại bỏ các token không được gửi đến \n",
    "        # các chuyên gia.\n",
    "        expert_mask *= tf.cast(\n",
    "            tf.math.less(\n",
    "                tf.cast(position_in_expert , tf.dtypes.int32) , self.expert_capacity\n",
    "            ),\n",
    "            tf.dtypes.float32,\n",
    "        )\n",
    "        # Tính expert_mask_flat là một tensor có kích thước [tokens_per_batch] \n",
    "        # bằng cách lấy tổng theo trục -1 của expert_mask. \n",
    "        # Giá trị này cho biết token nào được gửi đến ít nhất một chuyên gia.\n",
    "        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)\n",
    "        # Cập nhật lại expert_gate bằng cách nhân với expert_mask_flat để loại bỏ các token \n",
    "        # không được gửi đến bất kỳ chuyên gia nào.\n",
    "        expert_gate *= expert_mask_flat\n",
    "        #  Giá trị này cho biết xác suất định tuyến và hệ số cân bằng tải của mỗi token \n",
    "        # đối với mỗi chuyên gia và mỗi vị trí trong hàng đợi của chuyên gia.\n",
    "        combined_tensor = tf.expand_dims(\n",
    "            expert_gate\n",
    "            * expert_mask_flat\n",
    "            * tf.squeeze(tf.one_hot(expert_index, depth=self.num_experts), 1),\n",
    "            -1,\n",
    "        ) * tf.squeeze(tf.one_hot(position_in_expert, depth=self.expert_capacity), 1)\n",
    "        # Tính dispatch_tensor là một tensor có kích thước \n",
    "        # [tokens_per_batch, num_experts, expert_capacity] \n",
    "        # bằng cách ép kiểu combined_tensor sang kiểu float32. \n",
    "        # Giá trị này cho biết token nào được gửi đến chuyên gia nào và vị trí nào \n",
    "        # trong hàng đợi của chuyên gia bằng giá trị nhị phân 0 hoặc 1.\n",
    "        dispatch_tensor = tf.cast(combined_tensor, tf.dtypes.float32)\n",
    "\n",
    "        return dispatch_tensor, combined_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Switch(layers.Layer):\n",
    "    def __init__(self, num_experts , embed_dim , num_tokens_per_batch , capacity_factor=1.0):\n",
    "        self.num_experts = num_experts\n",
    "        self.embed_dim = embed_dim\n",
    "        self.experts = [\n",
    "            create_feedforward_network(embed_dim) for _ in enumerate(num_experts)\n",
    "        ]\n",
    "        self.expert_capacity = num_tokens_per_batch // self.num_experts\n",
    "        self.router = Router(self.num_experts , self.expert_capacity)\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        num_tokens_per_example = tf.shape(inputs)[1]\n",
    "\n",
    "        inputs=  tf.reshape(inputs , [num_tokens_per_batch , self.embed_dim])\n",
    "         # dispatch_tensor (tensor gửi đi ) shape : [expert_capacity , num_experts, tokens_per_batch]\n",
    "        # combine_tensor (tensor kết hợp) shape : [token_per_batch , num_experts , expert_capacity]\n",
    "        dispatch_tensor, combine_tensor = self.router(inputs)\n",
    "        # expert_inputs shape : [num_experts, expert_capacity , embed_dim]\n",
    "        # tính toán một tensor mới có kích thước là [expert_capacity, num_experts, embed_dim] \n",
    "        # bằng cách thực hiện phép nhân ma trận giữa tensor inputs và tensor dispatch_tensor theo công thức ‘ab,acd->cdb’\n",
    "        expert_inputs = tf.einsum('ab , acd->cdb', inputs, dispatch_tensor)\n",
    "        expert_inputs = tf.reshape(\n",
    "            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n",
    "        )\n",
    "        # Dispatch to experts (gửi đến chuyên gia)\n",
    "        # dùng hàm tf.unstack để tách tensor expert_inputs thành một danh sách các tensor có kích thước là [expert_capacity, embed_dim] theo chiều thứ nhất (num_experts). \n",
    "        # Hàm tf.unstack cho phép bạn tách một tensor có kích thước là R thành một danh sách các tensor có kích thước là R-1 theo một chiều nào đó\n",
    "        expert_input_list = tf.unstack(expert_inputs, axis=0)\n",
    "        expert_output_list = [\n",
    "            self.experts[idx](expert_input)\n",
    "            for idx, expert_input in enumerate(expert_input_list)\n",
    "        ]\n",
    "        # Expert_outputs shape : [expert_capacity , num_expert , embeb_dim ]\n",
    "        expert_outputs = tf.stack(expert_output_list, axis=1)\n",
    "        # expert_outputs_combined shape: [tokens_per_batch, embed_dim]\n",
    "        expert_outputs_combined = tf.einsum(\n",
    "            \"abc,xba->xc\", expert_outputs, combine_tensor\n",
    "        )\n",
    "        # output_shape : [batch_size , num_tokens_per_example , embed_dim]\n",
    "        outputs = tf.reshape(\n",
    "            expert_outputs_combined,\n",
    "            [batch_size, num_tokens_per_example, self.embed_dim],\n",
    "        )\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim , num_heads , ffn , dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads , key_dim=embed_dim)\n",
    "        self.ffn = ffn  # The ffn can be either a standard feedforward network or a switch\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=16-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output , training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier():\n",
    "    switch = Switch(num_experts , embed_dim , num_tokens_per_batch)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads , switch)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_tokens_per_example ,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(\n",
    "        num_tokens_per_example , vocab_size , embed_dim\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(classifier):\n",
    "    classifier.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    history = classifier.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "classifier = create_classifier()\n",
    "run_experiment(classifier)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
