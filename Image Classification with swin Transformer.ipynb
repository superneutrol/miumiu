{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (2, 2)  # 2-by-2 sized patches\n",
    "dropout_rate = 0.03  # Dropout rate\n",
    "num_heads = 8  # Attention heads\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_mlp = 256  # MLP layer size\n",
    "qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "window_size = 2  # Size of attention window\n",
    "shift_size = 1  # Size of shifting window\n",
    "image_dimension = 32  # Initial image size\n",
    "\n",
    "num_patch_x = input_shape[0] // patch_size[0]\n",
    "num_patch_y = input_shape[1] // patch_size[1]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 40\n",
    "validation_split = 0.1\n",
    "weight_decay = 0.0001\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng phương thức tạo vách ngăn cửa số cho hình ảnh \n",
    "def window_partition (x , window_size):\n",
    "    # lấy ra các kích thước của 1 tensor đầu vào x \n",
    "    # gồm batch_size , h , w , c \n",
    "    _ , height , width , channels = x.shape \n",
    "    # lấy ra số lượng bản vá theo chiều x , y bằng cách chia \n",
    "    # 2 chiều cho kích thước cửa sổ \n",
    "    patch_num_x = width // window_size \n",
    "    patch_num_y = height // window_size \n",
    "    # Định hình lại x thành hình dạng  (x là 1 tensor)\n",
    "    # -1 số lượng hình ảnh , số lượng patch theo chiều y , chiều cao của patch\n",
    "    #  số lượng patch_theo x , chiều ngang của patch , số kênh màu \n",
    "    x = tf.reshape(\n",
    "        x , shape=(-1 , patch_num_y , window_size , patch_num_x , window_size , channels)\n",
    "    )\n",
    "    # sau đó chuyển vị lại hình ảnh \n",
    "    # với shape = [-1 , patch_num_y , patch_num_x , window_size , window_size , channels]\n",
    "    x = tf.transpose(x , (0 , 1 , 3 , 2, 4 ,5 ))\n",
    "    # định dạng lại kích thước của các cứa sổ và trả về nó \n",
    "    # shape = [số lượng cửa sổ , x , y , c]\n",
    "    windows = tf.reshape(x , shape=(-1 , window_size , window_size , channels))\n",
    "    return windows \n",
    "\n",
    "# Xây dựng hàm cứa sổ đảo ngược dùng để ghép các bản vá thành bản vá lớn hơn \n",
    "# hàm này có chức năng ngược lại với hàm tạo vách ngăn cửa sổ \n",
    "def window_reverse(windows, window_size, height, width, channels):\n",
    "    # Tính toán số lượng bản vá theo trục x và y \n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size \n",
    "    # định hình lại x thành dạng (x là 1 tensor)\n",
    "    # (số lượng patch_theo x, số lượng patch theo x, theo y , kích thước 2  chiều của mỗi patch , kênh màu)\n",
    "    x = tf.reshape(\n",
    "        windows , \n",
    "        shape =(-1  , patch_num_y , patch_num_y , window_size , window_size , channels)\n",
    "    )\n",
    "    # sau đó chuyển vị tensor này về dạng \n",
    "    # sahpe = (số lượng , patch_num_y , chiều cao patch (window_size) , patch_num_x (chiều nagng)\n",
    "    # só kênh màu (channels))\n",
    "    x = tf.transpose(x, perm=(0 ,1 ,3 ,2 ,4 ,5))\n",
    "    # định hình lại kích thước cho x với -  1 là tham số tự tính cho phù hợp \n",
    "    # với bước tính toán để có được số lượng cửa sổ phù hợp \n",
    "    x = tf.reshape(x , shape=(-1 , height , width , channels))\n",
    "    return x \n",
    "\n",
    "# Xây dựng hàm Dropath để loại bỏ ngẫu nhiên cho một tensor đầu vào \n",
    "layers.Dropout(0.1) \n",
    "class DropPath(layers.Layer):\n",
    "    def __init__(self, drop_prob=None , **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob \n",
    "\n",
    "    def call(self, x):\n",
    "        # lấy ra kích thước của tensor đầu vào x \n",
    "        input_shape = tf.shape(x)\n",
    "        # lấy ra kích thước lô \n",
    "        batch_size = input_shape[0]\n",
    "        # lấy ra số chiều của x bằng hàm rank \n",
    "        rank = x.shape.rank #  = 4 shape x [batch_size , window_size , widow_size , channels]\n",
    "        # tạo 1 biến shape có shape = batch_size , 1 , 1 ,1 \n",
    "        # đầu tiên ta tạo ma biến typle với batch_size phàn tử là chiều đầu tiên của shape \n",
    "        # sau đó ta tính toán số chiều còn lại  = 1 *(rank-1) tức là 3 chiều \n",
    "        # với shape 3 chiều  = 1 \n",
    "        shape = ( batch_size,) + (1,) * (rank-1) # shape = [batch_size , 1 , 1 ,1]\n",
    "        # sau đó tạo 1 tensor ngẫu nhiên = xác xuất 1 - drop_prob  + shape \n",
    "        # mục đích tạo ra 1 tensor với các phần tử đc lấy ngẫu nhiên [0 -> 1]\n",
    "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape , dtype=x.dtype)\n",
    "        # Xây dựng một ma trận Path_mask bằng cách làm tròn xuống các tỷ lệ của tensor \n",
    "        path_mask = tf.floor(random_tensor)\n",
    "        # sau đó tính đầu ra bằng thực hiện chia cho tỷ lệ 1 - drop_prob rồi nhân với ma trận \n",
    "        # tỷ lệ path_mask \n",
    "        output = tf.math.divide(x , 1 - self.drop_prob) * path_mask\n",
    "        return output \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng Của sổ chú ý cho khối swin transformer Block \n",
    "# với các tham số , dim (số chiều không gian vector) , window_size , num_heads ,\n",
    "# dropout_rate \n",
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(\n",
    "            self, dim , window_size , num_heads , qkv_bias=True , dropout_rate=0.0, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim \n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        # tạo một hệ số tỷ lệ  = _/ dim // num_heads\n",
    "        # sử dụng cho phép tính toán self-attention \n",
    "        self.scale = (dim // self.num_heads) ** -0.5\n",
    "        self.qkv = layers.Dense(dim*3 , use_bias=qkv_bias)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        # thêm một lớp nhúng tuyến tính \n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "    # Xây dựng phương thức sử dụng để tính toán các chỉ số vị trị tương đối \n",
    "    # cho mỗi cặp phần tử trong cửa sổ \n",
    "    def build (self, input_shape):\n",
    "        # Khởi tạo ma trận chứa số lượng phần tử trong cửa sổ \n",
    "        num_window_elements = (2 * self.window_size[0]- 1) * (\n",
    "            2 * self.window_size [1] - 1\n",
    "        )\n",
    "        # Xây dựng bảng chứa các giá trị bias vị trí tương đối \n",
    "        # cho mỗi cặp các phần tử trong các cứa sổ \n",
    "        # shape = [num_elemnet , num _heads]\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            shape =(num_window_elements , self.num_heads) , \n",
    "            # đặt tất acr các tham số  trong ma bảng = 0 và \n",
    "            # cho phép cập nhật nó trong quá trình huấn luyện\n",
    "            initializer= tf.initializers.Zeros(),\n",
    "            trainable = True,\n",
    "        )\n",
    "        # khởi tạo 2 mảng chứa các chỉ số hàng và cột của các phần tử trong cửa sổ .\n",
    "        coords_h =  np.arange(self.window_size[0])\n",
    "        coords_w =  np.arange(self.window_size[1])\n",
    "        # Xây dựng ma trận coords từ ma trận hàng và cột ở trênn\n",
    "        # đặt indexing = ij để mỗi phần tủ trong ma trận là duy nhất\n",
    "        coords_matrix = np.meshgrid(coords_h , coords_w , indexing='ij')\n",
    "        # xây dựng  tensor coords bằng cách xếp trồng ma trận coords \n",
    "        # shap = [2 , window_size[0] , window_size[1]]  chiều đầu tiên là chiều của các chỉ số \n",
    "        # hàng và cột\n",
    "        coords = np.stack(coords_matrix)\n",
    "        # sau flatten lại tensor này thành 2 chiều \n",
    "        # shape = [2 , -1]  với -1 là chỉ số tự tính  = num_window_element \n",
    "        coords_flatten = coords.reshape(2, -1)\n",
    "        # Xây dựng ma trận relative_coords  bằng cách thêm chiều cho ma trận flattent\n",
    "        # sau đó thực hiện phép trừ để có được ma trận relative_coords \n",
    "        # shape = [2, num_window_elements, num_window_elements] \n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :,]\n",
    "        # Sau đó thực hiện chuyển vị ma trận shape = [num_window_elements, num_window_elements, 2]\n",
    "        # Chiều cuối cùng cho biết khoảng cách vị trí hàng và cột \n",
    "        relative_coords = relative_coords.transpose([1 , 2 , 0])\n",
    "        # Cộng các chỉ số khoảng cách theo hàng và cột của ma trận với một hằng số duy nhất \n",
    "        relative_coords[:, :, 0] += self.window_size[0] -1 \n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        # Nhân các chỉ só vị trí tương đối của ma trận với hằng số để có được giá trị duy nhất \n",
    "        # cho mỗi hàng \n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        # Cộng hai chỉ số hàng và cột cửa ma trận relative_coords theo chiều cuối để thu được \n",
    "        # một ma trận chứa các cặp chỉ số tương đối cho mỗi cặp phần tử trong cửa sổ \n",
    "        # shape = [num_elements, num_elements]\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        # Xây dựng ma trận chỉ số vị trí tương đối cho mỗi cặp phần tử trong cửa sổ\n",
    "        # và accs chỉ số này không được cập nhật\n",
    "        self.relative_position_index = tf.Variable(\n",
    "            initial_value=tf.convert_to_tensor(relative_position_index), \n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "    # Xây dựng phương thức tính toán Attention , ma trận position_bias , và masked attention\n",
    "    def call(self, x,  mask = None):\n",
    "        # lấy ra kích thước của tensor x\n",
    "        _ , size , channels  = x.shape \n",
    "        head_dim = channels // self.num_heads\n",
    "        # Tính toán qkv bằng cách ánh xạ x qua lớp mạng dày đặc \n",
    "        x_qkv = self.qkv(x)\n",
    "        # thay đổi hjinhf dạng của tensor x  = [ batch_size , size , 3, heads , head_dim]\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n",
    "        # Chuyển vị x_qkv = [3 , batch_size , heads , size , head_dim] để phân biệt được \n",
    "        # 3 vector q , k , v\n",
    "        x_qkv = x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n",
    "        # lấy ra q , k , v lần lượt \n",
    "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
    "        # sau đó nhân q với hệ só scale để tăng tính đa dạng cho q\n",
    "        q = q *self.scale \n",
    "        # Thực hiện chuyển vị k để có thể nhân k với q \n",
    "        # shape = [batch_size , num_heads , head_dim , size]\n",
    "        k = tf.transpose(k , perm=(0 ,1 ,3 ,2))\n",
    "        # Nhân ma trận q vs  k để thu được tensor \n",
    "        # shape = [batch_size,  num_heads , size ,size]\n",
    "        attn = q @ k \n",
    "\n",
    "\n",
    "        # Tính toán só lượng phần tử trong một cửa sổ \n",
    "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
    "        # Thay đổi hình dạng của tensor relative_position_idex \n",
    "        # thành ma trận 1 chiều duy nhất chứa các chỉ số idx của các cặp phần tử\n",
    "        relative_position_index_flat = tf.reshape(\n",
    "            self.relative_position_index, shape=(-1,)\n",
    "        )\n",
    "        # lấy các giá trị bias cho các vị trí tương đối từ bảng relative_position_bias\n",
    "        #  theo chỉ số relative_position_index_flat\n",
    "        relative_position_bias = tf.gather(\n",
    "            self.relative_position_bias_table, relative_position_index_flat\n",
    "        )\n",
    "        # Định hình lại hình dạng tensor thanh [num_elements , num_elements , num heads]\n",
    "        relative_position_bias = tf.reshape(\n",
    "            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n",
    "        )\n",
    "        # chuyển vị ma trận relative_position_bias shape []\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n",
    "        # sau đó công ma attn vs relative_position_bias \n",
    "        # để thêm bias cho các vị trí tương đối trong cửa sổ # chú ý 2 ma trận này có cùng kích thước như nhau khác chỉ số batch_size \n",
    "        #  S sẽ có dạng (batch_size, num_heads, size, size)\n",
    "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
    "\n",
    "        # Xây dựng mặt nạ cho window attention \n",
    "        # Kiểm tra xem Mask có tồn tại không là 1 tensor nhị phân\n",
    "        if mask is not None :\n",
    "            # lấy ra kích thước thứ nhất của mask là số lượng cửa sổ  trong x\n",
    "            num_Window = mask.get_shape()[0]\n",
    "            # chuyển mask thành kiểu số thực và thêm 2 chiều rỗng \n",
    "            # shap = (1, 1, num_windows, window_size * window_size, window_size * window_size)\n",
    "            mask_float = tf.cast(\n",
    "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n",
    "            )\n",
    "            # thay đổi hình dnagj của attention , thêm số lượng cửa sổ vào attent \n",
    "            # sau đó cộng mask choa atention để che đi các chỉ số \n",
    "            # shape attn = -1 , num_Window , self.num_heads , size , size\n",
    "            attn = (\n",
    "                # [batch_size , num_window ,num head, size , size]\n",
    "                tf.reshape(attn, shape=(-1 , num_Window , self.num_heads , size , size))\n",
    "                # + (1, 1, num_windows, window_size * window_size, window_size * window_size)\n",
    "                + mask_float\n",
    "            )\n",
    "            # Sau khi thêm mặt nạ attention ta trả attention về kích thước ban đầu \n",
    "            attn = tf.reshape(attn, shape=(-1 , self.num_heads , size, size))\n",
    "            # áp dụng tính toán softmax cho attention theo chiều cuối tức size \n",
    "            # là kích thước theo chiều của ma trận \n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        else:\n",
    "            # áp dụng tính toán softmax cho attention theo chiều cuối tức size ngay lập tức\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        # loại bỏ bớt các tham số có tỷ lệ kém \n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Tính điểm socre cho attention shap = [ batch_size , num_heads , size , head_dim]\n",
    "        x_qkv = attn @ v\n",
    "        # Hoán vị các chiều của x_qkv để có dạng (batch_size, size, num_heads, head_dim)\n",
    "        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n",
    "        # Thay đổi hình dạng của x_qkv thành (batch_size, size, channels), \n",
    "        # để nối các đầu chú ý lại với nhau theo chiều thứ 3 \n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
    "        # áp dụng lớp dense để biến đổi về 1 tensor có kích thước ban đầu và 1 lớp bỏ học \n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng khối swin transformer \n",
    "class SwinTransformer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_patch,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        num_mlp=1024,\n",
    "        qkv_bias=True,\n",
    "        dropout_rate=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dim = dim  # number of input dimensions\n",
    "        self.num_patch = num_patch  # number of embedded patches\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "        self.window_size = window_size  # size of window\n",
    "        self.shift_size = shift_size  # size of window shift\n",
    "        self.num_mlp = num_mlp  # number of MLP nodes\n",
    "\n",
    "        # Xây dựng lớp layerNormalization \n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        # Lớp attention \n",
    "        self.attn = WindowAttention(\n",
    "            dim , \n",
    "            window_size=(self.window_size , window_size),\n",
    "            qkv_bias=qkv_bias,\n",
    "            num_heads=num_heads,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.drop_path = DropPath(dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(num_mlp),\n",
    "                layers.Activation(keras.activations.gelu),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(dim),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        if min(self.num_patch) < self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.num_patch)\n",
    "        \n",
    "    #  Xây dựng phương thức thực hiện Shifted Window Attention \n",
    "    def build(self, input_shape):\n",
    "        # Kiểm tra xem shift size có bằng  \n",
    "        if self.shift_size == 0 :\n",
    "            # nếu = 0 không cần xây dựng mặt nạ \n",
    "            self.attn_mask = None \n",
    "        # xét trường hợp còn lại\n",
    "        else:\n",
    "            # lấy ra chiều h và w từ num_patch\n",
    "            # num_patch là một ma trận chứa số lượng bản vá h , w là số lượng bản vá theo \n",
    "            # chiều tương ứng \n",
    "            height , width = self.num_patch\n",
    "            # xây dựng lát cắt theo chiều cao \n",
    "            # mỗi lát cắt dài = window_size và được dịch chuyển với \n",
    "            # bước nhảy = shift_size \n",
    "            h_slices = (\n",
    "                # mỗi trục gồm 3 lát cắt \n",
    "                slice(0 , - self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                # lát cắt cuối trượt từ - 1 đến hết \n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            # xây dựng lát cắt theo chiều cao \n",
    "            # mỗi lát cắt dài = window_size và được dịch chuyển với \n",
    "            # bước nhảy = shift_size\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            # Xây dựng một mặt nạ ma trận = 0 shape = [1 , h , w , 1]\n",
    "            mask_array = np.ones((1 , height , width , 1))\n",
    "            # khởi tạo biến count cho biết số lượng cửa sổ đã duyệt qua \n",
    "            count = 0 \n",
    "            # duyệt qua các lát cắt của slice\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    # sau đó gán cho mỗi phần tử trong mảng từ 0 -> 8 theo count \n",
    "                    mask_array[:, h , w , :] = count \n",
    "                    count += 1\n",
    "            # chuyển đổi mask_array thành 1 tensor \n",
    "            mask_array = tf.convert_to_tensor(mask_array)\n",
    "            # Xây dựng ma trận mặt nạ cho cửa sổ \n",
    "            # phân chia mask_array thành các cửa sổ nhỏ hơn có kích thước window_size\n",
    "            # trả về 1 tensor mỗi phần tử của tensor này  giá trị 0- > nố lượng patch trong 1 ô \n",
    "            # tương ứng với vị trị của cửa sổ trong mội khối \n",
    "            mask_windows = window_partition(mask_array , self.window_size)\n",
    "            # Reshape lại tensor với shape = [-1 , window_size * window_size]\n",
    "            # tức là mỗi hàng là một cứa sổ \n",
    "            mask_windows = tf.reshape(\n",
    "                mask_windows , shape=[-1 , self.window_size * self.window_size]\n",
    "            )\n",
    "            # Tính toán Attention_mask bằng cách lấy hiệu giữa các hàng của tensor \n",
    "            # shape = [num_window , window_size * window_size ,  window_size * window_size]\n",
    "            # mỗi phần tử trong tensor là 1 ma trận vuông shape = [window_size * window_size * window_size * window_size]\n",
    "            # biểu diễn mức độ tương qua giữa các patch trong một cửa sổ với các patch trong cửa sổ khác \n",
    "            attn_mask = tf.expand_dims(mask_windows , axis=1) - tf.expand_dims(mask_windows , axis=2)\n",
    "            # Thay đổi giá trị trong attn_mask theo hiệu của mask_windows \n",
    "            # nếu hai hàng có giá trị khác nhau thì không trùng = -100 cần tính attention \n",
    "            attn_mask = tf.where(attn_mask != 0, -100.0 ,attn_mask )\n",
    "            # nếu 2 hàng bằng nhau gán  = 0.0 không cần tính attention \n",
    "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
    "            # Khởi tạo att_mask là một biến không thay đổi\n",
    "            # shape = [num_window * window_size * window_size * window_size * window_size]\n",
    "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n",
    "\n",
    "\n",
    "    # Xây dựng phương thức thực hiện cơ chế chú ý cửa sổ trên dữ liệu đầu vào \n",
    "    def call(self, x): # x là dữ liệu đầu vào shape = batch_size , num_patch , channels \n",
    "        # lấy ra chiều cao và chiều rộng từ ma trận num_patch \n",
    "        height , width = self.num_patch \n",
    "        # lấy ra các kích thước cửa x \n",
    "        _ , num_patches_before , channels = x.shape \n",
    "        # tạo một biến x_skip để lưu trữ lại x ban đầu , sử dụng cho phép cộng dư thừa \n",
    "        # sau này \n",
    "        x_skip = x\n",
    "        # Áp dụng một lớp chuẩn hóa theo tầng cho lớp Mục đích để x có phân bố nhất quán \n",
    "        # không bị ảnh hưởng quá nhiều bởi các giá trị ngoại lai. Giúp cho việc xây dựng \n",
    "        # cơ chế chú ý cửa sổ cho x sau đó được hiệu quả và ổn định \n",
    "        x = self.norm1(x)\n",
    "        # reshape lại x với kích thước [ batch_size , height , width , channels]\n",
    "        # để có thể thực hiện cửa sổ trượt cho x \n",
    "        x = tf.reshape(x , shape=(-1 , height, width , channels))\n",
    "        # kiểm tra xem kích thước chuyển đổi shifted_size có > 0 \n",
    "        # nếu có ta thực hiện cửa sổ dịch chuyển cho ma trận x \n",
    "        if self.shift_size > 0:\n",
    "            # ta thực hiện dịch chuyển cửa sổ theo 2 chiều ngang và dọc\n",
    "            # tức là ta áp dụng trên chiều 1 và 2 của x \n",
    "            shifted_x = tf.roll(\n",
    "                x , shift=[-self.shift_size , -self.shift_size], axis=[1,2]\n",
    "            )\n",
    "        # trường hợp còn lại tức là không tồn tại kích thuớc chuyển đổi \n",
    "        else:\n",
    "            # ta gán shift = x\n",
    "            shifted_x = x \n",
    "        # sau khi thực hiện phương pháp cửa sổ dịch chuyển ta tiến hành tách các \n",
    "        # ô cửa sổ trong không gian dịch chuyển thành các vách ngăn cửa sỏ rồi sau đó \n",
    "        # thực hiện mặt nạ và tính toán chú ý trên các ô cửa sổ \n",
    "        x_windows = window_partition(shifted_x , self.window_size)\n",
    "        # định hình lại x_windows thành [batch_size , window_size * window_size, channels]\n",
    "        # để có thể áp dụng lớp chú ý vào \n",
    "        x_windows = tf.reshape(x_windows , shape=(-1 , window_size * window_size, channels))\n",
    "        # Thực hiện tính toán attention cho các ô cửa sổ và áp dụng mặt nạ cho các ô \n",
    "        attn_windows = self.attn(x_windows , mask=self.attn_mask)\n",
    "        # thay đổi hình dạng attn_windows thành [batch_size , window_size, window_size , channels]\n",
    "        # để có thể chuyển đổi lại ma trận ban đầu \n",
    "        attn_windows = tf.reshape(\n",
    "            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n",
    "        )\n",
    "        # sau đó chuyển đổi lại thành ma trận ban đầu bằn phương thức window_reverse\n",
    "        # và chuyền vào các tham số tương ứng mục tiếu => shape [batch_size , height , weight , channels]\n",
    "        shifted_x = window_reverse(\n",
    "            attn_windows , self.window_size , height , width , channels \n",
    "        )\n",
    "        # nếu shifted_size > 0 thực hiện phép chuyển dịch ngược lại thành hình ảnh ban đầu \n",
    "        # theo chiều ngang và dọc \n",
    "        if self.shift_size > 0 : \n",
    "            x = tf.roll(\n",
    "                shifted_x , shift=[self.shift_size , self.shift_size] , axis= [1,2]\n",
    "            )\n",
    "        # trường hợp còn lại gán x = shifted_x \n",
    "        else:\n",
    "            x = shifted_x\n",
    "        \n",
    "        # Thay đổi lại hình dạng x thành [batch_size, height * width , channels] \n",
    "        # để có thể áp dụng lớp noron da tầng \n",
    "        x = tf.reshape(x , shape=(-1 , height * width , channels))\n",
    "        # chuẩn hóa x qua lớp DropPath\n",
    "        x = self.drop_path(x)\n",
    "        # tạo ra lớp kết nối dư bằng cách công x_kip vs x\n",
    "        x = x_skip + x\n",
    "        # gán lại x_skip = x để thực hiện cho lớp kết nối dư tiếp theo \n",
    "        x_skip = x\n",
    "        # thêm lớp norm2 , mlp ,drop_path\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        # cuối cùng thêm 1 lớp kết nối dư và trả về \n",
    "        x = x_skip + x\n",
    "        # output shape = [batch_size , num_patch , hidden_size]\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtract(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwrgs):\n",
    "        super().__init__(**kwrgs)\n",
    "        self.patch_size_x = patch_size[0]\n",
    "        self.patch_size_y = patch_size[1]\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images , \n",
    "            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            rates=(1, 1, 1, 1),\n",
    "            padding=\"VALID\", \n",
    "        )\n",
    "        patch_dim = patches.shape[-1]\n",
    "        patch_num = patches.shape[1]\n",
    "        return tf.reshape(patches , (batch_size , patch_num * patch_num , patch_dim))\n",
    "    \n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch , embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patch = num_patch \n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
    "        return self.proj(patch) + self.pos_embed(pos)\n",
    "    \n",
    "\n",
    "\n",
    "class PatchMerging(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape  x = [batch_size , num_patch , hiddent_dim]\n",
    "        height, width = self.num_patch\n",
    "        _, _, C = x.get_shape().as_list() # c = 64  \n",
    "        x = tf.reshape(x, shape=(-1, height, width, C))\n",
    "        # Tiếp theo, hàm call tách x thành bốn tensor con x0, x1, x2 và x3, \n",
    "        # mỗi tensor có kích thước (-1, height // 2, width // 2, C) và chứa các patch ở vị trí\n",
    "        # chẵn-chẵn, lẻ-chẵn, chẵn-lẻ và lẻ-lẻ của ma trận patch \n",
    "        # tức là lấy các chỉ số theo chỉ só hàng và cột với bước nhảy = 2\n",
    "        x0 = x[:, 0::2, 0::2, :]  # ở đây lấy các giá trị hàng chẵn cột chẵn step 2\n",
    "        x1 = x[:, 1::2, 0::2, :]  # các giá trị hàng lẻ cột chẵn  step = 2\n",
    "        x2 = x[:, 0::2, 1::2, :]  # các chỉ số theo hàng chẵn cột lẻ step = 2\n",
    "        x3 = x[:, 1::2, 1::2, :]  # các chỉ số theo hàng lẻ cột lẻ \n",
    "        # Nối các phần lại theo chiều cuối cùng, tức là tăng kích thước đặc trưng lên 4 lần (4C \n",
    "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
    "        #  tức là giảm số patch xuống một nửa theo chiều cao và chiều rộng, nhưng tăng kích thước đặc trưng lên gấp đôi\n",
    "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(input_shape)\n",
    "x = layers.RandomCrop(image_dimension, image_dimension)(input)\n",
    "x = layers.RandomFlip(\"horizontal\")(x)\n",
    "x = PatchExtract(patch_size)(x)\n",
    "x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=0,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=shift_size,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "output = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(input, output)\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    optimizer=tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=validation_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {round(loss, 2)}\")\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
