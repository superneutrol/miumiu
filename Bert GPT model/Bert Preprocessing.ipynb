{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Preprocessor class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp \n",
    "import tensorflow as tf \n",
    "keras_nlp.models.BertPreprocessor(\n",
    "    tokenizer , sequence_length=512 , truncate =\"round_robin\" , **kwargs\n",
    ")\n",
    "\n",
    "# Lớp tiền xử lý Bert mã hóa và đóng gói các đầu vào \n",
    "# Lớp tiền cử lý này sẽ thực hiện 3 việc \n",
    "# 1 : Mã hóa bất kỳ số lượng phân đoạn đầu vào bằng cách sử dụng Tokenizer \n",
    "# 2 : Đóng gói các yếu tố đầu vào lại với nhau băg cách sử dụng tệp \n",
    "# keras_nlp.layers.MultiSegmentPacker với '[cls]' , '[sep]' , '[pad]' mã thông báo thích hợp \n",
    "# 3 : Xây dựng một từ điển với các khóa 'token_ids' , 'segment_ids' , 'padding_mask' , \n",
    "# có thể được chuyển trực tiếp đến mô hình Bert \n",
    "\n",
    "# Lớp này có thể sử dụng trực tiếp tf.data.Dataset.map để tiền xử lý chuỗi ở (x ,y , sample_weight)\n",
    "# Định dạng được sử dụng bơur keras.Model.fit \n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_base_en_uncased\"\n",
    ")\n",
    "# Tokenize and pack a sigle sentence\n",
    "preprocessor(\"The quick brown fox jumped.\")\n",
    "\n",
    "# Tokenizer a batch of sigle sentences \n",
    "preprocessor(['Ther quick brown fox jumped.','Call me Ishmael.'])\n",
    "\n",
    "# Preprocess a batch of sentence pairs \n",
    "# When handling multiple sequences ,always convert to tensors first \n",
    "first = tf.constant(['The quick brown fox jumped.',\" Call me Ishmael.\"])\n",
    "second = tf.constant(['The fox tripped.', 'Oh look, a whale.'])\n",
    "preprocessor((first ,second))\n",
    "\n",
    "# Custom vocabulary \n",
    "vocab = ['[UNK]' , '[CLS]', '[SEP]', '[PAD]' , '[MASK]']\n",
    "vocab += [\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \".\"] \n",
    "tokenizer = keras_nlp.models.BertTokenizer(vocabulary=vocab)\n",
    "preprocessor = keras_nlp.models.BertPreprocessor(tokenizer)\n",
    "preprocessor(\"The quick brown fox jumped.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    'bert_base_en_uncased'\n",
    ")\n",
    "first = tf.constant(['The quick brown fox jumped.', 'Call me Ismael.'])\n",
    "second = tf.constant(['The fox tripped.',\"Oh look, a wale.\"])\n",
    "label = tf.constant([1,1])\n",
    "\n",
    "# Lập đồ thị dán nhãn cho câu đơn\n",
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (first ,label))\n",
    "ds = ds.map(preprocessor , num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Lập đồ thị không dán nhãn cho câu đơn \n",
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    first\n",
    ")\n",
    "ds = ds.map(preprocessor , num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Lập đồ thị dán nhãn cho cặp câu \n",
    "ds = tf.data.Dataset.from_tensor_slices(((first, second), label))\n",
    "ds = ds.map(preprocessor , num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Lập đồ thị  không dán nhãn cho các cặp câu \n",
    "ds =  tf.data.Dataset.from_tensor_slices((first , second))\n",
    "\n",
    "ds = ds.map(\n",
    "    lambda  first , second : preprocessor(x=(first ,second)), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải một lớp trình xử lý từ đặt trước \n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toekenizer property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_nlp.models.BertPreprocessor.tokenizer "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
