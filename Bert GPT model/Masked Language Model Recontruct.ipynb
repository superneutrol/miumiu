{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from keras import layers \n",
    "from dataclasses import dataclass # có chức năg tạo các lớp dữ liệu \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import re \n",
    "from pprint import pprint \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class Config:\n",
    "    MAX_LEN = 256 \n",
    "    BATCH_SIZE = 32 \n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128 \n",
    "    NUM_HEADS = 8\n",
    "    FF_DIM = 128 \n",
    "    NUM_LAYERS = 1 \n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng hàm lấy dữ liệu văn bản từ các files tài liệu \n",
    "def get_text_list_from_file(files):\n",
    "    # tạo 1 danh sách để chứa các dòng văn bản của những đoạn văn bản có tronh các filé \n",
    "    text_list = []\n",
    "    # duyệt qua các file \n",
    "    for _ in files:\n",
    "        # sử dụng with open để mở các file và gán nó cho f \n",
    "        with open(_) as f:\n",
    "            # Duyệt qua các dòng văn bản có trong file và add vào list \n",
    "            for line in f :\n",
    "                text_list.append(line)\n",
    "    # trả về danh sách các ròng văn bản \n",
    "    return text_list\n",
    "\n",
    "# Xây dựng hàm đọc và định dạng văn bản từ văn bản thô \n",
    "# và trả về 1 bảng dữ liệu có 2 cột là review và sentiment \n",
    "def get_data_from_text_files(folder_name):\n",
    "    # sử dụng biến glob để định dạng toàn bộ văn bản thành đồng dạng \n",
    "    # của thư mục folder_name và các tệp này có định dạng  aclImdb/folder_name/pos/*.txt\n",
    "    pos_files = glob.glob('aclImdb/' + folder_name + '/pos/*.txt')\n",
    "    # Tiến hành đọc và lấy ra các dòng dữ liệu từ file \n",
    "    pos_texts = get_text_list_from_file(pos_files)\n",
    "    # Thực hiện tương tự với định dạng thư mục aclImdb/folder_name/neg/*.txt \n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_file(neg_files)\n",
    "\n",
    "    # Tiến hành xây dựng dataFrame với 2 cột review và sentiment \n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'review': pos_texts + neg_texts ,# số lượng phần từ trong một cột \n",
    "            # với cột sentiment ta biến đổi dạng 0 và 1 \n",
    "            'sentiment' : [0] * len(pos_texts) + [1] * len(neg_texts)\n",
    "        }\n",
    "    )\n",
    "    # sử dụng hàm samples để xáo trộn mẫu ngẫu nhiên trong tệp df \n",
    "    # kết quả alf 1 dataFrame có cùng kích thước với df ban đầu nhưng bị xáo trộn \n",
    "    # sau đó dùng hàm reset_index để đặt lại chỉ số index lại từ đầu cho các hàng \n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "\n",
    "# Xây dựng dữ liệu train và tets \n",
    "train_df = get_data_from_text_files('train')\n",
    "test_df = get_data_from_text_files('test')\n",
    "all_data = train_df.append(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparetion ( Chuẩn bị bộ dữ liệu cho Mô hình)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng hàm điều chỉnh tiêu chuẩn hóa dưc liệu \n",
    "# 1 : định dạnh đồng bộ from chữ \n",
    "# 2 : xử dụng biểu thức chính quy loại bỏ định dạng xuống dòng trong HTML \n",
    "# 3 : tiến hành loại bỏ và thay thế các ký tự đặc biệt \n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    # loại bỏ định dạng HTML \n",
    "    trippedhtml = tf.strings.regex_replace(lowercase , \"<br />\" ,\" \")\n",
    "    # Loại bỏ đi ký tự dặc biệt dùng escape để thoát khỏi ký tự đặc biệt trong \n",
    "    # biểu thức chính quy \n",
    "    return tf.strings.regex_replace (\n",
    "        trippedhtml , '[%s]' % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"),\"\"\n",
    "    )\n",
    "\n",
    "# Xây dựng hàm thực hiện vector hóa các tokens \n",
    "\n",
    "def get_vector_layer(texts , vocab_size , max_seq , special_tokens=['[MASK]']):\n",
    "    \"\"\"\"\n",
    "        Xây dựng vector hóa văn bản \n",
    "\n",
    "    Args:\n",
    "        text (list) : List of string i.e input texts \n",
    "        vocab_size (int) : Size of vocabulary\n",
    "        max_seq (int) : Maximum sequence length.\n",
    "        special_tokens (list , optional) : List of special tokens . Default to ['[MASK]].\n",
    "\n",
    "    Return :\n",
    "     layers.Layer : Return TextVectorization Keras Layer\n",
    "\n",
    "    \"\"\"\n",
    "    # Khởi tạo hàm TextVectorization từ keras\n",
    "    vectorization = keras.layers.TextVectorization(\n",
    "        # max_tokens  \\\\ vocab_size  , output frome = int \n",
    "        max_tokens=vocab_size , \n",
    "        output_mode= 'int',\n",
    "        standardize= custom_standardization , \n",
    "        output_sequence_length=max_seq, \n",
    "    )\n",
    "    # sử dụng hàm adapt để biến đổi phù hợp đầu vào với vectorization\n",
    "    vectorization.adapt(texts)\n",
    "\n",
    "    # Xây dựng 1 từ điển bằng cách sử dụng hàm get_vocabulary () và nhận về tần xuất \n",
    "    # xuất hiện của các tokens giảm dần \n",
    "    vocab = vectorization.get_vocabulary()\n",
    "    # Tiến hành xây dựng bộ từ vựng bằng cachs bỏ đi 2 phần tử đầu thường là các tokens\n",
    "    # cls , esp và thay thế các tokens đực biệt = ['mask']\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + ['[mask]']\n",
    "    # cuối cung cập nhật lại bộ từ vựng và trả về nó \n",
    "    vectorization.set_vocabulary(vocab)\n",
    "    return vectorization \n",
    "\n",
    "vectorize_layer = get_vector_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Nhận mặt nạ id mã thông báo cho mô hình mặt nạ ngôn ngữ\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "\n",
    "# xây dựng hàm mã hóa dữ liệu \n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts\n",
    "\n",
    "# Xây dựng hàm thực hiện che đi 15 % number of tokens và gán nhãn cho chúng \n",
    "def get_mask_input_and_labels(encoded_texts):\n",
    "    # che đi 15 % số tokens \n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15 \n",
    "    # Bỏ che những tokens đặc biệt \n",
    "    inp_mask[encoded_texts <= 2] = False \n",
    "    # Thiết lập bộ nhãn tương ứng = -1\n",
    "    labels = -1 * np.ones(encoded_texts.shape , dtype=int) \n",
    "    # Gán nhãn cho mặt nạ mã thông báo \n",
    "    # tất cả các tokens bị tre đi = -1 và còn lại được set bởi chỉ số tokens của encoded_texts\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Chuẩn bị một bộ dữ liệu sao chép nguyên bản  \n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Thay thế 90 % các token bị che đi thành tokens mask và giữ lại 10 % \n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) <  0.9)\n",
    "    # sau đó thay thế các token bị chee mặt nạ từ danh sách encoded_texts_mask\n",
    "    # bằng Tokens Mask \n",
    "    encoded_texts_masked [inp_mask_2mask] = mask_token_id \n",
    "\n",
    "    # đặt ra 10 % số tokens còn lại của inp_mask_2mask là các tokens bị che đi không bị thay thế \n",
    "    # bởi Tokens Mask thành các tokens ngẫu nhiên khác trừ nhũng token đặc biệt \n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1/9 )\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3 , mask_token_id , inp_mask_2random.sum()\n",
    "    ) # cuối cùng encoded_texts_mask sẽ chứa 10% tokens ngẫu nhiên và 90% token bị che đi \n",
    "\n",
    "    # Chuẩn bị sample_weights chuyển đến phương thức phù hợp \n",
    "    # khởi tạo samples = shape(labels) with full 1\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    # che đi các nhãn có giá trị = -1 thay thế = 0 nó thường là các tokens_mask , padded_tokens..\n",
    "    # để bỏ qua trong quá trình tính toán và giảm thiểu chi phí tính toán \n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # Khởi tạo bộ nhãn y_labels bằng chính dữ liệu nguyên bản . \n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    # trả về encoded_texts_masked , y_labels , sample_weight \n",
    "    return encoded_texts_masked , y_labels , sample_weights\n",
    "\n",
    "\n",
    "\n",
    "# Xây dựng các tầng dữ liệu cho các lớp mô hình \n",
    "\n",
    "# bộ 25000 mẫu cho huấn luyện mô hình\n",
    "# phải chuyển đổi x thành các tokens_ids (int) và lấy ra giá trị của cột review  \n",
    "x_train = encode(train_df.review.values)  # encoded review with vectorizer \n",
    "y_train = train_df.sentiment.values \n",
    "\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train , y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# Và một bộ tương tự với 25 0000 dữ liệu thử nghiệm \n",
    "# thực hiện tương tự như bộ dữ liệu huân luyện \n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values \n",
    "\n",
    "test_classifier_df = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ").batch(config.BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Prepare data for masked language model \n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train , y_masked_labels , sample_weights = get_mask_input_and_labels(\n",
    "    x_all_review\n",
    ")\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bert model (Pretraining Model) ỏ masked language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.905882352941177\n"
     ]
    }
   ],
   "source": [
    "# xây dựng bert module \n",
    "def bert_module(q , k , v ,  i):\n",
    "    # Add multihead Attention \n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEADS, \n",
    "        key_dim=config.EMBED_DIM , \n",
    "        name='encoder_{}multiheadattrntion'.format(i),\n",
    "    )(q , k , v)\n",
    "\n",
    "    # add dropout for attention output \n",
    "    attention_output = layers.Dropout(\n",
    "        0.1 ,\n",
    "        name ='encoder_{}/attn_dropout'.format(i)\n",
    "    )(attention_output)  \n",
    "    # add layers Normalization \n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6,name='encoder_{}/att_layernormalization'.format(i)\n",
    "    )(q + attention_output)\n",
    "\n",
    "\n",
    "    # Implement Feedforward netwwordk \n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(units=config.FF_DIM,activation=\"relu\"), \n",
    "            layers.Dense(config.EMBED_DIM)\n",
    "        ], \n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    # add ffn layers \n",
    "    ffn_output = ffn(attention_output)\n",
    "    # add ffn dropout\n",
    "    ffn_output = layers.Dropout(rate=0.1,name='encoder_{}/ffn_dropout'.format(i))(ffn_output)\n",
    "    # add output layers = atention_ouput + ffn_output \n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "# Xây dụng hàm tạo ma trận positional encoding \n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            # theo công thức pos / 10000^2 *j // d / emb\n",
    "            # công thức này là công thức tính toán vị trí trong transformer gốc \n",
    "            [pos / np.power(10000, 2 *(j // 2) /d_emb) for j in range(d_emb)    ]\n",
    "            if pos != 0 \n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    # T thay thế các giá trị ở chiều chằn và lẻ \n",
    "    # trường hợp chẵn ta sử dụng hàm sin\n",
    "    pos_enc[1:, 0 :: 2] = np.sin(pos_enc[1:, 0:: 2])\n",
    "    # Trường hợp lẻ ta sử dụng cos function \n",
    "    pos_enc[1:, 1:: 2] = np.cos(pos_enc[1:, 1:: 2])\n",
    "    # Cuối cùng trả về vị trí cho các tokens \n",
    "\n",
    "# xây dựng hàm loss và phương thức loss tracker để cập nhật số liệu \n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3 :\n",
    "            features , labels , sample_weight = inputs \n",
    "        else : \n",
    "            features , labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Thực hiện dự đoán \n",
    "            predictions = self(features , training=True)\n",
    "            # Tính toán chi phí \n",
    "            loss = loss_fn(labels , predictions , sample_weight=sample_weight)\n",
    "        \n",
    "        # Tính toán gradient \n",
    "        gradients = tape.gradient(loss , self.trainable_variables) \n",
    "\n",
    "        # cập nhật trọng số bằng trình tối ưu hóa\n",
    "        self.optimizer.apply_gradients(zip(gradients , self.trainable_variables))\n",
    "        #Tính toán và cập nhật số liệu \n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # TRả lại tên chỉ số ánh xạ chính tả và chỉ số hiện tại \n",
    "        return {'loss': loss_tracker.result()}\n",
    "    @property \n",
    "    def metrics(self):\n",
    "        return [loss_tracker]\n",
    "    \n",
    "\n",
    "# Khởi tạo mô hình bert mặt nạ ngôn ngữ mlm\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embedding = layers.Embedding(\n",
    "        config.VOCAB_SIZE , config.EMBED_DIM, name='word_embedding'\n",
    "    )(inputs)\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN, \n",
    "        output_dim=config.EMBED_DIM, \n",
    "        weights=[get_pos_encoding_matrix[config.MAX_LEN, config.EMBED_DIM]],\n",
    "        name='position_embedding',\n",
    "    )(tf.range(start=0 ,limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embedding + position_embedding\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "# tạo ra 2 bộ từ điển mới và đặt cho bộ từ điển nàylà 1 iterator có thể lặp \n",
    "# và tần xuất các từ được xuất hiện giảm dần \n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y : x for x  , y in id2token.items()}\n",
    "\n",
    "# Xây dựng lớp maskedGenerator với mục đích để tạo ra các dự đoán cho các tokens \n",
    "# bị ẩn trong văn bản \n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k =5):\n",
    "        # sample_tokens là một mảng các tokens được mã hóa bởi vectorize_layers \n",
    "        self.sample_tokens = sample_tokens\n",
    "        # Top k là tham số đại diện cho số lượng tokens có xác xuất cao nhất \n",
    "        # được chọn để dự đoán cho token bị che đi \n",
    "        self.k = top_k\n",
    "        \n",
    "    # Xây dựng phương thưc mã hóa (phương thức mã hóa ngược )\n",
    "    def decode(self, tokens):\n",
    "        # tạo ra 1 danh sách các token tương ứng với các mã số trong mảng tokens\n",
    "        # bỏ qua các mã số = 0 || sử dụng tập từ điển xây dựng trước để ánh xạ \n",
    "        # các mã số tokens_id (int) sang các token \n",
    "        # sử dụng join để nối các token trong dnah sách thành văn bản \n",
    "        return \" \". join([id2token[t] for t in tokens if t != 0])\n",
    "    \n",
    "    # Xây dựng phương thức chuyển đổi một mã số thành các token tương ứng \n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        # Tham số id là chỉ số index của token trong bộ từ điển \n",
    "        # và trả về 1 token tương ứng với vị trí idex trong từ điển \n",
    "        return id2token(id)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # dự đoán tokens thay thế cho câu tar về 1 xác xuất soft max \n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        # Tìm vị trí của tokens bị che đi trong mảng sample_tokens sử dụng hàm where để\n",
    "        # tìm kiếm chỉ số tokens tương ứng của mask trong ma trận masked_tokens_id\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        # sau đó lấy ra chỉ số cột của của tokens bị ẩn trong mảng sample_tokens \n",
    "        masked_index = masked_index[1]\n",
    "\n",
    "        # lấy ra ma trận có xác xuất cao nhất của mỗi tokens bị che giấu \n",
    "        # shape = [1, vocabulary] mỗi phần tử trong ma trận là xác xuất của 1 token \n",
    "        # có thể được điền vaò vị trí bị che giấu \n",
    "        masked_prediction  = prediction[0][masked_index]\n",
    "        # Tiếp theo sắp xếp các tokens theo thứ tự giảm dần của xác xuất , và lấy ra \n",
    "        # tokens có xác xuất cao nhất top.k . Kết quả sẽ được lưu vào hai biến: \n",
    "        # top_indices và values. Biến top_indices là một mảng chứa các mã số của các token được chọn,\n",
    "        # biến values là một mảng chứa các xác suất tương ứng.        \n",
    "        top_indices = masked_prediction[0].argsort()[-self.k :][:: -1]\n",
    "        values = masked_prediction[0][top_indices]\n",
    "\n",
    "\n",
    "        for i in range (len(top_indices)): \n",
    "            p = top_indices[i] # lấy ra mã số của token i \n",
    "            v = values[i] # lấy ra xác xuất của token i 3\n",
    "            tokens = np.copy(sample_tokens[0]) \n",
    "            # p là mã số theo xác xuất cao nhất của tokens có nghĩa P là biến có khả năng là \n",
    "            # token bị che đi \n",
    "            #  Đồng thời cũng cho biết vị trí của P trong ma trận chứa tokens bị che đi \n",
    "            tokens[masked_index[0]] = p \n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune a sentiment classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained bert model \n",
    "mlm_model = keras.models.load_model(\n",
    "    'bert_mlm_imdb.h5', custom_objects={'MaskedLanguageModel': MaskedLanguageModel}\n",
    ")\n",
    "\n",
    "pretrained_bert_model = tf.keras.Model(\n",
    "    mlm_model.input , mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    ")\n",
    "# Freeze it\n",
    "pretrained_bert_model.trainable = False\n",
    "\n",
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64 , activation='relu')(pooled_output)\n",
    "    outputs = layers.Dense(1 , activation='sigmoid')(hidden_layer)\n",
    "    classifier_model = keras.Model(inputs , outputs=outputs ,name='classification')\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifier_model.compile(\n",
    "        optimizer=optimizer , loss='binary_crossentropy' , metrics=['accuracy']\n",
    "    )\n",
    "    return classifier_model\n",
    "\n",
    "classifier_model = create_classifier_bert_model()\n",
    "classifier_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier with fronze bert stage \n",
    "classifier_model.fit(\n",
    "    train_classifier_ds, \n",
    "    epochs = 5, \n",
    "    validation_batch_size=test_classifier_df\n",
    ")\n",
    "\n",
    "# Bỏ đóng băng BERT cho tinh chỉnh mô hình \n",
    "pretrained_bert_model.trainable = True \n",
    "classifier_model.compile(\n",
    "    train_classifier_ds , \n",
    "    epochs = 5 , \n",
    "    validation_data = test_classifier_df, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an end-to-end model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_end_to_end(model):\n",
    "    inputs_string = layers.Input(shape=(1,), stype='string')\n",
    "    # Thực hiện Bert Tokenizer \n",
    "    indices = vectorize_layer(inputs_string)\n",
    "    outputs = model(indices)\n",
    "    # End-to-end model \n",
    "    end_to_end_model = keras.Model(inputs=inputs_string, outputs = outputs,\n",
    "                    name='end_to_end_model')\n",
    "    \n",
    "    # optimizers \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer , loss= 'binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifier_model)\n",
    "end_to_end_classification_model.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
