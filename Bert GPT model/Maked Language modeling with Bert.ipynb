{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from keras import layers \n",
    "from dataclasses import dataclass # có chức năng tạo các lớp dữ liệu data \n",
    "import numpy as np \n",
    "import  pandas as pd \n",
    "import glob \n",
    "import re \n",
    "from pprint import pprint #Hàm pprint trong python là một hàm\n",
    "# dùng để in các cấu trúc dữ liệu trong python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thiết lập cấu hình "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class  Config:\n",
    "    MAX_LEN = 256 \n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128 \n",
    "    NUM_HEAD = 8 # USE IN BERT MODEL \n",
    "    FF_DIM  = 128 # USE IN BERT MODEL \n",
    "    NUM_LAYERS = 1 \n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tải dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng hàm lấy danh sách các văn bản từ file \n",
    "def get_text_list_from_file(files):\n",
    "    # tạo một danh sách để chứa các text \n",
    "    text_list = []\n",
    "    # Duyệt qua mỗi phần tử trong danh sách \n",
    "    for name in files:\n",
    "        # sử dụng dòng lệnh with open(name) để mở các file và gán nó cho biến f \n",
    "        with open(name) as f :\n",
    "            # duyệt qua mỗi dòng văn bản thông qua các tệp file mở \n",
    "            for line in f:\n",
    "                # add các dòng vào danh sách \n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "# Xây dựng hàm đọc tệp dữ liệu từ các dòng văn bản trong thư mục \n",
    "# và trả về 1 bảng dữ liệu với 2 cột là review và sentiment\n",
    "def get_data_from_text_files(folder_name):\n",
    "    # sử dụng hàm glob để lấy danh sách các tệp văn bản trong thư mục con pos \n",
    "    # của thư mục folder_name và các tệp này có định dạng  aclImdb/folder_name/pos/*.txt\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    # Tiến hành đọc và lấy các dòng dữ liệu từ file \n",
    "    pos_texts = get_text_list_from_file(pos_files) \n",
    "    # thực hiện tương tự với định dạng thư mục aclImdb/folder_name/neg/*.txt\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_file(neg_files)\n",
    "\n",
    "    # Tiến hành xây dựng dataFrame với 2 cột review và sentiment \n",
    "    df = pd.DataFrame (\n",
    "        {\n",
    "            'review' : pos_texts + neg_files, # là số lượng phần tử trong cột \n",
    "            # với cột sentiment ta biến đổi dạng 0 và 1 \n",
    "            'sentiment': [0] * len(pos_texts) + [1] * len(neg_texts), \n",
    "            \n",
    "        }\n",
    "    )\n",
    "    # sử dụng hàm sample để lấy mẫu ngẫu nhiên với len (df) là độ dài của các cột \n",
    "    # kết quả là một dataFrame mới có cùng kính thước nhưng số lượng hàng nhưng \n",
    "    # số lượng hàng bị xáo trộn \n",
    "    # Sử dụng hàm reset_index để đặt lại ví trí ccho các hàng sau khi bị trộn \n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "\n",
    "# Xây dựng dữ liệu train vè test \n",
    "train_df = get_data_from_text_files('train')\n",
    "test_df = get_data_from_text_files('test')\n",
    "all_data = train_df.append(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dụng hàm tùy chỉnh tiêu chuẩn hóa dữ liệu \n",
    "# Có chức năng thay thế loại bỏ xuống ròng và biến đổi thường văn bản \n",
    "def custom_standardization(input_data):\n",
    "    # biến đổi dữ liệu thành chữ thường \n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    # xây dựng phương thức xử lý thay thế chính quy regex để thay thế \n",
    "    # biểu thức chính quy <br /> trong html thành các khoảng trắng \n",
    "    tripped_html = tf.strings.regex_replace(lowercase , \"<br />\" , \" \")\n",
    "    # xây dựng phương thức trả về bằng cách sử dụng biểu thức thay thế regex\n",
    "    # phương thức sẽ tìm kiếm các ký tự tromng danh sách và thay thế nó \n",
    "    # Sử dụng re.escape để thoát khỏi các kỹ tự có ý nghĩa đặc biệt trong biểu thức \n",
    "    # chính quy \n",
    "    return tf.strings.regex_replace(\n",
    "        tripped_html , \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"),\"\"\n",
    "    )\n",
    "\n",
    "# Xây dựng hàm Thực hiện vector hóa dữ liệu \n",
    "def get_vectorize_layer(text, vocab_size , max_seq , special_tokens=['[MASK]']):\n",
    "    \"\"\"Xây dựng Vector hóa văn bản \n",
    "\n",
    "    Args:\n",
    "        Text (list) : Danh sách của chuỗi i.e văn bản đầu vào . \n",
    "        Vocab (int) : Kích thước của tập từ điển . \n",
    "        Max seq length (int) : Độ dài mô hình nhận được .\n",
    "        Special_tokens (list , optional) : Danh sách các mã thông báo đặc biệt . Xác định [MASK]\n",
    "\n",
    "    Return \n",
    "        layers.Layer: Return TextVectorization keras.Layer\n",
    "    \"\"\"\n",
    "    # Thiết lập phương thức vector háo token int \n",
    "    vectorize_layer = keras.layers.TextVectorization(\n",
    "            # truyền vào số lượng token tối đa  = vocab_size \n",
    "            max_tokens=vocab_size, \n",
    "            # Dạng biến đổi = int for each token\n",
    "            output_mode='int',\n",
    "            # Tiêu chuẩn cho các tokens , loại bỏ đi cá ký tự đầu vào \n",
    "            standardize=custom_standardization,\n",
    "            # kích thước đầu ra \n",
    "            output_sequence_length=max_seq,\n",
    "    )\n",
    "    # biến đổi phù hợp dữ liệu text để đưa vào vector hóa\n",
    "    vectorize_layer.adapt(text)\n",
    "    \n",
    "    # Xử lý và trèn mặt ạn cho câu \n",
    "\n",
    "    # Xay dựng tập từ vựng với get_vocabulary () trả về 1 danh sách từ vựng \n",
    "    # Trong phương thức Tokenizer of text với tần suất xuất hiện giảm dần \n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    \n",
    "    # Thực hiện quy định cho bộ từ vựng \n",
    "    # Tiến hành xây dựng bộ từ vựng mới bằng cachs loại bỏ đi 2 phần từ đầu \n",
    "    # và loại bỏ các tokens đặc biệt sau đó sử dụng mặt nạ mask để bù vào số phần tử \n",
    "    # bị loại bỏ cho tập từ vựng nhằm giữ cho tính đa dạng tập từ vựng \n",
    "    vocab = vocab[2 : vocab - len(special_tokens)] + ['[MASK]'] \n",
    "    # Cập nhật lại bộ từ vụng \n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "\n",
    "\n",
    "# biến đổi trên tập dữ liệu \n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    text= all_data.review.values.tolist(), \n",
    "    vocab_size = config.VOCAB_SIZE, \n",
    "    max_len  = config.MAX_LEN, \n",
    "    special_tokens=['[mask]'],\n",
    ")\n",
    "\n",
    "# Nhận id mã thông báo mặt ạn cho mô hình ngôn ngữ \n",
    "# Biến đổi mặt nạ trong tập từ điển thành mảng numpy int như các tokens \n",
    "\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "\n",
    "# Xây dựng hàm thực hiện mã hóa dữ liệu \n",
    "# sử dụng hàm vectorize_layer được tạo truơc \n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    # sau khi thực hiện mã hóa tokens ta chuyển các vector dạng numpy \n",
    "    # để được các ma trận dngj int \n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "\n",
    "# Xây dựng lớp mặt nạ cho các token (int) \n",
    "# Lớp này sẽ thực hiện che đi 15 % số lượng token từ chuỗi token đầu vào \n",
    "def get_masked_input_labels(encoded_texts):\n",
    "    # khởi tạo mặt nạ đầu vào với kích thước = input \n",
    "    # che đi 15 % số phần tử \n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Không tre những tokens có giá trị nhỏ hơn 2 \n",
    "    # tức là những giá trị <= 2 bị bỏ qua \n",
    "    inp_mask[encoded_texts <= 2 ] = False \n",
    "    # tạo ma trận labels có shape [input] và input fully = -1 \n",
    "    # Trong quá trình huấn luyện các giá trị = -1 bị bỏ qua \n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "\n",
    "    # Ta đặt nhãn cho những tokens bị che đi bằng - 1 bằng cách gán \n",
    "    # giá trị của chúng trong encode_text vào vị trí tương ứng trong ma trận labels \n",
    "    # ma trận labels[inp_mask] có các giá trị = -1 \n",
    "    # và ma trận encoded_text[inp_mask]  có các gia strị khác trừ 1\n",
    "\n",
    "    # kết quả ma trận labels những phần tử bị che mặt nạ trong ma trận \n",
    "    # được gán giá trị tương tự với vị trí của nó trong labels[inp_mask] = -1 \n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "    \n",
    "    # Chuẩn bị dữ liệu \n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "\n",
    "    # Tập dữ liệu đầu vào là [mask] là các token cuối cùng cho 90 % số mã thông báo  \n",
    "    # Điều này có nghĩa là giữ nguyên 10 % \n",
    "    # và trong số này không tính những token đặc biệt \n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.9)\n",
    "    # Sau đó thay thế các token được che mặt nạ bằng token Mask \n",
    "    encoded_texts_masked [ inp_mask_2mask ] =  mask_token_id  \n",
    "\n",
    "    # Tạo ra bộ mặt nạ khác để chọn 10% từ số lượng 90% token mask \n",
    "    inp_mask_2random = inp_mask_2mask &  (np.random.rand(*encoded_texts.shape)< 1 / 9)\n",
    "    # Sau đó thay thế các tokens được chọn bởi mặt nạ inp_mask_2random = các tokens ngẫu nhiên\n",
    "    # khác trừ những token đặc biệt khác trừ các token đặc biệt \n",
    "    encoded_texts_masked[inp_mask_2mask] = np.random.randint(\n",
    "        3 , mask_token_id , inp_mask_2random.sum)\n",
    "    \n",
    "    # Chuẩn bị sample_weights để chuyển sang phương thức weights \n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    # Gán cho nhãn bằng -1 tức là các tokens đặc biệt và trọng số mâũ = 0 để\n",
    "    # qua trong quá trình huấn luyện \n",
    "    sample_weights[labels == - 1] = 0\n",
    "\n",
    "    # Xây dựng nhãn y cho bộ tokens mã hóa \n",
    "    # gán  = encoded_text \n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked , y_labels , sample_weights \n",
    "\n",
    "\n",
    "# Tách dữ liệu huấn luyện và dữ liệu đào tạo \n",
    "x_train = encode(train_df.review.values)\n",
    "y_train = train_df.sentiment.values \n",
    "\n",
    "# Từ x , y train t gộp nó thành một tensor data\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train , y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# 25000 mẫu dữ liệu cho thử nghiệm \n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values \n",
    "\n",
    "test_classifier_ds =  (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test , y_test))\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "\n",
    "# Xây dựng đầu vào từ đầu đến cuối cho mô hình ( sẽ được sử dụng ở cuối cùng ) \n",
    "# Xử dụng hàm tensor_slices để tạo một dataset từ các tensor \n",
    "test_raw_classifier_ds =  tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ")\n",
    "\n",
    "# Chuẩn bị dữ liệu cho mô hình mặt nạ ngôn ngữ \n",
    "all_review = encode(all_data.review.values)\n",
    "x_masked_train , y_masked_train , sample_weights = get_masked_input_labels(\n",
    "    all_review \n",
    ")\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train , y_masked_train, sample_weights )\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bert model (Pretraining Model)  for masked language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model(query , key, value, i):\n",
    "    # Multi head self-attention \n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD , \n",
    "        key_dim=config.NUM_HEAD, \n",
    "        key_dim= config.EMBED_DIM // config.NUM_HEAD, \n",
    "        name='encoder_{}/multiheadattention'.format(i), \n",
    "\n",
    "    )(query , key , value)\n",
    "    # Add and norm layer for multi head attention output\n",
    "    attention_output = layers.Dropout(0.1, name='encoder_{}/attn_dropout'.format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    # ADD layerNormalization for attention\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name='encoder_{}/att_layernormalization'.format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM , activation='relu'),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name ='encoder_{}/ffn'.format(i),\n",
    "    )\n",
    "    # ffn layer\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dense(0.1 , name='encoder_{}/ffn_dropout'.format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    # sequence output = attention_output + ffn_output\n",
    "    sequence_output = layers.LayerNormalization(epsilon=1e-6,\n",
    "                        name='encoder_{}/ffn_layernormalization'.format(i)\n",
    "        )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "# Xây dụng hàm tạo ma trận positional encoding \n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            # theo công thức pos / 10000^2 *j // d / emb\n",
    "            # công thức này là công thức tính toán vị trí trong transformer gốc \n",
    "            [pos / np.power(10000, 2 *(j // 2) /d_emb) for j in range(d_emb)    ]\n",
    "            if pos != 0 \n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    # T thay thế các giá trị ở chiều chằn và lẻ \n",
    "    # trường hợp chẵn ta sử dụng hàm sin\n",
    "    pos_enc[1:, 0 :: 2] = np.sin(pos_enc[1:, 0:: 2])\n",
    "    # Trường hợp lẻ ta sử dụng cos function \n",
    "    pos_enc[1:, 1:: 2] = np.cos(pos_enc[1:, 1:: 2])\n",
    "    # Cuối cùng trả về vị trí cho các tokens \n",
    "\n",
    "# xây dựng hàm loss và phương thức loss tracker để cập nhật số liệu \n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "\n",
    "# Xây dựng mô hình mặt nạ ngôn ngữ \n",
    "class MaskedLanguageModel(keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3 :\n",
    "            features, labels , sample_weight = inputs\n",
    "        else:\n",
    "            features , labels = inputs \n",
    "            sample_weight = None\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss , trainable_vars)\n",
    "\n",
    "        # update Weights \n",
    "        self.optimizer.apply_gradients(zip(gradients , trainable_vars))\n",
    "        # Tính toán và cập nhật số liệu \n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # TRả lại tên chỉ số ánh xạ chính tả và chỉ số hiện tại \n",
    "        return {'loss': loss_tracker.result()}\n",
    "    \n",
    "\n",
    "    @ property \n",
    "    def metrics(self):\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "#  Khởi tạo mô hình mặt nạ ngôn ngữ với bert \n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    # Create input layers shape = max_sequen_length data type = int64 \n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    # Create nhúng từ + position embedding \n",
    "    word_embedding = layers.Embedding(\n",
    "        input_dim = config.VOCAB_SIZE ,\n",
    "        output_dim = config.EMBED_DIM , name='word_embedding'\n",
    "    )\n",
    "    position_embedding = layers.Embedding (\n",
    "        config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM, \n",
    "        weights = [get_pos_encoding_matrix(max_len=config.MAX_LEN,d_emb=config.EMBED_DIM)], \n",
    "        name='position_embedding'\n",
    "    )(tf.range(start=0 , limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embedding +  position_embedding\n",
    "\n",
    "    encoder_output = embeddings \n",
    "\n",
    "    # ADD 8 layers bert modeule\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_model(encoder_output, encoder_output, encoder_output,i)\n",
    "\n",
    "    # Create mlm_output\n",
    "    mlm_output = layers.Dense(units=config.VOCAB_SIZE , name='mlm_cls',\n",
    "                 activation='softmax')(encoder_output)\n",
    "    \n",
    "    mlm_model = MaskedLanguageModel(inputs , mlm_output , name='masked_bert_model')\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model \n",
    "\n",
    "\n",
    "# tạo ra 2 bộ từ điển mới \n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y : x for x , y in id2token.items() }\n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens \n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self,tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t!= 0])\n",
    "    \n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "    \n",
    "    def on_epoch_end(self, epoch , logs=None):\n",
    "        # dự đoán các token có thể điền vào vị trí của token bị che đi \n",
    "        # Kết quả là một ma trận chứa xác xuất cho mỗi token có thể \n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "        # tìm ra chỉ số của token che dấu trong câu chỉ số này được lưu trong \n",
    "        # ma trận mask_token_id là các chỉ số cột một cột = 1 token\n",
    "        masked_idex = np.where(self.sample_tokens == mask_token_id)\n",
    "        # ta chỉ quan tâm đến chỉ số hàng nên ta để là chiều thứ 2 \n",
    "        masked_idex = masked_idex[1]\n",
    "        # lấy ra ma trận có xác xuất cao nhất cho token bị che giấu\n",
    "        # shape = [1, vocabulary] mỗi phần tử trong ma trận là xác xuất của 1 token \n",
    "        # có thể được điền vaò vị trí bị che giấu \n",
    "        masked_prediction = prediction[0][masked_idex]\n",
    "\n",
    "        # sắp xếp các chỉ số xác xuất tokens từ cao xuống thấp \n",
    "        top_indices = masked_prediction[0].argsort()[-self.k :][:: -1]\n",
    "        # lấy ra các giá trị tương ứng với xác xuất cao nhất.\n",
    "        values = masked_prediction[0][top_indices]\n",
    "\n",
    "        # Duyệt qua ma trận kết quả chứa xác xuất \n",
    "        for i in range(len(top_indices)):\n",
    "            # lấy ra từng xác xuất của token tương ứng\n",
    "            p = top_indices[i]\n",
    "            # giá trị tương ứng với xác xuất \n",
    "            v = values[i]\n",
    "            # tạo một bản sao của câu ban đầu \n",
    "            # tránh thay đổi giá trị của câu nguyên mẫu gốc khi thực hiện \n",
    "            # các thao tác \n",
    "            tokens = np.copy(self.sample_tokens[0])\n",
    "            # thay thế các token bị tre giấu bằng xác xuất dự đoán p \n",
    "            # t đặt chỉ số hàng để xác xuất có thể đặt vào vị trí token bị che đi \n",
    "            tokens[masked_idex[0]] =  p \n",
    "            # Thực hiện biến đổi kết quả \n",
    "            result = {\n",
    "                # biến đổi mẫu đầu vào thành ma trận số\n",
    "                \"input_text\" : self.decode(self.sample_tokens[0].numpy()),\n",
    "                \"prediction\" : self.decode(tokens),\n",
    "                # xác xuất của token dự đoán p được lấy từ giá trị v\n",
    "                \"probability\": v,\n",
    "                # chuyển đổi xác xuất p và v tương ứng vói nó thành chuỗi \n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p)\n",
    "\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer(['I have watched this [mask] , and it was awesome'])\n",
    "generator_callbacks = MaskedTextGenerator(sample_tokens.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_masked_model.fit(mlm_ds , epochs=5 ,callbacks=[generator_callbacks])\n",
    "bert_masked_model.save('bert_mlm_imdb.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune a sentiment classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained bert model \n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.h5\" , custom_objects= {'MaskedLanguageModel': MaskedLanguageModel}\n",
    ")\n",
    "pretrained_bert_model = tf.keras.Model(\n",
    "    mlm_model.input , mlm_model.get_layer('encoder_0/ffn_layernormalization').output\n",
    ")\n",
    "\n",
    "# Đóng băng mô hình \n",
    "pretrained_bert_model.trainable = False \n",
    "\n",
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "\n",
    "    hidden_layer= layers.Dense(units= 64, activation='relu')(pooled_output)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "    classifier_model = keras.Model(inputs=inputs , outputs=outputs, name='classification')\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    classifier_model.compile(optimizer=optimizer, loss='binary_crossentropy'\n",
    "                             , metrics=['accuracy'])\n",
    "    return classifier_model\n",
    "\n",
    "\n",
    "\n",
    "classifier_model = create_classifier_bert_model()\n",
    "classifier_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier with fronze bert stage\n",
    "classifier_model.fit(\n",
    "    train_classifier_ds ,\n",
    "    epochs=5, \n",
    "    validation_data=test_classifier_ds,\n",
    ")\n",
    "\n",
    "# Unfreeze the Bert model for fine-tuning \n",
    "pretrained_bert_model.trainable= True\n",
    "optimizer = keras.optimizers.Adam()\n",
    "classifier_model.compile(\n",
    "    optimizer=optimizer , loss='bonary_crossentropy', metrics=['accuracy']\n",
    ")\n",
    "classifier_model.fit(\n",
    "    train_classifier_ds , \n",
    "    epochs=5 , \n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create end-to-end model and evaluate it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_end_to_end(model):\n",
    "    inputs_string = layers.Input(shape=(1,), stype='string')\n",
    "    # thực hiện bert Tokenizer \n",
    "    indices = vectorize_layer(inputs_string)\n",
    "    outputs = model(indices)\n",
    "    # End-to-end model \n",
    "    end_to_end_model = keras.Model(inputs=inputs_string , outputs=outputs,\n",
    "                        name='end_to_end_model')\n",
    "    \n",
    "    # optimizers \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer , loss= 'binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifier_model)\n",
    "end_to_end_classification_model.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
