{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128 # Maximum Length of input sentences to the model \n",
    "batch_size =32 # Kích thước hàng loạt\n",
    "epochs  = 2 # số kỷ nguyên huấn luyện mô hình \n",
    "\n",
    "# Labels in our dataset | các bộ loại nhãn cho dưc liệu mô hình [nhãn mâu thuẫn câu [0] , nhãn kế thừa câu[1] , nhãn trung lập [2]]\n",
    "labels = [\"contradiction\" , \"entailment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
    "!tar -xvzf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are more than 500k samples in total: we will use 100k for this example. \n",
    "train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n",
    "# Lấy ra validation data \n",
    "valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\n",
    "# Lấy ra dữ liệu cho thử nghiệm mô hình \n",
    "test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n",
    "\n",
    "\n",
    "# Shape of the data in ra số lượng dữ liệu theo chiều hàng với mỗi 1 bộ =  1000 mẫu\n",
    "print(f\"Total train samples: {train_df.shape[0]}\") \n",
    "print(f\"Total validation samples: {valid_df.shape[0]}\")\n",
    "print(f\"Total test samples: {valid_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sentence1: {train_df.iloc[1 , 'sentence1']}\") \n",
    "print(f\"sentence2: {train_df.iloc[1 , 'sentence2']}\")\n",
    "print(f\"Similarity: {train_df.iloc[1 , 'similaryty']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some NaN entries(Những mục) in our train data, we will simply drop them\n",
    "print(\"Num of missing values\")\n",
    "print(train_df.isnull().sum()) # In ra số lượng dữ liệu nan trên 3 bộ dữ liệu\n",
    "train_df.dropna(axis=0 , inplace=True) # loại bỏ những dòng có dữ liệu NaN và thay thế nó "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Target DisTribution')\n",
    "print(train_df.similarity.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Target Distribution\")\n",
    "print(valid_df.similarity.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện làm sạch dữ liệu loại bỏ đi các  dấu [-] đồng thời trộn số mẫu đặt lại chỉ số idx sau khi \n",
    "# xáo trộn chật tự\n",
    "train_df = (\n",
    "    train_df[train_df.similarity != '-']\n",
    "    .sample(frac=1.0 , random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "valid_df = (\n",
    "    valid_df[valid_df.similarity != \"-\"]\n",
    "    .sample(frac=1.0, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập nhãn đào tạo \n",
    "# tạo một cột mới thêm nó vào data Frame cột này sẽ có giá trị = 0 , 1 , 2\n",
    "train_df['labels'] = train_df['similarity'].apply(\n",
    "    lambda x : 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
    "\n",
    ")\n",
    "# Từ cột giá trị labels ta tách nó thành 3 cột riêng biệt \n",
    "y_train = tf.keras.utils.to_categorical(train_df.labels , num_classes=3)\n",
    "\n",
    "valid_df ['labels'] = valid_df['similarity'].apply(\n",
    "    lambda x : 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
    ")\n",
    "y_val = tf.keras.utils.to_categorical(valid_df.labels , num_classes=3)\n",
    "\n",
    "test_df['labels'] = test_df['similarity'].apply(\n",
    "    lambda x : 0  if x == 'contradiction' else 1 if x == 'entailment' else 2\n",
    ")\n",
    "y_test = tf.keras.utils.to_categorical(test_df.labels , num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data. [Tạo ra các lô của dữ liệu]\n",
    "    \n",
    "    Args:\n",
    "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size. \n",
    "        shuffle: boolean, whether to shuffle the data. \n",
    "        include_targets: boolean, whether to include the labels .\n",
    "\n",
    "    Returns: \n",
    "        Tuples '([input_ids , attention_mask, 'token_type_ids], labels)'\n",
    "        (or just '[input_ids , attention_mask, 'token_type_ids]'if 'include_targets =False)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        sentence_pairs,\n",
    "        labels, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        include_targets=True\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels \n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "\n",
    "        # Load the Bert Tokenizer to encode the text. \n",
    "        # We will use base-base-uncased pretrained Model.\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\" , do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # biểu thi số lượng lô trên mỗi kỷ nguyên \n",
    "        return len(self.sentence_pairs) // self.batch_size \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Truy xuất chỉ mục \n",
    "        # Tính chỉ số của các cặp câu trong lô nhân idx vơí batch_size\n",
    "        # và lấy 1 đoạn con từ danh sách các chỉ số self.indexes\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        # trả về một danh sách các cặp câu từ các chỉ số đã tính toán tương ứng \n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "        \n",
    "        # With Bert tokenizer's batch_encode_plus batch of both the sentences are \n",
    "        # encoded togerther and separated (tách ly)  by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True ,\n",
    "            max_length=max_length , return_attention_mask=True, \n",
    "            return_token_type_ids =True, \n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='tf',\n",
    "        )\n",
    "\n",
    "        #Chuyển đổi các lô của các đặc trưng được mã hóa thành mảng numpy \n",
    "        input_ids = np.array(encoded['input_ids'], dtype='int32')\n",
    "        attention_mask = np.array(encoded['attention_mask'], dtype='int32')\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # Đặt thành true nếu dữ liệu trình đào tạo dữ liệu được sử dụng cho huấn luyện và thẩm định \n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype='int32')\n",
    "            return [input_ids , attention_mask , token_type_ids] , labels \n",
    "\n",
    "        else:\n",
    "            return [input_ids, attention_mask, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        # Xáo trộn các chỉ mục \n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo mô hình theo phạm vi chiến lược phân phối \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Với sự tồn tại của phạm vi chiến lược \n",
    "with strategy.scope():\n",
    "    # Mã hóa token_ids từ Bert tokenizer \n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,) , dtype=tf.int32 , name='input_ids'\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attentioned to.\n",
    "    attention_mask = tf.keras.layers.Input(\n",
    "        shape=(max_length,) , dtype=tf.int32 , name='attention_mask'\n",
    "    )\n",
    "    # Token_type_ids are binary mask indentifying(nhận dạng) difderent sequences in the moel. \n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,) , dtype=tf.int32 , name='token_type_ids'\n",
    "    )\n",
    "\n",
    "    # Loading pretrained Bert model. \n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\n",
    "        \"bert-base-uncased\"\n",
    "    )\n",
    "    # Freeze the Bert model te reuse the pretrained features without modifying them. \n",
    "    bert_model.trainable = False \n",
    "\n",
    "    bert_output = bert_model.bert(\n",
    "        input_ids , attention_mask=attention_mask , token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state \n",
    "    pooled_output = bert_output.pooler_output \n",
    "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the \n",
    "    # new data \n",
    "    bi_lstm = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64 , return_sequences=True)\n",
    "    )(sequence_output)\n",
    "\n",
    "    # Applying hyprid pooling approach to bi_lstm sequence output \n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
    "\n",
    "    concat = tf.keras.layers.concatenate([avg_pool , max_pool])\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    output = tf.keras.layers.Dense(3 , activation='softmax')(dropout)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_ids , attention_mask , token_type_ids] , outputs = output\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        metric = ['acc'],\n",
    "    )\n",
    "\n",
    "print(f\"Strategy: {strategy}\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BertSemanticDataGenerator(\n",
    "    train_df[['sentence1' ,'sentence2']].values.astype('str'),\n",
    "    y_train, \n",
    "    batch_size=batch_size, \n",
    "    shuffle =True,\n",
    ")\n",
    "\n",
    "valid_data = BertSemanticDataGenerator(\n",
    "    valid_df[['sentence1' , 'sentence2']].values.astype(\"str\"),\n",
    "    y_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model Similarity estimate Bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng phương thức phù hợp cho Mô hình \n",
    "history = model.fit(\n",
    "    # train , anh validation data\n",
    "    train_data, \n",
    "    validation_data = valid_data,\n",
    "    epochs = epochs , \n",
    "    # Ta sử dụng phương thức xử lý đa chiều \n",
    "    use_multiprocessing=True , \n",
    "    workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning and optimizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bỏ đóng băng bert để nó có thể được cập nhật số liệu \n",
    "bert_model.trainable = True \n",
    "# Biên dịch lại mô hình bằng việc làm thay đổi hiệu quả \n",
    "model.compile (\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    # sử dụng hàm loss crossentropy \n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "# xem tổng quát hóa mô hình \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the entire model end-to-end || Đào tạo toàn bộ mô hình từ đầu đến cuối "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data, \n",
    "    epochs = epochs, \n",
    "    use_multiprocessing=True, \n",
    "    workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = BertSemanticDataGenerator(\n",
    "    test_df[['sentence1', 'sentence2']].values.astype('str'),\n",
    "    y_test , \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    ")\n",
    "model.evaluate(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on custom sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(sentence1 , sentence2):\n",
    "    sentence_pairs = np.array([[str(sentence1) , str(sentence2)]])\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs=sentence_pairs, labels=None , batch_size=1 ,\n",
    "        shuffle=False , include_targets=False\n",
    "    )\n",
    "    proba = model.predict(test_data[0])[0]\n",
    "    idx = np.argmax(proba)\n",
    "    proba = f\"{proba[idx]: .2f}%\"\n",
    "    pred = labels[idx]\n",
    "    return pred, proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Two women are observing something together.\"\n",
    "sentence2 = \"Two women are standing with their eyes closed.\"\n",
    "check_similarity(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"A soccer game with multiple males playing\"\n",
    "sentence2 = \"Some men are playing a sport\"\n",
    "check_similarity(sentence1, sentence2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
