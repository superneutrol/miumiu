{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import json \n",
    "import string \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from keras import layers \n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer , TFBertModel , BertConfig \n",
    "\n",
    "max_len = 384 \n",
    "configuration = BertConfig() # default parameters and configuration for Bert \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up Bert tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the slow pretrained tokenizer \n",
    "slow_tokenizer = BertTokenizer.from_pretrained('bert-base-unscased')\n",
    "save_path = 'bert_base_uncased/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "slow_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Load the fast tokenizer from saved file \n",
    "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json'\n",
    "train_path = keras.utils.get_file('train.json', train_data_url)\n",
    "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "eval_path = keras.utils.get_file('eval.json', eval_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample:\n",
    "    def __init__(self, question ,context, start_char_idx, answer_text, all_answer):\n",
    "        self.question = question \n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answer = all_answer\n",
    "        self.skip = False \n",
    "    def preprocess(self):\n",
    "        context = self.context \n",
    "        question = self.question\n",
    "        answer_text = self.answer_text\n",
    "        start_char_idx = self.start_char_idx\n",
    "\n",
    "        # Clean context, answer and question \n",
    "        context = \" \".join(str(context).split())\n",
    "        question = \" \".join(str(question).split())\n",
    "        answer = \" \".join(str(answer_text).split())\n",
    "    \n",
    "    # Sẽ cần thực hiện các chức năng sau \n",
    "    # Tìm kiếm các chỉ số ký tự bắt đầu và kết thuc câu \n",
    "    # Và tạo ra các đầu vào cho mô hình \n",
    "\n",
    "        # 1 : Xây dựng phương thức tiềm kiếm vị trí ký tự cuối cùng trong câu trả lời\n",
    "        # bằng cách lấy chỉ số ký tự đầu của câu trả lời + độ dài của toàn bộ xâu trả lời \n",
    "        # vd start = 17 và độ dài của 1 câu trả lời đó  = 7 \n",
    "        #  24 - 1 \n",
    "        end_char_idx = start_char_idx + len(answer)\n",
    "        # kiểm tra xem chỉ số ký tự kết thúc có vượt quá độ dài ngữ cảnh \n",
    "        # Nếu có thì bỏ qua và thoát khỏi vòng lặp \n",
    "        if end_char_idx >= len(context):\n",
    "            self.skip = True \n",
    "            return \n",
    "        \n",
    "        # 2 : Đánh giấu các chỉ mục ký tự trong ngữ cảnh thuộc về câu trả lời \n",
    "        # và xác định các ký tự nào trong ngữ cảnh là thuộc cho câu tl \n",
    "        # và đánh dấu chúng = cách gán =  1 \n",
    "        # Đầu tiên nhân tất cả giá trị ngữ cảnh của câu = 0 \n",
    "        # Tức là sé sinh ra  1 vector có độ dài bằng với câu đấy vơis values = 0\n",
    "        is_char_in_ans = [0] * len(context)\n",
    "        # lặp qua câu thông qua ký tự đầu tiên và kết thúc ở ký tự tự cuối \n",
    "        # và gán cho các ký tự thuộc ctl = 1\n",
    "        for idx in range(start_char_idx , end_char_idx):\n",
    "            is_char_in_ans[idx] = 1 \n",
    "\n",
    "        # 3 : Thực hiện token hóa ngữ cảnh \n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "\n",
    "        # 4 :tạo ra một danh sách các chỉ số của token mà thuộc về câu trả lời\n",
    "        ans_token_idx = [] # nhận chỉ số vị trí của câu trả lời là vị trí token theo từ \n",
    "        # vd Việt Nam có vị trí 7 , 8 thì nó sẽ là [7,8]\n",
    "        # Duyệt qua các token trong câu và lấy ra vị trí đâù và kết thúc cho mỗi token\n",
    "        # từ offsets sẽ trả về vị trí bắt đầu và kết thúc cho mỗi token vd Viet (17,20) nam (21 ,23)\n",
    "        # giúp cho việc đếm số thứ tự các phần tử trong danh sách vd (7,20) idx = 7 sẽ add 7 ,\n",
    "        # (21 ,23)  = 8 add 8 => =[7 ,8]\n",
    "        for idx , (start, end) in enumerate(tokenized_context.offsets):\n",
    "            # nếu có chứa ít nhất một ký tự thuộc câu trả lời thì ta thêm chỉ số của token \n",
    "            if sum(is_char_in_ans[start:end] >0):\n",
    "                ans_token_idx.append(idx) # IDX là chỉ số cho vị trí token\n",
    "        \n",
    "        # Kiểm tra xem danh sách có rỗng hay không \n",
    "        # Rỗng nghĩa là không token nào được tạo da từ ký tự của câu trả lời \n",
    "        if len(ans_token_idx) == 0:\n",
    "            self.skip = True \n",
    "            return \n",
    "        \n",
    "        # 5: Lấy ra vị trí bắt đầu và kết thúc cho toàn bộ câu trả lời \n",
    "        # Từ chỉ số đầu và cuối được lưu trữ từ vị trí token cho câu trả lời \n",
    "        start_token_idx = ans_token_idx[0]\n",
    "        end_token_idx = ans_token_idx[-1]\n",
    "\n",
    "        # 6 : Tính toán token hóa câu hỏi \n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "\n",
    "        # 7: xây dựng lớp đầu vào cho mô hình bert lưu ý loại bỏ vị trí 0 là cls của question \n",
    "        # inputs =  tokenized_context.ids  + tokenized_question.ids[1:]\n",
    "        # Sau đó thực hiện biến đổi lấp đầy 2 đoạn bơỉ 0 và 1\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
    "            tokenized_question.ids[1:]\n",
    "        )\n",
    "        # 8 :khởi tạo mặt attention shape = inputs shape fully = 1\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # 9 : Dệm và tạo mặt nạ chú ý \n",
    "        # bỏ qua nếu cần cắt bớt \n",
    "        # Tính toán độ dài phần đệm = độ dài tối đa cho phép (maxlen) - độ dài câu (input_ids)\n",
    "        padding_length =  max_len - len(input_ids)\n",
    "        # Nếu như padding_length > 0 thì ta cần thực hiện đệm và mã hóa cho đệm =0\n",
    "        if padding_length > 0: # pad\n",
    "            # thêm đệm vào input_ids và tham số của đệm = 0\n",
    "            input_ids = input_ids + ( [0] * padding_length)\n",
    "            # tương tự với mặt nạ chú ý và token_type \n",
    "            attention_mask = attention_mask + ( [0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ( [0] * padding_length)\n",
    "        # Nếu độ dài câu = max len thì bỏ qua ko \n",
    "        elif padding_length < 0:\n",
    "            self.skip = True \n",
    "            return \n",
    "        \n",
    "        # 10 : trả về loạt các danh sách \n",
    "        # đầu vào \n",
    "        self.input_ids = input_ids # shape = interger parameter + padding 0\n",
    "        self.token_type_ids = token_type_ids # shape = 0 , 1 + padding 0\n",
    "        self.attention_mask= attention_mask # hape =  1 + padding 0\n",
    "        self.start_token_idx = start_token_idx # là vị trí bắt đầu của câu tl theo vị trí các token\n",
    "        self.end_token_idx = end_token_idx # là vị trí kết thúc của câu tl theo vị trí các token trong câu \n",
    "        # Cuối cùng là trả về vị trí bát đầu và kết thúc của token được giới hạn bởi \n",
    "        # số ký tự có trong token vd viet = (17 , số ký tự  = 4) - 1 = (17,20)\n",
    "        self.context_token_to_char = tokenized_context.offsets \n",
    "\n",
    "\n",
    "# đọc dữ liệu huấn luyện và đọc dữ liệu thẩm đinh \n",
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "\n",
    "# Khởi tạo đội hình ví dụ dựa vào data train \n",
    "# gồm question , answer , all_answer , start_char_idx \n",
    "# sau đó xử lý các vector \n",
    "\n",
    "def create_squad_examples(raw_data):\n",
    "    squad_examples = []\n",
    "    # Duyệt qua các phần tử trong raw_data['data] mỗi phần tử sẽ tương tự với 1 bài \n",
    "    # viết trong wiki và chứa nhiều đoạn văn bản \n",
    "    for item in raw_data['data']:\n",
    "        # Duyệt qua các đoạn đoạn văn bản trong item[\"paragraphs\"]\n",
    "        for para in item['paragraphs']:\n",
    "            # lấy ra các đoạn văn bản mỗi đoạn chứa 1 chuỗi context \n",
    "            context = para['context']\n",
    "            # Duyêt qua một danh sách qa gồm các câu hỏi và câu tl \n",
    "            for qa in para['qas']:\n",
    "                # Lấy ra Chuỗi câu hỏi được đặt ra dựa trên context.\n",
    "                question = qa['question']\n",
    "                # Chuỗi câu trả lời đúng cho câu hỏi, được lấy ra từ context.\n",
    "                answer_text = qa['answers'][0]['text'] \n",
    "                # Lấy ra danh sách các chuỗi câu trả lời đúng cho câu hỏi \n",
    "                # có thể có nhiều cách trả lời khác nhau cho một câu hỏi \n",
    "                all_answers = [ om['text']  for om in qa['answers']]\n",
    "                # lấy ra vị chí của ký tự bắt đầu cho câu trả lời \n",
    "                start_char_idx = qa['answers'][0]['answer_start']\n",
    "\n",
    "                # Thực hiện tiền xử lý dữ liệu thô \n",
    "                squad_eg = SquadExample(\n",
    "                    question , context, start_char_idx , answer_text , all_answers\n",
    "                )\n",
    "                squad_eg.preprocess()\n",
    "                squad_examples.append( squad_eg)\n",
    "\n",
    "    return squad_examples\n",
    "\n",
    "# Xây dựng bộ phần xử lý đầu vào từ danh sách squad_examples \n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        'inputs_ids' : []\n",
    "        ,'token_type_ids':[],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\" :[],\n",
    "        \"end_token_idx\" : [],\n",
    "    }\n",
    "    # Duyệt qua đối tượng squad_example để lấy ra các item chứa trong nó \n",
    "    for item in squad_examples:\n",
    "        # nếu các item không được bỏ qua thì duyệt qua các key của từ điển tạo sẵn \n",
    "        if item.skip == False :\n",
    "            for key in dataset_dict:\n",
    "                # Thêm các item của squad _example vào với từ điển chứa các từ khóa \n",
    "                # tương ứng  , mỗi từ điển sẽ chứa 1 danh sách tương ứng với nó \n",
    "                # Sử dụng hàm getattr để lấy giá trị của thuộc tính đối tượng \n",
    "                # áp dụng cho từ key một \n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    \n",
    "    # duyệt qua các key trong từ điển và biến đổi nó thành mảng aray \n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    \n",
    "    # tạo x và y để phục vụ cho huấn luyện mô hình \n",
    "    x = [\n",
    "        dataset_dict[\"inputs_ids\"],\n",
    "        dataset_dict['token_type_ids'],\n",
    "        dataset_dict['attention_mask'],\n",
    "    ]\n",
    "        \n",
    "    y = [\n",
    "        dataset_dict['start_token_idx'],\n",
    "        dataset_dict [' end_token_idx'],\n",
    "    ]\n",
    "    return x , y\n",
    "\n",
    "\n",
    "# Tiến hành sử lý hóa dữ liệu từ train và eval cho mô hình \n",
    "train_squad_examples = create_squad_examples(raw_train_data)\n",
    "x_train , y_train = create_inputs_targets(train_squad_examples)\n",
    "\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
    "x_eval , y_eval=create_inputs_targets(eval_squad_examples)\n",
    "\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Question-Answering Model using BERT and Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Bert encoder\n",
    "    encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    # QA model\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = layers.Input(shape=(max_len,), stype=tf.int32)\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    embedding = encoder(\n",
    "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )[0]\n",
    "\n",
    "    start_logist = layers.Dense(units=1 , name='start_logit', use_bias=False)(embedding)\n",
    "    start_logist = layers.Flatten()(start_logist)\n",
    "\n",
    "    end_logits = layers.Dense(units=1 , name='end_logit', use_bias=False)(embedding)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logist)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask], outputs=[start_probs, end_probs]\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = tf.optimizers.Adam(lr=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_tpu = True \n",
    "if use_tpu:\n",
    "    # create distribution stratery \n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "\n",
    "    # Create model\n",
    "    with strategy.scope():\n",
    "        model = create_model()\n",
    "else:\n",
    "    model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create evaluation Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuations xóa dấu câu\n",
    "    exclude = set(string.punctuation)\n",
    "    # sau đó nối lại các ký tự trong văn bản và không thuộc tập exclude \n",
    "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Remove articles sử dụng biên dịch biểu thức \n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    # sử dụng re.sub để thay thế các ký tự =  \" \"  và trả về 1 text \n",
    "    text = re.sub(regex, ' ', text)\n",
    "\n",
    "    # Xoa khoẳng trắng thừa \n",
    "    text =  \" \".join(text.split())\n",
    "    return text \n",
    "\n",
    "class ExactMatch(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Mỗi đối tượng `SquadExample` chứa độ lệch cấp độ ký tự cho mỗi mã thông báo\n",
    "    trong đoạn đầu vào của nó. Chúng tôi sử dụng chúng để lấy lại khoảng văn bản tương ứng\n",
    "    đến các mã thông báo giữa mã thông báo bắt đầu và kết thúc được dự đoán .\n",
    "    Tất cả các câu trả lời thực tế cũng có trong mỗi đối tượng `SquadExample`.\n",
    "    tính toán tỷ lệ phần trăm của các điểm dữ liệu mà khoảng văn bản thu được\n",
    "    từ các dự đoán mô hình phù hợp với một trong những câu trả lời đúng sự thật.\n",
    "        \"\"\"\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self , epoch, logs=None):\n",
    "        pred_start , pred_end = self.model.predict(self.x_eval)\n",
    "        # Khởi tạo cuont để đếm số lượng câu trả lời chính xác \n",
    "        count = 0\n",
    "        # Lọc qua những vi dụ trong tập dư liệu kiểm tra eval_squad_examples mà không\n",
    "        # bị bỏ qua \n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # Duyệt qua những cặp vị trí bắt đầu và kết thúc dự đoán cùng với những chỉ số tương ứng \n",
    "        for idx , (start , end) in enumerate(zip(pred_start , pred_end)):\n",
    "            \n",
    "            # Lấy ra ví dụ tương ứng trong tập dữ liệu kiểm tra đã lọc\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # lấy ra danh sách cac vị trí ký tự trong đoạn văn bản tương ứng với từng token\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            # Nếu vị trí bắt đầu lớn hơn hoặc bằng độ dài của danh sách offsets,\n",
    "            # phương thức này sẽ bỏ qua ví dụ này và tiếp tục vòng lặp\n",
    "            # tức là dự đoán không thuộc offsets\n",
    "            if start >=  len(offsets):\n",
    "                continue\n",
    "            # Nếu không, lấy ra vị trí ký tự bắt đầu của token bắt đầu \n",
    "            pred_char_start = offsets[start][0]\n",
    "            # Nếu vị trí kết thúc nhỏ hơn độ dài của danh sách offsets, phương thức này sẽ lấy ra \n",
    "            # vị trí ký tự bắt đầu và kết thúc của câu trả lời trong văn bản ngữ cảnh \n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                # thực hiện khoanh vung cho câu tl\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else : \n",
    "            # Ngược lại ta lấy từ vị trí ký tự dự đoán đầu tiên đến hết câu . \n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            \n",
    "            # Chuẩn hóa câu trả lời dự đoán\n",
    "            normalized_pred_ans = normalize_text(pred_ans)\n",
    "            # tạo danh sách chuẩn hóa tất cả các nhãn thực \n",
    "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            \n",
    "            # nếu như các nhãn dự đoán mà thuộc nhãn thực thì tăng count lên 1 \n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1 \n",
    "        # Sau khi duyệt hết các ví dụ kiểm tra, phương thức này sẽ tính toán độ chính xác của mô hình\n",
    "        # bằng cách chia số lượng câu trả lời chính xác cho số lượng ví dụ kiểm tra\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,  # For demonstration, 3 epochs are recommended\n",
    "    verbose=2,\n",
    "    batch_size=64,\n",
    "    callbacks=[exact_match_callback],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
