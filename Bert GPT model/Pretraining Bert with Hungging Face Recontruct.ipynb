{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install datasets\n",
    "!pip install huggingface-hub\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import random \n",
    "import logging \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "\n",
    "nltk.download('punkt')\n",
    "# chỉ ghi lại thông báo lỗi \n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "# Set random Seed \n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define certain variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256 # Kích thước hàng loạt để đào tạo mã thôngh báo trên \n",
    "TOKENIZER_VOCABULARY = 25000 # TỔNG SỐ PHỤ TỪ DUY NHẤT MÀ TOKENIZER CÓ THỂ CÓ \n",
    "\n",
    "BLOCK_SIZE = 128 # SỐ LƯỢNG TOKENS TỐI ĐA CỦA MỘT BỘ MẪU ĐẦU VÀO \n",
    "NSP_PROB = 0.50 # XÁC XUẤT SINH RA CÂU TIẾP THEO THỰC TẾ TRONG NSP \n",
    "SHORT_SEQ_PROB = 0.1 # XÁC XUẤT XINH RA CHUỖI NGẮN HƠN ĐỂ GIẢM THIỂU SỰ KHÔNG KHỚP \n",
    "# GIỮA TIỀN HUẤN LUYỆN VÀ TINH CHỈNH \n",
    "MAX_LENGTH = 512 # SỐ LƯỢNG TỐI ĐA MẪU TOKENS ĐẦU VÀO SAU KHI ĐỆM \n",
    "\n",
    "MLM_PROB = 0.2  # XÁC XUẤT VỚI NHỮNG TOKENS ĐƯỢC CHE TRONG MASKED LANGUAGE MODEL \n",
    "\n",
    "TRAIN_BATCH_SIZE = 2 # KÍCH THƯỚC HÀNG LOẠT CHO HUẤN LUYỆN TRƯỚC MÔ HÌNH TRÊN \n",
    "MAX_EPOCHS = 1 # SÓ LƯỢNG KỶ NGUYÊN TỐI THIỂU CHO HUẤN LUYỆN MÔ HÌNH \n",
    "LEARNING_RATE =1e-4 \n",
    "\n",
    "MODEL_CHECKPOINT = 'bert-base-cased' # Name of pretrained model from 🤗 Model Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laod the WikiText dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a new Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lọc ra tất cả dữ liệu train \n",
    "# bỏ qua các trường hợp độ dài  = 0 và định dạng xuống dòng trong html \n",
    "# từ đó ta có bộ text nối liên tiếp \n",
    "all_texts = [\n",
    "    _ for _ in dataset['train']['test'] if len(_) > 0 and not _.startwith(\" =\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    # duyệt qua bộ dự liệu mỗi bước nhảy = 1 lô từ đó lấy ra các lô \n",
    "    for i in range(0 , len(all_texts) ,TOKENIZER_BATCH_SIZE ):\n",
    "        # Sử dụng từ khóa yield để trả về 1 lô văn bản từ danh sách \n",
    "        # kích thước mỗi lô = TOKENIZER_BATCH_SIZE \n",
    "        yield all_texts[i : i + TOKENIZER_BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator() , vocab_size=TOKENIZER_VOCABULARY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].select([i for i in range(1000)])\n",
    "dataset['validation'] = dataset['validation'].select([i for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement :\n",
    "    # 1 :  Chuẩn bị sắn tập dữ liệu cho nhiệm vụ NSP bằng cách tạo các cặp câu (A,B) \n",
    "    # trong đó B thực sự theo sau A hoặc B được lấy mẫu ngẫu nhiên từ một nơi khác trong \n",
    "    # văn bản .Nó cũng sẽ tạo một bộ nhãn tương ứng cho mỗi cặp , = 1 nếu B thực sự theo sau A else = 0\n",
    "\n",
    "    # 2 : Mã hóa tập dữ liệu văn bản thành id mã thông báo tương ứng sẽ được sử dụng để nhúng \n",
    "    # tra cứu trong bert \n",
    "\n",
    "    # 3 : Tạo các  đầu vào bổ sung cho mô hình như token_type_ids , attention_mask \n",
    "\n",
    "\n",
    "\n",
    "# Xác định số lượng tokens tối đa sau khi tokenizer (token hóa) mỗi mẫu huấn luyện \n",
    "# thực hiện\n",
    "max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "# Xây dựng hàm sử dụng huấn luyện các tính năng \n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    \"\"\" \n",
    "        Function to prepare features for NSP tasl \n",
    "    \n",
    "    Arguments :\n",
    "        examples : A dictionary with 1 key('text)\n",
    "            text :  List of raw documents (str)\n",
    "    Returns:\n",
    "        examples: A dictionary with 4 keys\n",
    "        1 Input_ids : List of tokenized , concatnated , and batched \n",
    "            sentences from the individual raw documents (int) \n",
    "        2 Token_type_ids : List of integers (0 or 1) corresponding \n",
    "            to : 0 for sentences A . 1 for sentences B \n",
    "        3 Attention_mask : List of integers (0 or 1 ) corresponding\n",
    "            to: 1 for non-padded tokens, 0 for padded\n",
    "        \n",
    "        4 Next_sentence_label: List of integers (0 or 1) corresponding\n",
    "            to: 1 if the second sentence actually follows the first,\n",
    "            0 if the senetence is sampled from somewhere else in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # Xử lý \n",
    "    # xóa đi những mẫu không mong muốn trong tệp huấn luyện \n",
    "    examples['document'] = [\n",
    "        # bỏ đi khoảng trống = strip () và biến đổi các text thành string \n",
    "        # sau đó kiểm tra xem chuỗi document có bắt đầu bằng ký tự = hay không \n",
    "        d.strip()  for d in examples['text'] if len(d) > 0 and not d.startwith(\" =\")\n",
    "    ]\n",
    "    # Tiếp theo tách cái tài liệu từ tập dữ liệu thành các câu riêng lẻ \n",
    "    examples['sentences'] = [\n",
    "        nltk.tokenize.sent_tokenize(document) for document in examples['document']\n",
    "    ]\n",
    "\n",
    "    # Chuyển các tokens trong danh sách các câu thành các id (int) để huấn luyện mô hình \n",
    "    examples['tokenized_sequences'] = [\n",
    "        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]\n",
    "        for doc in examples['sentences']\n",
    "    ]\n",
    "\n",
    "    # Xác định danh sách các đầu ra gồm 4 danh sách \n",
    "    examples ['input_ids'] = [] # int\n",
    "    examples ['token_type_ids'] = [] # o or 1 \n",
    "    examples[\"attention_mask\"] = []\n",
    "    examples[\"next_sentence_label\"] = []\n",
    "\n",
    "    # Duyệt qua danh sách chứa các câu từ tokenized sentences\n",
    "    # lấy ra nội dung và chỉ số tương ứng của nó \n",
    "    # sử dụng một enumerate như một iterator \n",
    "    for doc_index , document in enumerate(examples['tokenized_sentences']):\n",
    "        # khởi tạo 1 danh sách rỗng current_chunk để lưu trữ các đoạn văn \n",
    "        # bản đang được xử lý , mỗi đoạn văn bản là mỗi danh sách các câu \n",
    "        # đã được mã hóa thành danh sách các mã thông báo .\n",
    "        current_chunk = []\n",
    "        # Khởi tạo biến current_length để lưu trữ độ dài hiện tại của đoạn văn bản đang xử lý \n",
    "        # Độ dài được tính bằng số lượng tokens của đoạn văn bản \n",
    "        current_length = []\n",
    "        # khởi tạo biến i = 0 để lưu các đoạn hiện tại trong danh sách \n",
    "        i = 0 \n",
    "        # gán cho độ dài của câu mục tiêu băng với độ dài tối da \n",
    "        target_seq_length = max_num_tokens \n",
    "\n",
    "        # sử dụng hàm random.random để sinh ra một số ngẫu nhiên [0 -> 1]\n",
    "        # và so sánh với biến SHORT_SEQ_LENGTH là xác xuất để sinh ra đoạn văn bản ngắn hơn \n",
    "        # nếu số ngẫu nhiên nhỏ hơn hơn 0.1  thì  thì target sẽ được gán bằng \n",
    "        # một số nguyên ngẫu nhiên từ 2 -> max \n",
    "        if random.random() < SHORT_SEQ_PROB:\n",
    "            target_seq_length = random.randint(2 , max_num_tokens)\n",
    "\n",
    "        # Bắt đầu vòng while duyệt qua các đoạn trong tài liệu \n",
    "        # Thực hiện khối lệnh trong nó khi số đoạn hiện tại < số đoạn trong document \n",
    "        while i < len(document):\n",
    "            # thêm đoạn thứ i của document vào biến segment để chứa các đoạn\n",
    "            segment = document[i]\n",
    "            # sau đó thêm đoạn vào danh sách chứa các đoạn hiện tại current_chunk \n",
    "            current_chunk.append(segment)\n",
    "            # Cộng thêm số lượng tokens của đoạn vào độ dài hiện tiện \n",
    "            # tức là độ dài được cộng têm \n",
    "            target_seq_length += len(segment)\n",
    "\n",
    "            # kiểm tra xem số đoạn hiện tại có băng với số đoạn trong document - 1 `\n",
    "            # hay độ dài hiện tại có >= độ dài mục tiêu không \n",
    "            # Nếu có thì tiếp tục sử lý các đoạn hiện tại \n",
    "            if i == len(document) -1  or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                # Đặt biến a_end là số lượng các đoạn từ danh sách chứa số lượng các đoạn hiện tại sẽ được \n",
    "                # sử dụng cho câu A # các đoạn này được lưu trữ trong current_chunk \n",
    "                    a_end = 1 \n",
    "                    # Nếu hiện tại có nhiều hơn 2 đoạn thì \n",
    "                    if len(current_chunk) >= 2 :\n",
    "                        # a_end sẽ được lấy ngẫu nhiên từ 1 -> số lượng các đoạn - 1\n",
    "                        # tức là 1 -> len(current_chunk) - 1\n",
    "                        a_end = random.randint(1 , len(current_chunk) - 1)\n",
    "\n",
    "                    # Khởi tạo danh sách để chứa đựng tokens_a \n",
    "                    tokens_a = []\n",
    "                    # Lặp qua từ 0 đến a_end , a_end cho biết các đoạn hiện tại hiện tại hiện có \n",
    "                    # trong current_chunk \n",
    "                    for j in range(a_end):\n",
    "                        # sau đó thêm các tokens trong các đoạn hiện tại vào tokens a\n",
    "                        # sử dung extend để nối các đoạn \n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    \n",
    "                    # Khởi tại danh sách chứa các tokens B là các tokens sau A\n",
    "                    tokens_b = []\n",
    "                    # kiểm tra xem có phải current_chunk có phải chỉ chứa 1 đoạn không \n",
    "                    # hay xác xuất sinh ra câu < xác xuất sinh câu tiếp thep NSP_PROB 0.50\n",
    "                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n",
    "                        # Đặt random next = False sẽ được sử dụng để chỉ ra rằng câu B là câu \n",
    "                        # ngẫu nhiên chứ không phải là câu tiếp theo của A trong document \n",
    "                        # Điều này để tạo ra nhãn cho cac tác vụ NSP\n",
    "                        is_random_next = True # False \n",
    "                        # đặt độ dài mục tiêu của b = độ dìa mục tiêu - độ dài tokens a\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "                        for _ in range(10):\n",
    "                            # khởi tạo Random_document để lưu trữ những chỉ số ngẫu nhiên của \n",
    "                            # văn bản khác với tài liệu hiện tại từ examples['tokenized_sentences]\n",
    "                            # sau mỗi lần lặp gán cho random_document_index bộ chỉ số từ 0\n",
    "                            # đến examples['tokenized_sentences'] - 1\n",
    "                            random_document_index = random.randint(\n",
    "                                0 , len(examples['tokenized_sentences']) - 1\n",
    "                            )\n",
    "                            # Kiểm tra điều kiện nếu số ngẫu nhiên của câu sinh ra có khác với \n",
    "                            # chỉ số của tài liệu hiện tại không có thì thoát \n",
    "                            # tức là đoạn mã này sẽ chọn 1 bộ tài liệu bắt kỳ nếu trùng chọn lại \n",
    "                        # Mục đích của đoạn mã này để đảm bảo rằng tại liệu ngẫu nhiên không \n",
    "                        # Trùng với tài liệu hiện tại đoạn mã sẽ kết thúc khi đạt được kết quả là \n",
    "                        # một bộ chỉ số khác nhau tức là khi đạt được mục đích \n",
    "                            if random_document_index != doc_index: \n",
    "                                break \n",
    "                        # Lấy đoạn tài liệu ngẫu nhiên từ examples[\"tokenized_sentences\"] theo các chỉ \n",
    "                        # số đã lấy được trước đó random_document_index và gán cho biến random_document\n",
    "                        random_document = examples['tokenized_sentences'][\n",
    "                            random_document_index\n",
    "                        ]\n",
    "                        # chọn một số ngẫu nhiên 0 -> số lượng đoạn trong tài liệu ngẫu nhiên -1\n",
    "                        random_start = random.randint(0 , len(random_document) -1)\n",
    "\n",
    "                        # Lặp các đoạn ngẫu nhiên từ random_start đến hết tài liệu ngẫu nhiên \n",
    "                        for j in range(random_start , len(random_document)):\n",
    "                            # và thêm nó vào danh sách tokens_b \n",
    "                            # sử dụng extend để nối các đoạn \n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            # nếu số lượng tokens_b vượt quá độ dài mục tiêu b \n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                # dừng lại\n",
    "                                break\n",
    "                        # Đếm số lượng các đoạn văn bản không được ử dụng trong danh sách hiện tại \n",
    "                        # Giảm giá trị của i đi num_unused_segments để quay lại các đoạn chưa được xử l\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Trường hợp còn lại nếu có nhiều hơn một đoạn trong danh sách các đoạn hiện tại \n",
    "                    # và xác xuấ NSP_PROB không xảy ra \n",
    "                    else: \n",
    "                        # Chỉ ra B là câu tiếp theo của A và có nhãn = 1 = True \n",
    "                        is_random_next =False # True \n",
    "                        # lặp qua danh sách chứa các đoạn hiện tại a_end đến hết số lượng \n",
    "                        # các đoạn hiện tại \n",
    "                        for j in range(a_end , len(current_chunk)):\n",
    "                            # Thêm các đoạn hiện tại từ current vào danh sách tokens_ b    \n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # Khời tạp đầu vào input_ids (int) \n",
    "                    input_ids = tokenizer.build_inputs_with_special_tokens(\n",
    "                        tokens_a , tokens_b\n",
    "                    )\n",
    "\n",
    "                    # Xây dựng bộ tokens_type_ids 0 , 1 \n",
    "                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "                    # thêm đệm cho bộ input_ids và token_type_ids \n",
    "                    padded = tokenizer.pad(\n",
    "                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                    )\n",
    "\n",
    "                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n",
    "                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n",
    "                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n",
    "                    examples[\"next_sentence_label\"].append(1 if is_random_next else 0)\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "            i += 1\n",
    "    # We delete all the un-necessary columns from our dataset\n",
    "    del examples[\"document\"]\n",
    "    del examples[\"sentences\"]\n",
    "    del examples[\"text\"]\n",
    "    del examples[\"tokenized_sentences\"]\n",
    "\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset= dataset.map(\n",
    "    prepare_train_features , batched=True , remove_columns=['text'] , num_proc=1\n",
    ")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling \n",
    "\n",
    "collater = DataCollatorForLanguageModeling ( \n",
    "    tokenizer=tokenizer , mlm=True , mlm_probability=MLM_PROB, return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset['train'].to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols =['labels', 'next_sentences_label'], \n",
    "    batch_size = TRAIN_BATCH_SIZE, \n",
    "    shuffle =True , \n",
    "    collate_fn =collater, \n",
    ")\n",
    "\n",
    "validation = tokenized_dataset['validation'].to_tf_dataset(\n",
    "    columns=['input_ids' , 'token_type_ids'], \n",
    "    label_cols =['labels' , 'next_sentences_label'],\n",
    "    batch_size = TRAIN_BATCH_SIZE , \n",
    "    suffle=True , \n",
    "    collate_fn=collater, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig \n",
    "config = BertConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining(config)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, validation_data=validation, epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n",
    "tokenizer.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n",
    "\n",
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained(\"your-username/my-awesome-model\")\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
