{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install datasets\n",
    "!pip install huggingface-hub\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import random \n",
    "import logging \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "\n",
    "nltk.download('punkt')\n",
    "# ch·ªâ ghi l·∫°i th√¥ng b√°o l·ªói \n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "# Set random Seed \n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define certain variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256 # K√≠ch th∆∞·ªõc h√†ng lo·∫°t ƒë·ªÉ ƒë√†o t·∫°o m√£ th√¥ngh b√°o tr√™n \n",
    "TOKENIZER_VOCABULARY = 25000 # T·ªîNG S·ªê PH·ª§ T·ª™ DUY NH·∫§T M√Ä TOKENIZER C√ì TH·ªÇ C√ì \n",
    "\n",
    "BLOCK_SIZE = 128 # S·ªê L∆Ø·ª¢NG TOKENS T·ªêI ƒêA C·ª¶A M·ªòT B·ªò M·∫™U ƒê·∫¶U V√ÄO \n",
    "NSP_PROB = 0.50 # X√ÅC XU·∫§T SINH RA C√ÇU TI·∫æP THEO TH·ª∞C T·∫æ TRONG NSP \n",
    "SHORT_SEQ_PROB = 0.1 # X√ÅC XU·∫§T XINH RA CHU·ªñI NG·∫ÆN H∆†N ƒê·ªÇ GI·∫¢M THI·ªÇU S·ª∞ KH√îNG KH·ªöP \n",
    "# GI·ªÆA TI·ªÄN HU·∫§N LUY·ªÜN V√Ä TINH CH·ªàNH \n",
    "MAX_LENGTH = 512 # S·ªê L∆Ø·ª¢NG T·ªêI ƒêA M·∫™U TOKENS ƒê·∫¶U V√ÄO SAU KHI ƒê·ªÜM \n",
    "\n",
    "MLM_PROB = 0.2  # X√ÅC XU·∫§T V·ªöI NH·ªÆNG TOKENS ƒê∆Ø·ª¢C CHE TRONG MASKED LANGUAGE MODEL \n",
    "\n",
    "TRAIN_BATCH_SIZE = 2 # K√çCH TH∆Ø·ªöC H√ÄNG LO·∫†T CHO HU·∫§N LUY·ªÜN TR∆Ø·ªöC M√î H√åNH TR√äN \n",
    "MAX_EPOCHS = 1 # S√ì L∆Ø·ª¢NG K·ª∂ NGUY√äN T·ªêI THI·ªÇU CHO HU·∫§N LUY·ªÜN M√î H√åNH \n",
    "LEARNING_RATE =1e-4 \n",
    "\n",
    "MODEL_CHECKPOINT = 'bert-base-cased' # Name of pretrained model from ü§ó Model Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laod the WikiText dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a new Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l·ªçc ra t·∫•t c·∫£ d·ªØ li·ªáu train \n",
    "# b·ªè qua c√°c tr∆∞·ªùng h·ª£p ƒë·ªô d√†i  = 0 v√† ƒë·ªãnh d·∫°ng xu·ªëng d√≤ng trong html \n",
    "# t·ª´ ƒë√≥ ta c√≥ b·ªô text n·ªëi li√™n ti·∫øp \n",
    "all_texts = [\n",
    "    _ for _ in dataset['train']['test'] if len(_) > 0 and not _.startwith(\" =\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    # duy·ªát qua b·ªô d·ª± li·ªáu m·ªói b∆∞·ªõc nh·∫£y = 1 l√¥ t·ª´ ƒë√≥ l·∫•y ra c√°c l√¥ \n",
    "    for i in range(0 , len(all_texts) ,TOKENIZER_BATCH_SIZE ):\n",
    "        # S·ª≠ d·ª•ng t·ª´ kh√≥a yield ƒë·ªÉ tr·∫£ v·ªÅ 1 l√¥ vƒÉn b·∫£n t·ª´ danh s√°ch \n",
    "        # k√≠ch th∆∞·ªõc m·ªói l√¥ = TOKENIZER_BATCH_SIZE \n",
    "        yield all_texts[i : i + TOKENIZER_BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator() , vocab_size=TOKENIZER_VOCABULARY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].select([i for i in range(1000)])\n",
    "dataset['validation'] = dataset['validation'].select([i for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement :\n",
    "    # 1 :  Chu·∫©n b·ªã s·∫Øn t·∫≠p d·ªØ li·ªáu cho nhi·ªám v·ª• NSP b·∫±ng c√°ch t·∫°o c√°c c·∫∑p c√¢u (A,B) \n",
    "    # trong ƒë√≥ B th·ª±c s·ª± theo sau A ho·∫∑c B ƒë∆∞·ª£c l·∫•y m·∫´u ng·∫´u nhi√™n t·ª´ m·ªôt n∆°i kh√°c trong \n",
    "    # vƒÉn b·∫£n .N√≥ c≈©ng s·∫Ω t·∫°o m·ªôt b·ªô nh√£n t∆∞∆°ng ·ª©ng cho m·ªói c·∫∑p , = 1 n·∫øu B th·ª±c s·ª± theo sau A else = 0\n",
    "\n",
    "    # 2 : M√£ h√≥a t·∫≠p d·ªØ li·ªáu vƒÉn b·∫£n th√†nh id m√£ th√¥ng b√°o t∆∞∆°ng ·ª©ng s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ nh√∫ng \n",
    "    # tra c·ª©u trong bert \n",
    "\n",
    "    # 3 : T·∫°o c√°c  ƒë·∫ßu v√†o b·ªï sung cho m√¥ h√¨nh nh∆∞ token_type_ids , attention_mask \n",
    "\n",
    "\n",
    "\n",
    "# X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng tokens t·ªëi ƒëa sau khi tokenizer (token h√≥a) m·ªói m·∫´u hu·∫•n luy·ªán \n",
    "# th·ª±c hi·ªán\n",
    "max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "# X√¢y d·ª±ng h√†m s·ª≠ d·ª•ng hu·∫•n luy·ªán c√°c t√≠nh nƒÉng \n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    \"\"\" \n",
    "        Function to prepare features for NSP tasl \n",
    "    \n",
    "    Arguments :\n",
    "        examples : A dictionary with 1 key('text)\n",
    "            text :  List of raw documents (str)\n",
    "    Returns:\n",
    "        examples: A dictionary with 4 keys\n",
    "        1 Input_ids : List of tokenized , concatnated , and batched \n",
    "            sentences from the individual raw documents (int) \n",
    "        2 Token_type_ids : List of integers (0 or 1) corresponding \n",
    "            to : 0 for sentences A . 1 for sentences B \n",
    "        3 Attention_mask : List of integers (0 or 1 ) corresponding\n",
    "            to: 1 for non-padded tokens, 0 for padded\n",
    "        \n",
    "        4 Next_sentence_label: List of integers (0 or 1) corresponding\n",
    "            to: 1 if the second sentence actually follows the first,\n",
    "            0 if the senetence is sampled from somewhere else in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # X·ª≠ l√Ω \n",
    "    # x√≥a ƒëi nh·ªØng m·∫´u kh√¥ng mong mu·ªën trong t·ªáp hu·∫•n luy·ªán \n",
    "    examples['document'] = [\n",
    "        # b·ªè ƒëi kho·∫£ng tr·ªëng = strip () v√† bi·∫øn ƒë·ªïi c√°c text th√†nh string \n",
    "        # sau ƒë√≥ ki·ªÉm tra xem chu·ªói document c√≥ b·∫Øt ƒë·∫ßu b·∫±ng k√Ω t·ª± = hay kh√¥ng \n",
    "        d.strip()  for d in examples['text'] if len(d) > 0 and not d.startwith(\" =\")\n",
    "    ]\n",
    "    # Ti·∫øp theo t√°ch c√°i t√†i li·ªáu t·ª´ t·∫≠p d·ªØ li·ªáu th√†nh c√°c c√¢u ri√™ng l·∫ª \n",
    "    examples['sentences'] = [\n",
    "        nltk.tokenize.sent_tokenize(document) for document in examples['document']\n",
    "    ]\n",
    "\n",
    "    # Chuy·ªÉn c√°c tokens trong danh s√°ch c√°c c√¢u th√†nh c√°c id (int) ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh \n",
    "    examples['tokenized_sequences'] = [\n",
    "        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]\n",
    "        for doc in examples['sentences']\n",
    "    ]\n",
    "\n",
    "    # X√°c ƒë·ªãnh danh s√°ch c√°c ƒë·∫ßu ra g·ªìm 4 danh s√°ch \n",
    "    examples ['input_ids'] = [] # int\n",
    "    examples ['token_type_ids'] = [] # o or 1 \n",
    "    examples[\"attention_mask\"] = []\n",
    "    examples[\"next_sentence_label\"] = []\n",
    "\n",
    "    # Duy·ªát qua danh s√°ch ch·ª©a c√°c c√¢u t·ª´ tokenized sentences\n",
    "    # l·∫•y ra n·ªôi dung v√† ch·ªâ s·ªë t∆∞∆°ng ·ª©ng c·ªßa n√≥ \n",
    "    # s·ª≠ d·ª•ng m·ªôt enumerate nh∆∞ m·ªôt iterator \n",
    "    for doc_index , document in enumerate(examples['tokenized_sentences']):\n",
    "        # kh·ªüi t·∫°o 1 danh s√°ch r·ªóng current_chunk ƒë·ªÉ l∆∞u tr·ªØ c√°c ƒëo·∫°n vƒÉn \n",
    "        # b·∫£n ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω , m·ªói ƒëo·∫°n vƒÉn b·∫£n l√† m·ªói danh s√°ch c√°c c√¢u \n",
    "        # ƒë√£ ƒë∆∞·ª£c m√£ h√≥a th√†nh danh s√°ch c√°c m√£ th√¥ng b√°o .\n",
    "        current_chunk = []\n",
    "        # Kh·ªüi t·∫°o bi·∫øn current_length ƒë·ªÉ l∆∞u tr·ªØ ƒë·ªô d√†i hi·ªán t·∫°i c·ªßa ƒëo·∫°n vƒÉn b·∫£n ƒëang x·ª≠ l√Ω \n",
    "        # ƒê·ªô d√†i ƒë∆∞·ª£c t√≠nh b·∫±ng s·ªë l∆∞·ª£ng tokens c·ªßa ƒëo·∫°n vƒÉn b·∫£n \n",
    "        current_length = []\n",
    "        # kh·ªüi t·∫°o bi·∫øn i = 0 ƒë·ªÉ l∆∞u c√°c ƒëo·∫°n hi·ªán t·∫°i trong danh s√°ch \n",
    "        i = 0 \n",
    "        # g√°n cho ƒë·ªô d√†i c·ªßa c√¢u m·ª•c ti√™u bƒÉng v·ªõi ƒë·ªô d√†i t·ªëi da \n",
    "        target_seq_length = max_num_tokens \n",
    "\n",
    "        # s·ª≠ d·ª•ng h√†m random.random ƒë·ªÉ sinh ra m·ªôt s·ªë ng·∫´u nhi√™n [0 -> 1]\n",
    "        # v√† so s√°nh v·ªõi bi·∫øn SHORT_SEQ_LENGTH l√† x√°c xu·∫•t ƒë·ªÉ sinh ra ƒëo·∫°n vƒÉn b·∫£n ng·∫Øn h∆°n \n",
    "        # n·∫øu s·ªë ng·∫´u nhi√™n nh·ªè h∆°n h∆°n 0.1  th√¨  th√¨ target s·∫Ω ƒë∆∞·ª£c g√°n b·∫±ng \n",
    "        # m·ªôt s·ªë nguy√™n ng·∫´u nhi√™n t·ª´ 2 -> max \n",
    "        if random.random() < SHORT_SEQ_PROB:\n",
    "            target_seq_length = random.randint(2 , max_num_tokens)\n",
    "\n",
    "        # B·∫Øt ƒë·∫ßu v√≤ng while duy·ªát qua c√°c ƒëo·∫°n trong t√†i li·ªáu \n",
    "        # Th·ª±c hi·ªán kh·ªëi l·ªánh trong n√≥ khi s·ªë ƒëo·∫°n hi·ªán t·∫°i < s·ªë ƒëo·∫°n trong document \n",
    "        while i < len(document):\n",
    "            # th√™m ƒëo·∫°n th·ª© i c·ªßa document v√†o bi·∫øn segment ƒë·ªÉ ch·ª©a c√°c ƒëo·∫°n\n",
    "            segment = document[i]\n",
    "            # sau ƒë√≥ th√™m ƒëo·∫°n v√†o danh s√°ch ch·ª©a c√°c ƒëo·∫°n hi·ªán t·∫°i current_chunk \n",
    "            current_chunk.append(segment)\n",
    "            # C·ªông th√™m s·ªë l∆∞·ª£ng tokens c·ªßa ƒëo·∫°n v√†o ƒë·ªô d√†i hi·ªán ti·ªán \n",
    "            # t·ª©c l√† ƒë·ªô d√†i ƒë∆∞·ª£c c·ªông t√™m \n",
    "            target_seq_length += len(segment)\n",
    "\n",
    "            # ki·ªÉm tra xem s·ªë ƒëo·∫°n hi·ªán t·∫°i c√≥ bƒÉng v·ªõi s·ªë ƒëo·∫°n trong document - 1 `\n",
    "            # hay ƒë·ªô d√†i hi·ªán t·∫°i c√≥ >= ƒë·ªô d√†i m·ª•c ti√™u kh√¥ng \n",
    "            # N·∫øu c√≥ th√¨ ti·∫øp t·ª•c s·ª≠ l√Ω c√°c ƒëo·∫°n hi·ªán t·∫°i \n",
    "            if i == len(document) -1  or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                # ƒê·∫∑t bi·∫øn a_end l√† s·ªë l∆∞·ª£ng c√°c ƒëo·∫°n t·ª´ danh s√°ch ch·ª©a s·ªë l∆∞·ª£ng c√°c ƒëo·∫°n hi·ªán t·∫°i s·∫Ω ƒë∆∞·ª£c \n",
    "                # s·ª≠ d·ª•ng cho c√¢u A # c√°c ƒëo·∫°n n√†y ƒë∆∞·ª£c l∆∞u tr·ªØ trong current_chunk \n",
    "                    a_end = 1 \n",
    "                    # N·∫øu hi·ªán t·∫°i c√≥ nhi·ªÅu h∆°n 2 ƒëo·∫°n th√¨ \n",
    "                    if len(current_chunk) >= 2 :\n",
    "                        # a_end s·∫Ω ƒë∆∞·ª£c l·∫•y ng·∫´u nhi√™n t·ª´ 1 -> s·ªë l∆∞·ª£ng c√°c ƒëo·∫°n - 1\n",
    "                        # t·ª©c l√† 1 -> len(current_chunk) - 1\n",
    "                        a_end = random.randint(1 , len(current_chunk) - 1)\n",
    "\n",
    "                    # Kh·ªüi t·∫°o danh s√°ch ƒë·ªÉ ch·ª©a ƒë·ª±ng tokens_a \n",
    "                    tokens_a = []\n",
    "                    # L·∫∑p qua t·ª´ 0 ƒë·∫øn a_end , a_end cho bi·∫øt c√°c ƒëo·∫°n hi·ªán t·∫°i hi·ªán t·∫°i hi·ªán c√≥ \n",
    "                    # trong current_chunk \n",
    "                    for j in range(a_end):\n",
    "                        # sau ƒë√≥ th√™m c√°c tokens trong c√°c ƒëo·∫°n hi·ªán t·∫°i v√†o tokens a\n",
    "                        # s·ª≠ dung extend ƒë·ªÉ n·ªëi c√°c ƒëo·∫°n \n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    \n",
    "                    # Kh·ªüi t·∫°i danh s√°ch ch·ª©a c√°c tokens B l√† c√°c tokens sau A\n",
    "                    tokens_b = []\n",
    "                    # ki·ªÉm tra xem c√≥ ph·∫£i current_chunk c√≥ ph·∫£i ch·ªâ ch·ª©a 1 ƒëo·∫°n kh√¥ng \n",
    "                    # hay x√°c xu·∫•t sinh ra c√¢u < x√°c xu·∫•t sinh c√¢u ti·∫øp thep NSP_PROB 0.50\n",
    "                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n",
    "                        # ƒê·∫∑t random next = False s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ch·ªâ ra r·∫±ng c√¢u B l√† c√¢u \n",
    "                        # ng·∫´u nhi√™n ch·ª© kh√¥ng ph·∫£i l√† c√¢u ti·∫øp theo c·ªßa A trong document \n",
    "                        # ƒêi·ªÅu n√†y ƒë·ªÉ t·∫°o ra nh√£n cho cac t√°c v·ª• NSP\n",
    "                        is_random_next = True # False \n",
    "                        # ƒë·∫∑t ƒë·ªô d√†i m·ª•c ti√™u c·ªßa b = ƒë·ªô d√¨a m·ª•c ti√™u - ƒë·ªô d√†i tokens a\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "                        for _ in range(10):\n",
    "                            # kh·ªüi t·∫°o Random_document ƒë·ªÉ l∆∞u tr·ªØ nh·ªØng ch·ªâ s·ªë ng·∫´u nhi√™n c·ªßa \n",
    "                            # vƒÉn b·∫£n kh√°c v·ªõi t√†i li·ªáu hi·ªán t·∫°i t·ª´ examples['tokenized_sentences]\n",
    "                            # sau m·ªói l·∫ßn l·∫∑p g√°n cho random_document_index b·ªô ch·ªâ s·ªë t·ª´ 0\n",
    "                            # ƒë·∫øn examples['tokenized_sentences'] - 1\n",
    "                            random_document_index = random.randint(\n",
    "                                0 , len(examples['tokenized_sentences']) - 1\n",
    "                            )\n",
    "                            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán n·∫øu s·ªë ng·∫´u nhi√™n c·ªßa c√¢u sinh ra c√≥ kh√°c v·ªõi \n",
    "                            # ch·ªâ s·ªë c·ªßa t√†i li·ªáu hi·ªán t·∫°i kh√¥ng c√≥ th√¨ tho√°t \n",
    "                            # t·ª©c l√† ƒëo·∫°n m√£ n√†y s·∫Ω ch·ªçn 1 b·ªô t√†i li·ªáu b·∫Øt k·ª≥ n·∫øu tr√πng ch·ªçn l·∫°i \n",
    "                        # M·ª•c ƒë√≠ch c·ªßa ƒëo·∫°n m√£ n√†y ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng t·∫°i li·ªáu ng·∫´u nhi√™n kh√¥ng \n",
    "                        # Tr√πng v·ªõi t√†i li·ªáu hi·ªán t·∫°i ƒëo·∫°n m√£ s·∫Ω k·∫øt th√∫c khi ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ l√† \n",
    "                        # m·ªôt b·ªô ch·ªâ s·ªë kh√°c nhau t·ª©c l√† khi ƒë·∫°t ƒë∆∞·ª£c m·ª•c ƒë√≠ch \n",
    "                            if random_document_index != doc_index: \n",
    "                                break \n",
    "                        # L·∫•y ƒëo·∫°n t√†i li·ªáu ng·∫´u nhi√™n t·ª´ examples[\"tokenized_sentences\"] theo c√°c ch·ªâ \n",
    "                        # s·ªë ƒë√£ l·∫•y ƒë∆∞·ª£c tr∆∞·ªõc ƒë√≥ random_document_index v√† g√°n cho bi·∫øn random_document\n",
    "                        random_document = examples['tokenized_sentences'][\n",
    "                            random_document_index\n",
    "                        ]\n",
    "                        # ch·ªçn m·ªôt s·ªë ng·∫´u nhi√™n 0 -> s·ªë l∆∞·ª£ng ƒëo·∫°n trong t√†i li·ªáu ng·∫´u nhi√™n -1\n",
    "                        random_start = random.randint(0 , len(random_document) -1)\n",
    "\n",
    "                        # L·∫∑p c√°c ƒëo·∫°n ng·∫´u nhi√™n t·ª´ random_start ƒë·∫øn h·∫øt t√†i li·ªáu ng·∫´u nhi√™n \n",
    "                        for j in range(random_start , len(random_document)):\n",
    "                            # v√† th√™m n√≥ v√†o danh s√°ch tokens_b \n",
    "                            # s·ª≠ d·ª•ng extend ƒë·ªÉ n·ªëi c√°c ƒëo·∫°n \n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            # n·∫øu s·ªë l∆∞·ª£ng tokens_b v∆∞·ª£t qu√° ƒë·ªô d√†i m·ª•c ti√™u b \n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                # d·ª´ng l·∫°i\n",
    "                                break\n",
    "                        # ƒê·∫øm s·ªë l∆∞·ª£ng c√°c ƒëo·∫°n vƒÉn b·∫£n kh√¥ng ƒë∆∞·ª£c ·ª≠ d·ª•ng trong danh s√°ch hi·ªán t·∫°i \n",
    "                        # Gi·∫£m gi√° tr·ªã c·ªßa i ƒëi num_unused_segments ƒë·ªÉ quay l·∫°i c√°c ƒëo·∫°n ch∆∞a ƒë∆∞·ª£c x·ª≠ l\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Tr∆∞·ªùng h·ª£p c√≤n l·∫°i n·∫øu c√≥ nhi·ªÅu h∆°n m·ªôt ƒëo·∫°n trong danh s√°ch c√°c ƒëo·∫°n hi·ªán t·∫°i \n",
    "                    # v√† x√°c xu·∫• NSP_PROB kh√¥ng x·∫£y ra \n",
    "                    else: \n",
    "                        # Ch·ªâ ra B l√† c√¢u ti·∫øp theo c·ªßa A v√† c√≥ nh√£n = 1 = True \n",
    "                        is_random_next =False # True \n",
    "                        # l·∫∑p qua danh s√°ch ch·ª©a c√°c ƒëo·∫°n hi·ªán t·∫°i a_end ƒë·∫øn h·∫øt s·ªë l∆∞·ª£ng \n",
    "                        # c√°c ƒëo·∫°n hi·ªán t·∫°i \n",
    "                        for j in range(a_end , len(current_chunk)):\n",
    "                            # Th√™m c√°c ƒëo·∫°n hi·ªán t·∫°i t·ª´ current v√†o danh s√°ch tokens_ b    \n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # Kh·ªùi t·∫°p ƒë·∫ßu v√†o input_ids (int) \n",
    "                    input_ids = tokenizer.build_inputs_with_special_tokens(\n",
    "                        tokens_a , tokens_b\n",
    "                    )\n",
    "\n",
    "                    # X√¢y d·ª±ng b·ªô tokens_type_ids 0 , 1 \n",
    "                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "                    # th√™m ƒë·ªám cho b·ªô input_ids v√† token_type_ids \n",
    "                    padded = tokenizer.pad(\n",
    "                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                    )\n",
    "\n",
    "                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n",
    "                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n",
    "                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n",
    "                    examples[\"next_sentence_label\"].append(1 if is_random_next else 0)\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "            i += 1\n",
    "    # We delete all the un-necessary columns from our dataset\n",
    "    del examples[\"document\"]\n",
    "    del examples[\"sentences\"]\n",
    "    del examples[\"text\"]\n",
    "    del examples[\"tokenized_sentences\"]\n",
    "\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset= dataset.map(\n",
    "    prepare_train_features , batched=True , remove_columns=['text'] , num_proc=1\n",
    ")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling \n",
    "\n",
    "collater = DataCollatorForLanguageModeling ( \n",
    "    tokenizer=tokenizer , mlm=True , mlm_probability=MLM_PROB, return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset['train'].to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols =['labels', 'next_sentences_label'], \n",
    "    batch_size = TRAIN_BATCH_SIZE, \n",
    "    shuffle =True , \n",
    "    collate_fn =collater, \n",
    ")\n",
    "\n",
    "validation = tokenized_dataset['validation'].to_tf_dataset(\n",
    "    columns=['input_ids' , 'token_type_ids'], \n",
    "    label_cols =['labels' , 'next_sentences_label'],\n",
    "    batch_size = TRAIN_BATCH_SIZE , \n",
    "    suffle=True , \n",
    "    collate_fn=collater, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig \n",
    "config = BertConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining(config)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, validation_data=validation, epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n",
    "tokenizer.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n",
    "\n",
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained(\"your-username/my-awesome-model\")\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
