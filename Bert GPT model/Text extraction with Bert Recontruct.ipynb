{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import json \n",
    "import string \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import layers \n",
    "from tokenizers import BertWordPieceTokenizer \n",
    "from transformers import BertTokenizer , TFBertModel , BertConfig \n",
    "\n",
    "max_len = 384 \n",
    "configuration = BertConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the slow pretrained tokenizer\n",
    "slow_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "save_path = 'bert_base_uncased/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "slow_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Load the fasst tokenizer from saved file \n",
    "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
    "train_path = keras.utils.get_file('train.json', train_data_url)\n",
    "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "eval_path =  keras.utils.get_file(\"eval.json\", eval_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample:\n",
    "    def __init__(self, question , context , start_char_idx , answer_text, all_answers):\n",
    "        super().__init__()\n",
    "        self.question = question \n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "\n",
    "    def preprocessor (self):\n",
    "        context = self.context \n",
    "        question = self.question\n",
    "        answer_text = self.answer_text\n",
    "        start_char_idx = self.start_char_idx \n",
    "\n",
    "        # xóa đi các khoảng trắng trong câu hỏi và câu trả lời \n",
    "        context = ' '.join(str(context).split())\n",
    "        question = ' '.join(str(question).split())\n",
    "        answer = ' '.join(str(answer).split())\n",
    "\n",
    "        # Tìm kiêm vị trí cuối cùng của câu trả lời \n",
    "        # bằng kí tự đầu tiên + độ dài câu trả lời \n",
    "        end_char_idx = start_char_idx + len(answer)\n",
    "        if end_char_idx >= len(context):\n",
    "            self.skip = True \n",
    "            return \n",
    "        # Đánh dấu các chỉ mục ký tự trong ngữ cảnh có trong câu trả lời \n",
    "        # Đầu tiên biến các token có trong con text = 0\n",
    "        is_char_in_ans = [0] * len(context)\n",
    "        \n",
    "        # Đánh các token có trong câu tl bằng 1 \n",
    "        for idx in range(start_char_idx, end_char_idx):\n",
    "            is_char_in_ans[idx] = 1\n",
    "\n",
    "        # Thực hiện mã hóa thông báo context \n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "\n",
    "        # Tìm mã thông báo được tạo từ ký tự trả lời \n",
    "        # Khởi tạo 1 danh sách , danh sách này sẽ chứa vị trí của các token thuộc về câu trả lời \n",
    "        ans_token_idx =  []\n",
    "        # lặp qua danh sách các offeets mã hóa thông báo context từ start đến end \n",
    "        # từ đó ta sẽ có được vị trí và số ký tự cho mỗi token\n",
    "        for idx , (start , end) in enumerate(tokenized_context.offsets):\n",
    "            # Trường hợp nếu tổng của ký tự cho câu trả lời của token # 0 \n",
    "            # Tức là trong context có chứa câu trả lời \n",
    "            if sum(is_char_in_ans[start , end]) > 0:\n",
    "                # Ta sẽ thêm vị trí cho token thuộc về câu trả lời dựa vào danh sách offsets \n",
    "                ans_token_idx.append(idx)\n",
    "\n",
    "        # nếu như vị trí của token trong câu trả lừi = 0 tực khôngb không tồn tại câu tl \n",
    "        # ta bỏ qua \n",
    "        if len(ans_token_idx) == 0:\n",
    "            self.skip = True \n",
    "            return \n",
    "        \n",
    "        # Lấy ra vị trí bắt đầu và kết thúc của mỗi câu trả lời trong token\n",
    "        start_token_idx = ans_token_idx[0]\n",
    "        end_token_idx = ans_token_idx[-1]\n",
    "\n",
    "        # Thực hiện mã hóa cho câu hỏi \n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "\n",
    "        # Khởi tạo đầu vào cho mô hình \n",
    "        # bằng token_context + token_question là những ma trận mã hóa số \n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[:1]\n",
    "        # biến đổi thành các kiểu khác nhau cho cả 2 câu thành 0  và 1 \n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] *len(\n",
    "            tokenized_question.ids[1:]\n",
    "        )\n",
    "        # Khởi tạo mặt nạ attention \n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Tiến hành thêm đệm cho input , token_type , mask \n",
    "        # Tất cả các đệm này đêu là 0 \n",
    "        # Độ dài của phần đệm = độ dài tối đa - độ dài của đầu vào mô hình \n",
    "        padding_length = max_len - len(input_ids)\n",
    "        # trường hợp padding_length  > 0 tức là cần đệm \n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ( [0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        # Ngược lại nếu padding_length < 0  tức là không cầm đệm ta bỏ qua \n",
    "        elif padding_length < 0:\n",
    "            self.skip = True \n",
    "            return \n",
    "        \n",
    "\n",
    "        # Trả về một loạt các danh sách input_ids , token_type_ids , attention_mask\n",
    "        # start_token_idx . end_token_idx , tokenized_context.offsets \n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.start_token_idx = start_token_idx\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.context_token_to_char = tokenized_context.offsets\n",
    "\n",
    "\n",
    "with open(train_path)  as f:\n",
    "    raw_train_data = json.load(f)\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "# paragraphs  đoạn văn \n",
    "\n",
    "#  Thực hiện hóa tách và biến đổi dữ liệu \n",
    "def create_squad_examples(raw_data):\n",
    "    squad_examples = []\n",
    "    # Duyệt qua các phần tử trong raw_data['data'] \n",
    "    # mỗi phần tử tượng trưng như một bài viết trong từ điển wiki \n",
    "    for item in raw_data['data']:\n",
    "        # sau đó duyệt qua các đoạn văn bản có trong mỗi bài viết \n",
    "        for para in item['paragraphs']:\n",
    "            # Lấy các đoạn văn bản mỗi đoạn văn bản sẽ chứa các chuỗi context. \n",
    "            context = para['context']\n",
    "            # Tiếp tục duyệt qua một danh sách qa gồm các câu hỏi và trả lời \n",
    "            for qa in para['pas']:\n",
    "                # lấy ra các question từ tập qas  và answer từ qas\n",
    "                question = qa[\"question\"]\n",
    "                answer_text = qa[\"answers\"][0][\"text\"] \n",
    "                # lấy ra danh sách các chuỗi câu trả lời đúng cho câu hỏi từ đoạn văn bản \n",
    "                # vì có nhiều cách trả lời khác nhau cho câu hỏi \n",
    "                all_answers = [_['text'] for _ in qa['answers']]\n",
    "                # Lấy ra vị trí của kí tự đầu tiên thuộc câu trả lời \n",
    "                start_char_idx = qa['answers'][0]['answer_start']\n",
    "                squad_eg = SquadExample(\n",
    "                    question , context , start_char_idx , answer_text , all_answers\n",
    "                )\n",
    "                # Tiền xử lý \n",
    "                squad_eg.preprocessor()\n",
    "                squad_examples.append(squad_eg)\n",
    "    \n",
    "    return squad_examples\n",
    "\n",
    "# Xây dưngj lớp xử lý dữ liệu đầu vào \n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item , key))\n",
    "    \n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    \n",
    "    x =  [\n",
    "        dataset_dict['input_ids'],\n",
    "        dataset_dict['token_type_ids'],\n",
    "        dataset_dict['attention_mask'],\n",
    "    ]\n",
    "    y = [dataset_dict['start_token_idx'] , dataset_dict['end_token_idx']]\n",
    "    return x , y \n",
    "\n",
    "# Xây dữ liệu train và tets dựa trên raw_data đã tải trước đó \n",
    "train_squad_examples = create_squad_examples(raw_train_data)\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    ## Bert model \n",
    "    encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # QA Model \n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "     \n",
    "    embedding = encoder(\n",
    "        input_ids, token_type_ids , attention_mask \n",
    "    )[0]\n",
    "\n",
    "    start_logits = layers.Dense(1  , name='start_logit' , use_bais=False)(embedding)\n",
    "    start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "    \n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs = [input_ids , token_type_ids , attention_mask],\n",
    "        ouputs=[start_probs , end_probs],\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=[loss, loss]\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_tpu = True\n",
    "if use_tpu:\n",
    "    # Create distribution strategy\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "\n",
    "    # Create model\n",
    "    with strategy.scope():\n",
    "        model = create_model()\n",
    "else:\n",
    "    model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text): \n",
    "    # biến đổi văn bản thành chữ thường \n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuations xóa ký tự đắc biệt \n",
    "    # Dầu tiên tạo một dannh sách chứa các ký tự đặc biệt \n",
    "    exclude = set(string.punctuation)\n",
    "    # Sau đó tiến hành loại bỏ các ký tự đặc biệt \n",
    "    # bằng cách xây dựng lại 1 con text mới thay thế nó = ''\n",
    "    text = \"\".join(_ for _ in text if _ not in exclude)\n",
    "\n",
    "    # sử dụng re.sub để thay thế loạt chữ liệt kể = \" \"\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    text = re.sub(regex, \" \", text)\n",
    "\n",
    "    # Remove extra white space\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuations xóa dấu câu\n",
    "    exclude = set(string.punctuation)\n",
    "    # sau đó nối lại các ký tự trong văn bản và không thuộc tập exclude \n",
    "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Remove articles sử dụng biên dịch biểu thức \n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    # sử dụng re.sub để thay thế các ký tự =  \" \"  và trả về 1 text \n",
    "    text = re.sub(regex, ' ', text)\n",
    "\n",
    "    # Xoa khoẳng trắng thừa \n",
    "    text =  \" \".join(text.split())\n",
    "    return text \n",
    "\n",
    "class ExactMatch(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Mỗi đối tượng `SquadExample` chứa độ lệch cấp độ ký tự cho mỗi mã thông báo\n",
    "    trong đoạn đầu vào của nó.\n",
    "    \n",
    "    Chúng tôi sử dụng chúng để lấy lại khoảng văn bản tương ứng\n",
    "    đến các mã thông báo giữa mã thông báo bắt đầu và kết thúc được dự đoán .\n",
    "    \n",
    "    Tất cả các câu trả lời thực tế cũng có trong mỗi đối tượng `SquadExample`.\n",
    "    tính toán tỷ lệ phần trăm của các điểm dữ liệu mà khoảng văn bản thu được\n",
    "    từ các dự đoán mô hình phù hợp với một trong những câu trả lời đúng sự thật.\n",
    "        \"\"\"\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self , epoch, logs=None):\n",
    "        pred_start , pred_end = self.model.predict(self.x_eval)\n",
    "        # Khởi tạo cuont để đếm số lượng câu trả lời chính xác \n",
    "        count = 0\n",
    "        # Lọc qua những vi dụ trong tập dư liệu kiểm tra eval_squad_examples mà không\n",
    "        # bị bỏ qua \n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # Duyệt qua những cặp vị trí bắt đầu và kết thúc dự đoán cùng với những chỉ số tương ứng \n",
    "        for idx , (start , end) in enumerate(zip(pred_start , pred_end)):\n",
    "            # Lấy ra ví dụ tương ứng trong tập dữ liệu kiểm tra đã lọc\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # lấy ra danh sách cac vị trí ký tự trong đoạn văn bản tương ứng với từng token\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            # Nếu vị trí bắt đầu lớn hơn hoặc bằng độ dài của danh sách offsets,\n",
    "            #  phương thức này sẽ bỏ qua ví dụ này và tiếp tục vòng lặp\n",
    "            # tức là dự đoán không thuộc offsets\n",
    "            if start >=  len(offsets):\n",
    "                continue\n",
    "            # Nếu không, lấy ra vị trí ký tự bắt đầu của token bắt đầu \n",
    "            pred_char_start = offsets[start][0]\n",
    "            # Nếu vị trí kết thúc nhỏ hơn độ dài của danh sách offsets, phương thức này sẽ lấy ra \n",
    "            # vị trí ký tự bắt đầu và kết thúc của câu trả lời trong văn bản ngữ cảnh \n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                # thực hiện khoanh vung cho câu tl\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else : \n",
    "            # Ngược lại ta lấy từ vị trí ký tự dự đoán đầu tiên đến hết câu . \n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            \n",
    "            # Chuẩn hóa câu trả lời dự đoán\n",
    "            normalized_pred_ans = normalize_text(pred_ans)\n",
    "            # tạo danh sách chuẩn hóa tất cả các nhãn thực \n",
    "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            \n",
    "            # nếu như các nhãn dự đoán mà thuộc nhãn thực thì tăng count lên 1 \n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1 \n",
    "        # Sau khi duyệt hết các ví dụ kiểm tra, phương thức này sẽ tính toán độ chính xác của mô hình\n",
    "        # bằng cách chia số lượng câu trả lời chính xác cho số lượng ví dụ kiểm tra\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,  # For demonstration, 3 epochs are recommended\n",
    "    verbose=2,\n",
    "    batch_size=64,\n",
    "    callbacks=[exact_match_callback],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
