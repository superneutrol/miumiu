{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from keras import Model \n",
    "from keras.layers import (\n",
    "    Add , Dense,Dropout , Embedding ,GlobalAveragePooling1D ,\n",
    "    Input, Layer, LayerNormalization, MultiHeadAttention,\n",
    "    Softmax \n",
    ")\n",
    "\n",
    "from keras.initializers import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPartition(Layer):\n",
    "    def __init__(self, window_size =4 , channels=3, **kwargs):\n",
    "        super() .__init__(PatchPartition,self, **kwargs)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images, \n",
    "            sizes = [ 1, self.window_size, self.window_size ,1],\n",
    "            strides = [1 , self.window_size, self.window_size , 1],\n",
    "            rates = [1 , 1 ,1, 1],\n",
    "            padding = 'VALID',\n",
    "\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size , -1, patch_dims])\n",
    "        return patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s -o flower.jpeg https://images.unsplash.com/photo-1604085572504-a392ddf0d86a?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=224&q=224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plt.imread('flower.jpeg')\n",
    "image = tf.image.resize(tf.convert_to_tensor(image) , size=(244,244))\n",
    "plt.imshow(image.numpy().astype('uint8'))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tf.expand_dims(image, axis=0)\n",
    "patches = PatchPartition()(batch)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(np.sqrt(patches.shape[1]))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (4, 4, 3))\n",
    "    ax.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEmbedding(Layer):\n",
    "    def __init__(self, num_patches , projection_dim , **kwargs):\n",
    "        super(LinearEmbedding, self).__init__(**kwargs)\n",
    "        self.num_pacthes = num_patches \n",
    "        self.projection = Dense(projection_dim)\n",
    "        self.position_embedding = Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "    \n",
    "    def call (self, patch):\n",
    "        # tính toán bản vá nhúng \n",
    "        patches_embed = self.projection(patch)\n",
    "        # tính toán vị trí nhúng từ \n",
    "        positions = tf.range(start=0 , limit=self.num_pacthes , delta=1)\n",
    "        return patches_embed + self.position_embedding(positions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = LinearEmbedding(3136, 96)(patches)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(Layer):\n",
    "    def __init__(self, input_resolution , channels):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.channels = channels \n",
    "        self.linear_trans = Dense(2 *channels , use_bias=False)\n",
    "\n",
    "    def call (self, x):\n",
    "        height , width = self.input_resolution\n",
    "        _ , _ , C = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape=(-1 , height , width, C))\n",
    "        x0 = x[:, 0::2 , 0::2 , :]\n",
    "        x1 = x[:, 1::2 , 0::2 , :]\n",
    "        x2 = x[:, 0::2 , 1::2 , :]\n",
    "        x3 = x[:, 1::2 , 1::2 , :]\n",
    "\n",
    "        x = tf.concat((x0 , x1 , x2 , x3), axis=-1)\n",
    "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2) , 4*C))\n",
    "        return self.linear_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 96\n",
    "num_patch_x = 224 // 4\n",
    "num_patch_y = 224 // 4\n",
    "out_patches = PatchMerging((num_patch_x, num_patch_y), channels)(patches)\n",
    "print(f'Input shape (B,   H * W,  C) = {patches.shape}')\n",
    "print(f'Ouput shape (B, H/2*W/2, 4C) = {out_patches.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Layer):\n",
    "    def __init__(self, hidden_features , out_features, dropout_rate=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = Dense(hidden_features , activation=tf.nn.gelu)\n",
    "        self.dense2 = Dense(out_features)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        y = self.dropout(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(768 * 2, 768)\n",
    "y = mlp(tf.zeros((1, 197, 768)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(Layer):\n",
    "    def __init__(\n",
    "            self, dim , window_size , num_heads,\n",
    "            qkv_bias=True, \n",
    "            dropout_rate = 0.0 , \n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim \n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = Dense(dim* 3 , use_bias=qkv_bias)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.proj = Dense(dim)\n",
    "\n",
    "    def buil(self, input_shape):\n",
    "        num_window_elements = (2 * self.window_size[0] - 1) *(\n",
    "            2 * self.window_size[1] - 1\n",
    "        )\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            shape = (num_window_elements , self.num_heads),\n",
    "            initializer = tf.initializers.Zeros(),\n",
    "            trainable= True,\n",
    "        )\n",
    "         # get pair-wise relative position index for each token inside the window\n",
    "#Đoạn mã này thực hiện chức năng tính toán chỉ số vị trí tương đối giữa các cửa sổ trong mô hình Swin Transformer12. \n",
    "# Đoạn mã này thực hiện các bước sau:\n",
    "# Tạo ra một ma trận tọa độ cho mỗi điểm ảnh trong ảnh, với hai chiều là chiều cao và chiều rộng.\n",
    "# Nối hai ma trận tọa độ lại với nhau và làm phẳng chúng thành một vector hai chiều, mỗi hàng là một cặp tọa độ (y, x) cho một điểm ảnh.\n",
    "# Tính toán sự khác biệt tọa độ giữa mỗi cặp điểm ảnh bằng cách trừ vector tọa độ với chính nó theo hai chiều khác nhau.\n",
    "# Chuyển vị ma trận khác biệt tọa độ để có kích thước (size, size, 2), trong đó size là kích thước của một cửa sổ.\n",
    "# Cộng thêm self.window_size[0] - 1 và self.window_size1 - 1 vào ma trận khác biệt tọa độ để dịch chuyển các giá trị từ âm sang dương.\n",
    "# Nhân ma trận khác biệt tọa độ theo chiều ngang với 2 * self.window_size1 - 1 để biến đổi các giá trị theo chiều ngang của ma trận.\n",
    "# Tính tổng ma trận khác biệt tọa độ theo chiều cuối cùng để thu được relative_position_index, là ma trận chỉ số vị trí tương đối giữa các cửa sổ.\n",
    "# Đoạn mã này giúp cho mô hình Swin Transformer có thể học được các quan hệ không gian giữa các cửa sổ và áp dụng bias vị trí tương đối khi tính toán self-attention1.\n",
    "        coords_h = tf.range(self.window_size[0])\n",
    "        coords_w = tf.range(self.window_size[1])\n",
    "        coords_matrix = np.meshgrid(coords_h , coords_w, indexing='ij')\n",
    "        coords = np.stack(coords_matrix)\n",
    "        coords_flatten = coords.reshape(2 , -1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.transpose([1 , 2 , 0])\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "\n",
    "        self.relative_position_index = tf.Variable(\n",
    "            initial_value = tf.convert_to_tensor(\n",
    "                relative_position_index\n",
    "            ),\n",
    "            trainable = False\n",
    "        )\n",
    "\n",
    "        \n",
    "    def call(self, x , mask=None):\n",
    "        _ ,  size , channels = x.shape \n",
    "        head_dim = channels // self.num_heads\n",
    "        x_qkv = self.qkv(x)\n",
    "        x_qkv = tf.reshape(x_qkv , shape=(-1 , size , 3  , self.num_heads , head_dim))\n",
    "        x_qkv = tf.transpose(x_qkv , perm=(2,0,3,1,4))\n",
    "        q , k ,v = x_qkv[0] , x_qkv[1] , x_qkv[2]\n",
    "        q = q * self.scale \n",
    "        k = tf. transpose(k , perm=(0, 1, 2, 3))\n",
    "        attn = q@ k \n",
    "        \n",
    "        # Trèn tham số bias vào SW - MSA \n",
    "        num_window_elements = self.window_q[0] * self.window_size[1] # tính số phần tử trong một cửa sổ \n",
    "        relative_position_index_flat = tf.reshape( # định hình lại ma trận anyf thành vector 1 chiều \n",
    "            self.relative_position_index, shape=(-1,)\n",
    "        )\n",
    "        # Lấy các giá trị bias vị trí tương đối từ bảng self.relative_position_bias_table theo chỉ số trong vector trên.\n",
    "        relavtive_position_bias = tf.gather(\n",
    "            self.relative_position_bias_table , relative_position_index_flat\n",
    "        )\n",
    "        # Reshape lại ma trận bias vị trí tương đối thành kích thước (num_window_elements, num_window_elements, -1)\n",
    "        # tức là một ma trận cho mỗi head attention.\n",
    "        relative_position_bias = tf.reshape(\n",
    "            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n",
    "        )\n",
    "        # Chuyển vị ma trận bias vị trí tương đối theo thứ tự (2, 0, 1) và thêm một chiều ở đầu \n",
    "        # để có kích thước (1, -1, num_window_elements, num_window_elements).\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n",
    "        # Cộng ma trận bias vị trí tương đối vào ma trận attention.\n",
    "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.get_shape()[0]\n",
    "            mask_float = tf.cast(\n",
    "                tf.expand_dims(tf.expand_dims(mask , axis=1), axis=0), tf.float32\n",
    "            )\n",
    "            attn=  tf.reshape(attn, shape=(-1 , nW , self.num_heads, size , size))\n",
    "            attn = keras.activations.softmax(attn , axis=-1)\n",
    "        else: \n",
    "            attn = keras.activations.softmax(attn , axis= -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x_qkv = attn @ v \n",
    "        x_qkv = tf.transpose(x_qkv, perm=(0 ,1 , 2, 3 ))\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = WindowAttention(96, window_size=(4, 4), num_heads=8, qkv_bias=True, qk_scale=None, attn_drop=0.0, proj_drop=0.0)\n",
    "y = attn(tf.zeros((1, 196, 16, 96)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x , window_size):\n",
    "    _, H , W, C = x.shape\n",
    "    num_patch_y = H // window_size\n",
    "    num_patch_x = W // window_size\n",
    "    # định hinhf ảnh kích thước  -1 là số lượng lô  x và y  là số lượng cửa sổ theo chiều dọc và ngang \n",
    "    x = tf.reshape(x , [-1 , num_patch_y , window_size , num_patch_x, window_size, C])\n",
    "    x = tf.transpose(x , perm=[0, 1, 2, 3, 4, 5])\n",
    "    #  -1 là số lượng ảnh trong batch và num_patch_x * num_patch_y là tổng số lượng cửa sổ trong một ảnh.\n",
    "    windows = tf.reshape(x, [-1, num_patch_x * num_patch_y, window_size, window_size, C])\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = window_partition(batch, 4)\n",
    "print(f'Input shape (B,   H,  W,  C) = {batch.shape}')\n",
    "print(f'Ouput shape (num_windows*B, window_size, window_size, C) = {windows.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, height , width , channels):\n",
    "    num_patch_y = height // window_size\n",
    "    num_patch_x = width // window_size \n",
    "    x = tf.reshape(\n",
    "        windows, \n",
    "        shape = (-1 , num_patch_y, num_patch_x , window_size , window_size , channels)\n",
    "    )\n",
    "    x = tf.transpose(x, perm=(0 ,1 ,2 ,3 ,4 ,5))\n",
    "    x = tf.reshape(x , shape=(-1 , height , width , channels))\n",
    "    return x \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = window_reverse(windows, 4, 224, 224)\n",
    "print(f'Input shape (B, num_windows*B, window_size, window_size, C) = {windows.shape}')\n",
    "print(f'Ouput shape (B,   H,  W,  C) = {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(Layer):\n",
    "    def __init__(self, drop_prob =None , **kwargs):\n",
    "        super() .__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def call(self, x ):\n",
    "        input_shape = tf.shape(x)\n",
    "        batch_size = input_shape[0]\n",
    "        rank = x.shape.rank\n",
    "        shape = (batch_size,) +(1,) *(rank-1)\n",
    "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape , dtype = x.dtype)\n",
    "        path_mask = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(Layer):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else tf.identity\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(mlp_hidden_dim, dim, dropout_rate=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = np.zeros([1, H, W, 1])  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            img_mask = tf.constant(img_mask)\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = tf.reshape(mask_windows, [-1, self.window_size * self.window_size])\n",
    "            attn_mask = mask_windows[:, None, :] - mask_windows[:, :, None]\n",
    "            self.attn_mask = tf.where(attn_mask==0, -100., 0.)\n",
    "        else:\n",
    "            self.attn_mask = None\n",
    "\n",
    "    def call(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, [-1, H, W, C])\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = tf.reshape(x_windows, [-1, x_windows.shape[1], self.window_size * self.window_size, C])  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = tf.reshape(attn_windows, [-1, x_windows.shape[1], self.window_size, self.window_size, C])\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = tf.reshape(x, [-1, H * W, C])\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = SwinTransformerBlock(96, (56, 56), 8, window_size=4)\n",
    "y = block(embeddings)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SwinTransformer(num_classes, input_shape=(224, 224, 3), window_size=4, embed_dim=96, num_heads=8):\n",
    "    num_patch_x = input_shape[0] // window_size\n",
    "    num_patch_y = input_shape[1] // window_size\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Patch extractor\n",
    "    patches = PatchPartition(window_size)(inputs)\n",
    "    patches_embed = LinearEmbedding(num_patch_x * num_patch_y, embed_dim)(patches)\n",
    "    # first Swin Transformer block\n",
    "    out_stage_1 = SwinTransformerBlock(\n",
    "        dim=embed_dim,\n",
    "        input_resolution=(num_patch_x, num_patch_y),\n",
    "        num_heads=num_heads,\n",
    "        window_size=window_size,\n",
    "        shift_size=0\n",
    "    )(patches_embed)\n",
    "    # second Swin Transformer block\n",
    "    out_stage_1 = SwinTransformerBlock(\n",
    "        dim=embed_dim,\n",
    "        input_resolution=(num_patch_x, num_patch_y),\n",
    "        num_heads=num_heads,\n",
    "        window_size=window_size,\n",
    "        shift_size=1\n",
    "    )(out_stage_1)\n",
    "    # patch merging\n",
    "    representation = PatchMerging((num_patch_x, num_patch_y), channels=embed_dim)(out_stage_1)\n",
    "    # pooling\n",
    "    representation = GlobalAveragePooling1D()(representation)\n",
    "    # logits\n",
    "    output = Dense(num_classes, activation=\"softmax\")(representation)\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_SwinTransformer(2)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
