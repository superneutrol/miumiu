{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install datasets\n",
    "!pip install huggingface-hub\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import random \n",
    "import logging \n",
    "\n",
    "import tensorflow as tf \n",
    "import keras \n",
    "\n",
    "nltk.download('punkt')\n",
    "# ch·ªâ ghi l·∫°i th√¥ng b√°o l·ªói \n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# Set random seed \n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define certain variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256 # K√çCH TH∆Ø·ªöC H√ÄNG LO·∫†T ƒê·ªÇ ƒê√ÄO T·∫†O M√É TH√îNG B√ÅO TR√äN \n",
    "TOKENIZER_VOCABULARY = 25000 # T·ªîNG S·ªê T·ª™ PH·ª§ DUY NH·∫§T M√Ä TOKENIZER C√ì TH·ªÇ C√ì \n",
    "\n",
    "BLOCK_SIZE = 128 # S·ªê L∆Ø·ª¢NG T·ªêI ƒêA C·ª¶A TOKEN TRONG M·ªñI M·∫™U ƒê·∫¶U V√ÄO \n",
    "NSP_PROB = 0.5  # X√ÅC XU·∫§T C√ÇU TI·∫æP THEO L√Ä C√ÇU TI·∫æP THEO TH·ª∞C T·∫æ TRONG NSP \n",
    "SHORT_SEQ_PROB = 0.1 # X√°c xu·∫•t t·∫°o ra c√°c chu·ªói ngƒÉn h∆°n ƒë·ªÉ gi·∫£m thi·ªÉu \n",
    "# s·ª± kh√¥ng kh·ªõp gi·ªØa ti·ªÅn hu·∫•n luy·ªán v√† tinh ch·ªânh \n",
    "MAX_LENGTH = 512 # S·ªë l∆∞·ª£ng token t·ªëi ƒëa trong m·ªói ƒë·∫ßu v√†o sau ƒë·ªám\n",
    "\n",
    "MLM_PROB = 0.2  # X√°c xu·∫•t v·ªõi nh·ªØng tokens l√† m·∫∑t n·∫° trong MLM \n",
    "\n",
    "TRAIN_BATCH_SIZE = 2 # K√≠ch th∆∞·ªõc l√¥ cho m√¥ h√¨nh hu·∫•n luy·ªán tr∆∞·ªõc tr√™n \n",
    "MAX_EPOCHS = 1 # S·ªê K·ª∂ NGUY√äN T·ªêI ƒêA CHO HU·∫§N LUY·ªÜN M√î H√åNH \n",
    "LEARNING_RATE = 1e-4 # T·ª∂ L·ªÜ H·ªåC T·∫¨P CHO HU·∫§N LUY·ªÜN M√î H√åNH \n",
    "\n",
    "MODEL_CHECKPOINT = 'bert-base-cased' # M√¥ h√¨nh hu·∫•n luy·ªán tr∆∞·ªõc t·ª´ ü§ó Model Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the WikiText dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a new Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts =  [\n",
    "    _ for _ in dataset['train']['text'] if len(_) > 0 and not _.startswith(\"=\")\n",
    "]\n",
    "# X√¢y d·ª±ng h√†m ƒë·ªÉ t√°ch l√¥ vƒÉn b·∫£n \n",
    "def batch_iterator(): \n",
    "    # l·∫∑p qua t·∫•t c·∫£ d·ªØ li·ªáu v·ªõi m·ªói b∆∞·ªõc nh·∫£y b·∫±ng 1 l√¥\n",
    "    for i in range(0 ,len(all_texts), TOKENIZER_BATCH_SIZE):\n",
    "        # S·ª≠ d·ª•ng t·ª´ kh√≥a yield ƒë·ªÉ tr·∫£ v·ªÅ 1 l√¥ vƒÉn b·∫£n t·ª´ danh s√°ch \n",
    "        # k√≠ch th∆∞·ªõc m·ªói l√¥ = TOKENIZER_BATCH_SIZE \n",
    "        yield all_texts[i : i + TOKENIZER_BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator() , vocab_size=TOKENIZER_VOCABULARY \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l·∫•y 1000 d·ªØ li·ªáu t·ª´ d·ªØ li·ªáu train g·ªëc \n",
    "dataset['train'] = dataset['train'].select([_ for _ in range(1000)])\n",
    "# T∆∞∆°ng t·ª± v·ªõi validation data\n",
    "dataset['validation'] = dataset['validation'.select([i for i in range(1000)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We difine the s·ªë l∆∞·ª£ng tokes t·ªëi ƒëa sau khi th·ª±c hi·ªán tokenization m·ªói m·∫´u hu·∫•n luy·ªán\n",
    "# will have\n",
    "max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "def prepare_train_features(examples):\n",
    "    \"\"\"Function to prepare features for NSP task\n",
    "    \n",
    "    Argument: \n",
    "        example: A dictionary with 1 key ('text')\n",
    "        text : List of raw documents (str)\n",
    "    Returns: \n",
    "        examples: A dictionary with 4 keys \n",
    "            Input_ids : List of Tokenized , Concatnated , and Bacthed (t·ª´ng ƒë·ª£t)\n",
    "                sentences form the individual (c√° nh√¢n) raw documents (int)\n",
    "            Token_type_ids : List of intergers (0 or 1) corresponding \n",
    "                To : 0 for sentences no.1  and padding , 1 for sentence no.2 and padding \n",
    "            Attention_mask: List of intergers (0 or 1) corresponding \n",
    "                To : 1 for non-padded tokens , o for padded\n",
    "            Next_sentence_label: List of intergers (0 or 1) corresponding\n",
    "                To : 1 if the second sentence actually follows the fist  \n",
    "                0 if the senetence is sampled from somewhere else in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # X√≥a ƒëi nh·ªØng m·∫´u kh√¥ng mong mu·ªën t·ª´ d·ªØ li·ªáu hu·∫•n luy·ªán \n",
    "    # S·ª≠ d·ª•ng h√†m strip () ƒë·ªÉ bi·∫øn ƒë·ªïi c√°c texts trong document th√†nh string \n",
    "    # m·∫∑c ƒë·ªãnh h√†m n√†y s·∫Ω lo·∫°i b·ªè kho·∫£ng tr·∫Øng \n",
    "    # sau ƒë√≥ ki·ªÉm tra xem chu·ªói d c√≥ b·∫Øt ƒë·∫ßu b·∫±ng k√Ω t·ª± \"=\" hay kh√¥ng \n",
    "    examples['document'] = [\n",
    "        d.strip() for d in examples['document'] if len(d) > 0 and not d.startswitch(\" =\")\n",
    "    ]\n",
    "    # T√°ch c√°c d·ªØ li·ªáu t·ª´ t·∫≠p d·ªØ li·ªáu th√†nh c√°c c√¢u ri√™ng l·∫ª \n",
    "    # s·ª≠ d·ª•ng h√†m nltk.tokenize.sent_tokenize ƒë·ªÉ tr·∫£ v·ªÅ danh s√°ch chu·ªói \n",
    "    # m·ªói chu·ªói l√† m·ªôt c√¢u  \n",
    "    examples['sentences'] = [\n",
    "        nltk.tokenize.sent_tokenize(document) for document in examples['document']\n",
    "    ]\n",
    "    # Chuy·ªÉn ƒë·ªïi c√°c m√£ th√¥ng b√°o trong c√¢u th√†nh id (int) b·∫±ng c√°ch s·ª≠ d·ª•ng \n",
    "    # m√£ th√¥ng b√°o ƒë∆∞·ª£c ƒë√†o t·∫°o \n",
    "    examples['tokenized_sentences'] = [\n",
    "        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc ]\n",
    "        for doc in examples['sentences']\n",
    "    ]\n",
    "    \n",
    "    # x√°c ƒëinh danh s√°ch ƒë·∫ßu ra \n",
    "    examples['input_ids'] = []  # id return  input tokens type int \n",
    "    examples['token_type_ids'] = []  # sentences [0 or 1] second and first \n",
    "    examples['attention_mask']  = []  # [o , 1] : no 1  , for sentences , 0 for padded \n",
    "    examples['next_sentence_labels']=[]   #[0,1]: 1 is follower , 0 is not followed\n",
    "\n",
    "    # Duy·ªát qua tokens id c·ªßa m·∫´u l·∫•y ra v·ªã tr√≠ v√† gi√° tr·ªã \n",
    "    # S·ª≠ d·ª•ng enumerate nh∆∞ m·ªôt iterator m·ªôt bi·∫øn ch·ª©a gi√° tr·ªã c√≥ th·ªÉ l·∫∑p \n",
    "    for doc_index , document in enumerate(examples['tokenized_sentences']):\n",
    "        # kh·ªüi t·∫°o 2 m·∫£ng = None \n",
    "        # m·ªôt m·∫£ng s·∫Ω ch·ª©a 1 b·ªô ƒë·ªám l∆∞u tr·ªØ c√°c ph√¢n ƒëo·∫°n l√†m vi·ªác hi√™n t·∫°i \n",
    "        current_chunk = []\n",
    "        # v√† m·ªôt b·ªô s·∫Ω ch·ª©a ƒë·ªô d√†i c√¢u hi·ªán t·∫°i \n",
    "        current_length = []\n",
    "        i = 0 \n",
    "\n",
    "        \"\"\" \n",
    "            ƒê·ªÉ c√≥ th·ªÉ x·ª≠ l√Ω d·ªÖ dang h∆°n , th∆∞·ªùng t·∫°o ra c√°c ƒëo·∫°n vƒÉn b·∫£n c√≥ chi·ªÅu d√†i \n",
    "                b·∫±ng nhau block_size , \n",
    "            ƒê√¥i khi t·∫°o ra c√°c ƒëo·∫°n vƒÉn b·∫£n ng·∫Øn h∆°n (target_sequences_length) ƒë·ªÉ gi·∫£m \n",
    "                thi·ªÉu s·ª±u kh√°c bi·ªát gi∆∞a qu√° tr√¨nh ti·ªÅn hu·∫•n luy·ªán v√† tinh ch·ªânh \n",
    "        \"\"\"\n",
    "        # X√°c ƒë·ªãnh chi·ªÅu d√†i mong mu·ªën c·ªßa c√¢u \n",
    "        target_seq_length = max_num_tokens # max  = block_size ( k√≠ch th∆∞·ªõc t·ªëi ƒëa c√¢u - special tokens)\n",
    "\n",
    "        # N·∫øu s·ª± ng·∫´u nhi√™n < 10 % t·ª©c 0.1  l√† x√°c xu·∫•t ƒë·ªÉ t·∫°o ra c√°c chu·ªói ng·∫Øn h∆°n  \n",
    "        # ta kh·ªüi t·∫°o s·ªë m·ª•c ti√™u n√†y = 2 - > max_num_tokens \n",
    "        if random.random() < SHORT_SEQ_PROB:\n",
    "            target_seq_length = random.randint(2 , max_num_tokens)\n",
    "        \n",
    "\n",
    "        # X·ª≠ d·ª•ng while ki·ªÉm tra ƒëi·ªÅu ki·ªán i < len(document):\n",
    "        while i < len(document):\n",
    "            # g√°n ph√¢n ƒëo·∫°n c√¢u  = document [i]\n",
    "            segment = document[i]\n",
    "            # th√™m ph√¢n ƒëo·∫°n n√†y v√†o b·ªô ƒë·ªám l∆∞u ch·ªØ ph√¢n ƒëo·∫°n b∆∞·ªõc th·ªùi gian hi·ªán t·∫°i \n",
    "            current_chunk.append(segment)\n",
    "            # Th√™m ƒë·ªô d√†i cho b·ªô ƒë·ªám l∆∞u ch·ªØ ph√¢n ƒëo·∫°n b∆∞·ªõc th·ªùi gian\n",
    "            # b·∫±ng c√°ch  + ƒë·ªô d√†i ban ƒë·∫ßu v·ªõi ƒë·ªô d√†i ƒëo·∫°n c√¢u ph√¢n ƒëo·∫°n\n",
    "            current_length += len(segment)\n",
    "\n",
    "            # X√¢y d·ª±ng ƒëi·ªÅu ki·ªán khi i = len(document) - 1 \n",
    "            # ho·∫∑c ƒë·ªô d√†i cau hi·ªán t·∫°i >= target_seg_length t·ª©c l√† ƒë·ªô d√†i v∆∞·ª£t ch·ªâ ti√™u \n",
    "            if  i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                # kh·ªüi t·∫°o m·ªôt bi·∫øn a_end l√† bao nhi√™u ƒëo·∫°n t·ª´ 'current_chunk \n",
    "                # ƒëi v√†o A t·ª´ c√¢u ƒë·∫ßu ti√™n\n",
    "                if current_chunk : # n·∫øu c√≥ t·ªìn t·∫°i b·ªô ƒë·ªám l∆∞u tr·ªØ ph√¢n ƒëo·∫°n b∆∞·ª£c th·ªùi gia \n",
    "                    # Hi·ªán t·∫°i \n",
    "                    a_end = 1\n",
    "\n",
    "                    if len(current_chunk) >= 2 :\n",
    "                        a_end = random.randint(1 , len(current_chunk) - 1) \n",
    "                    \n",
    "                    tokens_a = []\n",
    "                    # l·∫∑p qua m·ªôt chu·ªói c√°c tokens a_and:\n",
    "                    for j in range(a_end):\n",
    "                        # L·∫∑p qua c√°c ƒëo·∫°n vƒÉn b·∫£n (current chunk t·ª´ 0 ƒë·∫øn a_end)\n",
    "                        # v·ªõi m·ªói ƒëo·∫°n vƒÉn b·∫£n , th√™m t·∫•t c·∫£ c√°c tokens c·ªßa ƒëo·∫°n ƒë√≥ \n",
    "                        # vao a_end s·ª≠ d·ª•ng extend ƒë·ªÉ n·ªëi c√°c ƒëo·∫°n danh s√°ch v·ªõi nhau \n",
    "                    # sau khi k·∫øt th√∫c tokens_a s·∫Ω nh·∫≠n ƒë∆∞·ª£c 1 list 0 -> a_end -1 c√°c \n",
    "                    # ƒëo·∫°n tokens c·ªßa current_chunk \n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                    # Kh·ªüi t·∫°o danh s√°ch token_ b \n",
    "                    tokens_b = []\n",
    "                    #  ki·ªÉm tra xem curren_chunk c√≥ ph·∫£i 1 ƒëo·∫°n kh√¥ng \n",
    "                    # ho·∫∑c x√°c xu·∫•t ng·∫´u nhi√™n c√≥ nh·ªè h∆°n nsp hay kh√¥ng \n",
    "                    # N·∫øu ƒëi·ªÅu ki·ªán ƒë√∫ng th√¨ tokens_b s·∫Ω ƒë∆∞·ª£c t·∫°o ra t·ª´ 1 vƒÉn b·∫£n ng·∫´u nhi√™n kh√°c \n",
    "                    # v·ªõi 1 vƒÉn b·∫£n hi·ªán t·∫°i \n",
    "                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n",
    "                        is_random_next = True \n",
    "                        # sau ƒë√≥ t√≠nh to√°n chi·ªÅu d√†i m·ª•c ti√™u token b \n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # Kh·ªüi t·∫°o bi·∫øn random_document_index ƒë·ªÉ l∆∞u ch·ªß nh·ªØng ch·ªâ s·ªë c·ªßa vƒÉn b·∫£n\n",
    "                        # ng·∫´u nhi√™n  l·∫∑p 10 l·∫ßn ƒë·ªÉ ch√°nh ch·ªçn l·∫°i vƒÉn b·∫£n hi·ªán t·∫°i \n",
    "                        for _ in range(10):\n",
    "                            # V·ªõi m·ªói l·∫ßn l·∫∑p g√°n cho random_document_index \n",
    "                            # b·ªô ch·ªâ s·ªë idx t·ª´ 0 ƒë·∫øn danh s√°ch c√°c vƒÉn b·∫£n ƒë∆∞·ª£c t√°ch th√†nh c√°c c√¢u - 1\n",
    "                            random_document_index = random.randint(\n",
    "                                0 , len(examples['tokenized_sentences']) - 1\n",
    "                            )\n",
    "                            # v√† ki·ªÉm tr·∫£ xem c√°c ch·ªâ s·ªë indx c√≥ kh√°c hay kh√¥ng n·∫øu 0 kh√°c ti√™p \n",
    "                        \n",
    "                            if random_document_index != doc_index:\n",
    "                                break \n",
    "                        # sau ƒë√≥ g√°n random_document b·∫±ng ph·∫ßn t·ª≠ t·ªáp vƒÉn b·∫£n ƒë√£ t√°ch c√¢u \n",
    "                        # t·∫°i nh·ªØng ch·ªâ s·ªë t∆∞∆°ng t·ª± c·ªßa random_document_indx\n",
    "                        random_document = examples['tokenized_sentences'][\n",
    "                            random_document_index \n",
    "                        ]\n",
    "                        # kh·ªüi t·∫°o bi·∫øn random_start  = 0 -> len_radom_document - 1 \n",
    "                        random_start = random.randint(0 , len(random_document) - 1)\n",
    "                        # Duy·ªát qua c√°c c√¢u trong random_document t·ª´ ch·ªâ s·ªë satrt ƒë·∫øn h·∫øt \n",
    "                        for j in range(random_start , len(random_document)):\n",
    "                            # L·∫∑p qua c√°c ƒëo·∫°n vƒÉn b·∫£n random_documen v√† th√™m c√°c ch·ªâ s·ªë tokens c·ªßa c√¢u\n",
    "                            # v√† s·ª≠ d·ª•ng extend ƒë·ªÉ n·ªëi c√°c c√¢u v·ªõi nhau \n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            # sau ƒë√≥ ki·ªÉm tr·∫£ xem ƒë·ªô d√†i tokens b c√≥ l·ªõn ho·∫∑c bƒÉngc hi·ªÅu ƒëai m·ª•c ti√™u \n",
    "                            # mong mu·ªën cho b hay kh√¥ng \n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                    break\n",
    "                            \n",
    "                            # sau ƒë√≥ t√≠nh s·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng \n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # tr∆∞·ªùng h·ª£p c√≤n l·∫°i \n",
    "                    else: \n",
    "                        is_random_next = False \n",
    "                        # duy·ªát qua danh s√°ch current_chunk v√† th√™m c√°c ch·ªâ s·ªë a_end \n",
    "                        # cho tokens_b , s·ª≠ d·ª•ng extend ƒë·ªÉ n·ªëi c√°c c√¢u v·ªõi nhau \n",
    "                        for j in range(a_end , len(current_chunk)):\n",
    "                            # s·ª≠ d·ª•ng extend ƒë·ªÉ n·ªëi c√°c c√¢u v·ªõi nhau \n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # Thi·∫øt l·∫≠p ƒë√¢√π v√†o t·ª´ 2 danh s√°ch tokens_a , tokens_b \n",
    "                    input_ids = tokenizer.build_inputs_width_special_tokens(\n",
    "                        tokens_a , tokens_b\n",
    "                    )\n",
    "                    # Bi·∫øn ƒë·ªïi type_tokens_ids 0 for sentences a , 1 for sentences b \n",
    "                    tokens_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
    "                        tokens_a , tokens_b\n",
    "                    )\n",
    "                    # ƒê·ªám cho tokens_ids v√† tokens_tyoe_ids \n",
    "                    padded = tokenizer.pad(\n",
    "                        {\"input_ids\" :input_ids , \" tokens_type_ids\": tokens_type_ids},\n",
    "                        padding = \"max_length\", \n",
    "                        max_length=MAX_LENGTH,\n",
    "                    )\n",
    "\n",
    "                    examples['input_ids'].append(padded['input_ids'])\n",
    "                    examples['token_type_ids'].append(padded['token_type_ids'])\n",
    "                    examples[\"attention_mask\"].append(padded['attention_mask']) # 1 for non-paded , 0 for padded\n",
    "                    examples['next_sentences_labels'].append(1 if is_random_next else 0)\n",
    "\n",
    "            i+ 1 \n",
    "        \n",
    "    # X√≥a ƒëi t·∫•t c·∫£ c√°c c·ªôt d·ªØ li·ªáu kh√¥ng c·∫ßn thi·∫øt t·ª´ t·ªáp d·ª± li·ªáu \n",
    "    del  examples['document']\n",
    "    del examples['sentences']\n",
    "    del examples['text']\n",
    "    del examples['tokenized_sentences']\n",
    "\n",
    "    return examples \n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_train_features , batchched=True , remove_columns=['text'] , num_proc =1, \n",
    ")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling \n",
    "\n",
    "collater = DataCollatorForLanguageModeling(\n",
    "    # tokenizer h√≥a vƒÉn b·∫£n , xacs xu·∫•t m·∫∑t n·∫° cho mlm = 0.2  20 % return tensor \n",
    "    tokeinzer= tokenizer , mlm=True , mlm_probability=MLM_PROB , return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset['train'].to_tf_dataset(\n",
    "    columns= ['input_ids' , 'token_type_ids' , 'attention_mask'],\n",
    "    label_col = ['labels' , 'next_sentences_label'],\n",
    "    batch_size = TRAIN_BATCH_SIZE , \n",
    "    shuffle = True , \n",
    "    cllate_fn = collater , \n",
    ")\n",
    "\n",
    "validation = tokenized_dataset['validation'].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\", \"next_sentence_label\"],\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collater,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig \n",
    "config = BertConfig.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "\n",
    "model = TFBertForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, validation_data=validation, epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('pretrained-bert' , organization='keras-io')\n",
    "tokenizer.push_to_hub('pretrained-bert', organization='keras-io')\n",
    "\n",
    "from transformers import TFBertForPreTraining \n",
    "model = TFBertForPreTraining.from_pretrained('your-username/my-awesome-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
